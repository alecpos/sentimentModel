{
  "codebase_summary": {
    "total_files": 38,
    "total_tokens": 29438,
    "file_extensions": {
      ".py": 38
    },
    "directory_structure": {},
    "complexity_hotspots": [
      {
        "file": "scripts/digest.py",
        "complexity": 95
      },
      {
        "file": "app/models/ml/prediction/account_health_predictor.py",
        "complexity": 55
      },
      {
        "file": "app/models/ml/prediction/anomaly_detector.py",
        "complexity": 15
      },
      {
        "file": "app/models/ml/prediction/ad_score_predictor.py",
        "complexity": 14
      },
      {
        "file": "app/core/data_lake/data_pipeline_service.py",
        "complexity": 10
      }
    ],
    "central_modules": [
      {
        "file": "app/models/ml/prediction/account_health_predictor.py",
        "imported_by_count": 74
      },
      {
        "file": "app/models/ml/prediction/ad_score_predictor.py",
        "imported_by_count": 74
      },
      {
        "file": "app/models/ml/prediction/anomaly_detector.py",
        "imported_by_count": 71
      },
      {
        "file": "app/models/ad_account_health_model.py",
        "imported_by_count": 70
      },
      {
        "file": "app/models/ad_score_model.py",
        "imported_by_count": 70
      }
    ],
    "generation_time": "2025-02-21T12:47:30.309018",
    "processing_time_seconds": 1.453667163848877
  },
  "dependencies": {
    "app/__init__.py": {
      "imports": [],
      "imported_by": []
    },
    "app/core/database.py": {
      "imports": [
        "sqlalchemy.create_engine",
        "sqlalchemy.orm.sessionmaker",
        "sqlalchemy.orm.declarative_base",
        "sqlalchemy.pool.QueuePool",
        "uuid"
      ],
      "imported_by": [
        "tests/test_data_lake.py"
      ]
    },
    "app/core/__init__.py": {
      "imports": [],
      "imported_by": []
    },
    "app/core/config/__init__.py": {
      "imports": [],
      "imported_by": []
    },
    "app/core/ml/__init__.py": {
      "imports": [],
      "imported_by": []
    },
    "app/core/data_lake/security_manager.py": {
      "imports": [
        "logging",
        "typing.Dict",
        "cryptography.fernet.Fernet"
      ],
      "imported_by": [
        "app/core/data_lake/data_pipeline_service.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_score_model.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/__init__.py",
        "app/schemas/ad_account_health_schema.py",
        "scripts/digest.py",
        "scripts/digest.py"
      ]
    },
    "app/core/data_lake/data_pipeline_service.py": {
      "imports": [
        "logging",
        "sqlalchemy.orm.Session",
        "app.core.data_lake.security_manager.SecurityManager",
        "app.models.domain.data_catalog_model.DataCatalogModel"
      ],
      "imported_by": [
        "app/core/data_lake/security_manager.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/domain/__init__.py",
        "app/services/domain/data_catalog_service.py",
        "tests/test_data_pipeline_service.py",
        "tests/test_data_pipeline_service.py",
        "tests/test_data_pipeline_service.py",
        "scripts/digest.py"
      ]
    },
    "app/utils/__init__.py": {
      "imports": [],
      "imported_by": []
    },
    "app/models/__init__.py": {
      "imports": [
        "app.core.database.BaseModel",
        "ad_score_model.AdScoreModel",
        "ad_score_model.AdScoreAnalysisModel",
        "ad_account_health_model.AdAccountHealthModel",
        "ad_account_health_model.PerformanceMetricModel",
        "ml.prediction.ad_score_predictor.AdScorePredictor",
        "ml.prediction.account_health_predictor.AdvancedHealthPredictor",
        "ml.prediction.anomaly_detector.EnhancedAnomalyDetector"
      ],
      "imported_by": [
        "app/models/ad_account_health_model.py",
        "app/models/ad_score_model.py",
        "app/models/ml/__init__.py",
        "app/models/ml/__init__.py",
        "app/models/ml/__init__.py",
        "app/models/ml/prediction/__init__.py",
        "app/models/ml/prediction/__init__.py",
        "app/models/ml/prediction/__init__.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_lake_model.py",
        "tests/test_anomaly_detector.py",
        "tests/performance_test.py",
        "tests/integration_test.py",
        "tests/error_test.py",
        "tests/error_test.py",
        "tests/test_ad_predictor.py"
      ]
    },
    "app/models/ad_account_health_model.py": {
      "imports": [
        "datetime.datetime",
        "typing.Dict",
        "typing.Any",
        "typing.List",
        "typing.Optional",
        "uuid.uuid4",
        "sqlalchemy.Column",
        "sqlalchemy.String",
        "sqlalchemy.JSON",
        "sqlalchemy.Float",
        "sqlalchemy.DateTime",
        "sqlalchemy.Boolean",
        "sqlalchemy.orm.relationship",
        "app.core.database.BaseModel"
      ],
      "imported_by": [
        "app/core/data_lake/security_manager.py",
        "app/models/__init__.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/services/domain/data_lake_service.py",
        "app/services/domain/data_catalog_service.py",
        "tests/test_data_lake.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py"
      ]
    },
    "app/models/ad_score_model.py": {
      "imports": [
        "datetime.datetime",
        "typing.Dict",
        "typing.Any",
        "typing.List",
        "typing.Optional",
        "uuid.uuid4",
        "sqlalchemy.Column",
        "sqlalchemy.String",
        "sqlalchemy.JSON",
        "sqlalchemy.Float",
        "sqlalchemy.DateTime",
        "sqlalchemy.orm.relationship",
        "app.core.database.BaseModel"
      ],
      "imported_by": [
        "app/core/data_lake/security_manager.py",
        "app/models/__init__.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/services/domain/data_lake_service.py",
        "app/services/domain/data_catalog_service.py",
        "tests/test_data_lake.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py"
      ]
    },
    "app/models/ml/__init__.py": {
      "imports": [
        "prediction.ad_score_predictor.AdScorePredictor",
        "prediction.account_health_predictor.AdvancedHealthPredictor",
        "prediction.anomaly_detector.EnhancedAnomalyDetector"
      ],
      "imported_by": [
        "app/models/__init__.py",
        "app/models/__init__.py",
        "app/models/__init__.py",
        "app/models/ml/prediction/__init__.py",
        "app/models/ml/prediction/__init__.py",
        "app/models/ml/prediction/__init__.py",
        "tests/test_anomaly_detector.py",
        "tests/performance_test.py",
        "tests/integration_test.py",
        "tests/error_test.py",
        "tests/error_test.py",
        "tests/test_ad_predictor.py"
      ]
    },
    "app/models/ml/prediction/account_health_predictor.py": {
      "imports": [
        "numpy",
        "pandas",
        "joblib",
        "logging",
        "tensorflow",
        "json",
        "os",
        "optuna",
        "shap",
        "datetime.datetime",
        "typing.Dict",
        "typing.Any",
        "typing.List",
        "typing.Optional",
        "sklearn.ensemble.GradientBoostingRegressor",
        "sklearn.impute.KNNImputer",
        "sklearn.preprocessing.StandardScaler",
        "sklearn.pipeline.Pipeline",
        "sklearn.model_selection.TimeSeriesSplit",
        "sklearn.model_selection.train_test_split",
        "sklearn.metrics.mean_squared_error",
        "sklearn.metrics.r2_score",
        "sklearn.metrics.mean_absolute_error",
        "sklearn.base.BaseEstimator",
        "sklearn.base.RegressorMixin",
        "sklearn.compose.ColumnTransformer",
        "keras.layers",
        "keras.models",
        "keras.callbacks",
        "keras.optimizers"
      ],
      "imported_by": [
        "app/core/data_lake/security_manager.py",
        "app/core/data_lake/security_manager.py",
        "app/core/data_lake/data_pipeline_service.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_lake_model.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "tests/performance_test.py",
        "tests/performance_test.py",
        "tests/integration_test.py",
        "tests/integration_test.py",
        "tests/integration_test.py",
        "tests/test_ad_predictor.py",
        "tests/test_ad_predictor.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py"
      ]
    },
    "app/models/ml/prediction/anomaly_detector.py": {
      "imports": [
        "torch",
        "numpy",
        "pandas",
        "logging",
        "typing.Dict",
        "typing.Any",
        "typing.List",
        "typing.Optional",
        "typing.Tuple",
        "datetime.datetime",
        "sklearn.ensemble.IsolationForest",
        "sklearn.preprocessing.StandardScaler",
        "sklearn.feature_selection.VarianceThreshold",
        "sklearn.svm.OneClassSVM",
        "sklearn.linear_model.ElasticNetCV",
        "sklearn.decomposition.PCA",
        "sklearn.pipeline.Pipeline",
        "torch.utils.data.DataLoader",
        "torch.utils.data.TensorDataset",
        "torch.nn",
        "optuna",
        "shap"
      ],
      "imported_by": [
        "app/core/data_lake/security_manager.py",
        "app/core/data_lake/security_manager.py",
        "app/core/data_lake/data_pipeline_service.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_lake_model.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "tests/performance_test.py",
        "tests/performance_test.py",
        "tests/integration_test.py",
        "tests/integration_test.py",
        "tests/integration_test.py",
        "tests/test_ad_predictor.py",
        "tests/test_ad_predictor.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py"
      ]
    },
    "app/models/ml/prediction/__init__.py": {
      "imports": [
        "ad_score_predictor.AdScorePredictor",
        "account_health_predictor.AdvancedHealthPredictor",
        "anomaly_detector.EnhancedAnomalyDetector"
      ],
      "imported_by": [
        "app/models/__init__.py",
        "app/models/__init__.py",
        "app/models/__init__.py",
        "app/models/ml/__init__.py",
        "app/models/ml/__init__.py",
        "app/models/ml/__init__.py",
        "tests/test_anomaly_detector.py",
        "tests/performance_test.py",
        "tests/integration_test.py",
        "tests/error_test.py",
        "tests/error_test.py",
        "tests/test_ad_predictor.py"
      ]
    },
    "app/models/ml/prediction/ad_score_predictor.py": {
      "imports": [
        "torch",
        "xgboost",
        "shap",
        "numpy",
        "pandas",
        "logging",
        "typing.Dict",
        "typing.Any",
        "typing.List",
        "typing.Optional",
        "typing.Tuple",
        "datetime.datetime",
        "sklearn.compose.ColumnTransformer",
        "sklearn.preprocessing.StandardScaler",
        "sklearn.preprocessing.OneHotEncoder",
        "sklearn.feature_extraction.text.TfidfVectorizer",
        "sklearn.metrics.mean_squared_error",
        "sklearn.metrics.r2_score",
        "sklearn.metrics.mean_absolute_error",
        "sklearn.model_selection.GridSearchCV",
        "sklearn.model_selection.train_test_split",
        "torch.utils.data.DataLoader",
        "torch.utils.data.TensorDataset",
        "torch.nn",
        "torch.optim",
        "scipy.optimize.minimize"
      ],
      "imported_by": [
        "app/core/data_lake/security_manager.py",
        "app/core/data_lake/security_manager.py",
        "app/core/data_lake/data_pipeline_service.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_lake_model.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "tests/performance_test.py",
        "tests/performance_test.py",
        "tests/integration_test.py",
        "tests/integration_test.py",
        "tests/test_ad_predictor.py",
        "tests/test_ad_predictor.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py"
      ]
    },
    "app/models/domain/__init__.py": {
      "imports": [
        "app.models.domain.data_lake_model.DataLakeModel",
        "app.models.domain.data_catalog_model.DataCatalogModel"
      ],
      "imported_by": [
        "app/core/data_lake/data_pipeline_service.py",
        "app/services/domain/data_lake_service.py",
        "app/services/domain/data_catalog_service.py",
        "tests/test_data_pipeline_service.py",
        "tests/test_data_lake.py"
      ]
    },
    "app/models/domain/data_catalog_model.py": {
      "imports": [
        "datetime.datetime",
        "uuid.uuid4",
        "sqlalchemy.Column",
        "sqlalchemy.String",
        "sqlalchemy.JSON",
        "sqlalchemy.DateTime",
        "sqlalchemy.ForeignKey",
        "sqlalchemy.orm.relationship",
        "app.core.database.BaseModel"
      ],
      "imported_by": [
        "app/models/__init__.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/models/domain/data_lake_model.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/__init__.py",
        "app/schemas/ad_account_health_schema.py",
        "app/services/domain/data_lake_service.py",
        "app/services/domain/data_catalog_service.py",
        "tests/test_data_lake.py",
        "scripts/digest.py",
        "scripts/digest.py"
      ]
    },
    "app/models/domain/data_lake_model.py": {
      "imports": [
        "datetime.datetime",
        "uuid.uuid4",
        "sqlalchemy.Column",
        "sqlalchemy.String",
        "sqlalchemy.JSON",
        "sqlalchemy.DateTime",
        "sqlalchemy.LargeBinary",
        "sqlalchemy.CheckConstraint",
        "sqlalchemy.orm.relationship",
        "sqlalchemy.orm.validates",
        "app.core.database.BaseModel"
      ],
      "imported_by": [
        "app/models/__init__.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_catalog_model.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/__init__.py",
        "app/schemas/ad_account_health_schema.py",
        "app/services/domain/data_lake_service.py",
        "app/services/domain/data_catalog_service.py",
        "tests/test_data_lake.py",
        "scripts/digest.py",
        "scripts/digest.py"
      ]
    },
    "app/schemas/ad_score_schema.py": {
      "imports": [
        "datetime.datetime",
        "typing.Dict",
        "typing.Any",
        "typing.List",
        "typing.Optional",
        "uuid.UUID",
        "pydantic.BaseModel",
        "pydantic.Field"
      ],
      "imported_by": [
        "app/core/data_lake/security_manager.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_lake_model.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/services/domain/data_catalog_service.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py"
      ]
    },
    "app/schemas/__init__.py": {
      "imports": [
        "datetime.datetime",
        "typing.Dict",
        "typing.Any",
        "typing.List",
        "typing.Optional",
        "uuid.UUID",
        "sqlalchemy.ext.declarative.declarative_base",
        "ad_score_schema.AdScoreRequestSchema",
        "ad_score_schema.AdScoreResponseSchema",
        "ad_score_schema.AdScoreAnalysisRequestSchema",
        "ad_score_schema.AdScoreAnalysisResponseSchema",
        "ad_account_health_schema.AdAccountHealthRequestSchema",
        "ad_account_health_schema.AdAccountHealthResponseSchema",
        "ad_account_health_schema.PerformanceMetricSchema"
      ],
      "imported_by": [
        "app/core/data_lake/security_manager.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_lake_model.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/services/domain/data_catalog_service.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py"
      ]
    },
    "app/schemas/ad_account_health_schema.py": {
      "imports": [
        "datetime.datetime",
        "typing.Dict",
        "typing.Any",
        "typing.List",
        "typing.Optional",
        "uuid.UUID",
        "pydantic.BaseModel",
        "pydantic.Field"
      ],
      "imported_by": [
        "app/core/data_lake/security_manager.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_lake_model.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/services/domain/data_catalog_service.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py",
        "scripts/digest.py"
      ]
    },
    "app/api/__init__.py": {
      "imports": [],
      "imported_by": []
    },
    "app/api/v1/__init__.py": {
      "imports": [],
      "imported_by": []
    },
    "app/api/v1/routes/__init__.py": {
      "imports": [],
      "imported_by": []
    },
    "app/services/__init__.py": {
      "imports": [],
      "imported_by": []
    },
    "app/services/ml/__init__.py": {
      "imports": [],
      "imported_by": []
    },
    "app/services/domain/data_lake_service.py": {
      "imports": [
        "app.core.database.SessionLocal",
        "app.models.domain.data_lake_model.DataLakeModel",
        "uuid.uuid4"
      ],
      "imported_by": [
        "app/models/ad_account_health_model.py",
        "app/models/ad_score_model.py",
        "app/models/domain/__init__.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_lake_model.py",
        "app/services/domain/data_catalog_service.py",
        "app/services/domain/data_catalog_service.py",
        "tests/test_data_lake.py",
        "tests/test_data_lake.py",
        "tests/test_data_lake.py"
      ]
    },
    "app/services/domain/data_catalog_service.py": {
      "imports": [
        "uuid.UUID",
        "uuid.uuid4",
        "app.core.database.SessionLocal",
        "app.models.domain.data_catalog_model.DataCatalogModel"
      ],
      "imported_by": [
        "app/core/data_lake/data_pipeline_service.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_score_model.py",
        "app/models/domain/__init__.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_lake_model.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/__init__.py",
        "app/schemas/ad_account_health_schema.py",
        "app/services/domain/data_lake_service.py",
        "app/services/domain/data_lake_service.py",
        "tests/test_data_pipeline_service.py",
        "tests/test_data_lake.py",
        "tests/test_data_lake.py"
      ]
    },
    "tests/test_data_pipeline_service.py": {
      "imports": [
        "unittest",
        "unittest.mock.MagicMock",
        "sqlalchemy.orm.Session",
        "app.core.data_lake.data_pipeline_service.DataPipelineService",
        "app.core.data_lake.security_manager.SecurityManager",
        "app.models.domain.data_catalog_model.DataCatalogModel"
      ],
      "imported_by": [
        "app/core/data_lake/data_pipeline_service.py",
        "app/core/data_lake/data_pipeline_service.py",
        "app/core/data_lake/data_pipeline_service.py",
        "app/models/domain/__init__.py",
        "app/services/domain/data_catalog_service.py"
      ]
    },
    "tests/test_data_lake.py": {
      "imports": [
        "pytest",
        "uuid",
        "uuid.uuid4",
        "sqlalchemy.exc.IntegrityError",
        "sqlalchemy.exc.DataError",
        "app.models.domain.data_lake_model.DataLakeModel",
        "app.core.database.SessionLocal",
        "app.core.database.engine",
        "app.core.database.Base",
        "sqlalchemy"
      ],
      "imported_by": [
        "app/core/database.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_score_model.py",
        "app/models/domain/__init__.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_lake_model.py",
        "app/services/domain/data_lake_service.py",
        "app/services/domain/data_lake_service.py",
        "app/services/domain/data_lake_service.py",
        "app/services/domain/data_catalog_service.py",
        "app/services/domain/data_catalog_service.py",
        "tests/test_anomaly_detector.py",
        "tests/performance_test.py",
        "tests/integration_test.py",
        "tests/error_test.py",
        "tests/test_ad_predictor.py"
      ]
    },
    "tests/__init__.py": {
      "imports": [],
      "imported_by": []
    },
    "tests/test_anomaly_detector.py": {
      "imports": [
        "pytest",
        "app.models.ml.prediction.anomaly_detector.EnhancedAnomalyDetector"
      ],
      "imported_by": [
        "app/models/__init__.py",
        "app/models/ml/__init__.py",
        "app/models/ml/prediction/__init__.py",
        "tests/test_data_lake.py",
        "tests/performance_test.py",
        "tests/integration_test.py",
        "tests/error_test.py",
        "tests/error_test.py",
        "tests/test_ad_predictor.py"
      ]
    },
    "tests/performance_test.py": {
      "imports": [
        "pytest",
        "pandas",
        "numpy",
        "app.models.ml.prediction.ad_score_predictor.AdScorePredictor"
      ],
      "imported_by": [
        "app/models/__init__.py",
        "app/models/ml/__init__.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/__init__.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "tests/test_data_lake.py",
        "tests/test_anomaly_detector.py",
        "tests/integration_test.py",
        "tests/integration_test.py",
        "tests/integration_test.py",
        "tests/error_test.py",
        "tests/error_test.py",
        "tests/test_ad_predictor.py",
        "tests/test_ad_predictor.py",
        "tests/test_ad_predictor.py",
        "tests/test_ad_predictor.py"
      ]
    },
    "tests/integration_test.py": {
      "imports": [
        "pytest",
        "pandas",
        "numpy",
        "optuna",
        "app.models.ml.prediction.account_health_predictor.AdvancedHealthPredictor"
      ],
      "imported_by": [
        "app/models/__init__.py",
        "app/models/ml/__init__.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/__init__.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "tests/test_data_lake.py",
        "tests/test_anomaly_detector.py",
        "tests/performance_test.py",
        "tests/performance_test.py",
        "tests/performance_test.py",
        "tests/error_test.py",
        "tests/test_ad_predictor.py",
        "tests/test_ad_predictor.py",
        "tests/test_ad_predictor.py"
      ]
    },
    "tests/error_test.py": {
      "imports": [
        "pytest",
        "app.models.ml.prediction.ad_score_predictor.AdScorePredictor",
        "app.models.ml.prediction.anomaly_detector.EnhancedAnomalyDetector"
      ],
      "imported_by": [
        "app/models/__init__.py",
        "app/models/__init__.py",
        "app/models/ml/__init__.py",
        "app/models/ml/__init__.py",
        "app/models/ml/prediction/__init__.py",
        "app/models/ml/prediction/__init__.py",
        "tests/test_data_lake.py",
        "tests/test_anomaly_detector.py",
        "tests/test_anomaly_detector.py",
        "tests/performance_test.py",
        "tests/performance_test.py",
        "tests/integration_test.py",
        "tests/test_ad_predictor.py",
        "tests/test_ad_predictor.py"
      ]
    },
    "tests/test_ad_predictor.py": {
      "imports": [
        "pytest",
        "app.models.ml.prediction.ad_score_predictor.AdScorePredictor",
        "pandas",
        "numpy"
      ],
      "imported_by": [
        "app/models/__init__.py",
        "app/models/ml/__init__.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/__init__.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "tests/test_data_lake.py",
        "tests/test_anomaly_detector.py",
        "tests/performance_test.py",
        "tests/performance_test.py",
        "tests/performance_test.py",
        "tests/performance_test.py",
        "tests/integration_test.py",
        "tests/integration_test.py",
        "tests/integration_test.py",
        "tests/error_test.py",
        "tests/error_test.py"
      ]
    },
    "scripts/digest.py": {
      "imports": [
        "os",
        "json",
        "tiktoken",
        "typing.Dict",
        "typing.List",
        "typing.Any",
        "typing.Optional",
        "ast",
        "re",
        "logging",
        "time",
        "datetime.datetime"
      ],
      "imported_by": [
        "app/core/data_lake/security_manager.py",
        "app/core/data_lake/security_manager.py",
        "app/core/data_lake/data_pipeline_service.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_account_health_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ad_score_model.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/account_health_predictor.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/anomaly_detector.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/ml/prediction/ad_score_predictor.py",
        "app/models/domain/data_catalog_model.py",
        "app/models/domain/data_lake_model.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/ad_score_schema.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/__init__.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py",
        "app/schemas/ad_account_health_schema.py"
      ]
    }
  },
  "code_metrics": {
    "app/__init__.py": {
      "line_count": 1,
      "comment_ratio": 0.0,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/core/database.py": {
      "line_count": 59,
      "comment_ratio": 0.2033898305084746,
      "avg_function_length": 4,
      "complexity_estimate": 1
    },
    "app/core/__init__.py": {
      "line_count": 1,
      "comment_ratio": 0.0,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/core/config/__init__.py": {
      "line_count": 1,
      "comment_ratio": 0.0,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/core/ml/__init__.py": {
      "line_count": 1,
      "comment_ratio": 0.0,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/core/data_lake/security_manager.py": {
      "line_count": 181,
      "comment_ratio": 0.27071823204419887,
      "avg_function_length": 9,
      "complexity_estimate": 4
    },
    "app/core/data_lake/data_pipeline_service.py": {
      "line_count": 126,
      "comment_ratio": 0.1984126984126984,
      "avg_function_length": 17,
      "complexity_estimate": 10
    },
    "app/utils/__init__.py": {
      "line_count": 1,
      "comment_ratio": 0.0,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/models/__init__.py": {
      "line_count": 56,
      "comment_ratio": 0.10714285714285714,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/models/ad_account_health_model.py": {
      "line_count": 44,
      "comment_ratio": 0.06818181818181818,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/models/ad_score_model.py": {
      "line_count": 47,
      "comment_ratio": 0.0851063829787234,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/models/ml/__init__.py": {
      "line_count": 11,
      "comment_ratio": 0.09090909090909091,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/models/ml/prediction/account_health_predictor.py": {
      "line_count": 652,
      "comment_ratio": 0.12269938650306748,
      "avg_function_length": 18,
      "complexity_estimate": 55
    },
    "app/models/ml/prediction/anomaly_detector.py": {
      "line_count": 358,
      "comment_ratio": 0.10893854748603352,
      "avg_function_length": 18,
      "complexity_estimate": 15
    },
    "app/models/ml/prediction/__init__.py": {
      "line_count": 11,
      "comment_ratio": 0.09090909090909091,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/models/ml/prediction/ad_score_predictor.py": {
      "line_count": 374,
      "comment_ratio": 0.13101604278074866,
      "avg_function_length": 16,
      "complexity_estimate": 14
    },
    "app/models/domain/__init__.py": {
      "line_count": 5,
      "comment_ratio": 0.2,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/models/domain/data_catalog_model.py": {
      "line_count": 36,
      "comment_ratio": 0.1111111111111111,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/models/domain/data_lake_model.py": {
      "line_count": 56,
      "comment_ratio": 0.16071428571428573,
      "avg_function_length": 4,
      "complexity_estimate": 0
    },
    "app/schemas/ad_score_schema.py": {
      "line_count": 104,
      "comment_ratio": 0.04807692307692308,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/schemas/__init__.py": {
      "line_count": 63,
      "comment_ratio": 0.12698412698412698,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/schemas/ad_account_health_schema.py": {
      "line_count": 93,
      "comment_ratio": 0.043010752688172046,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/api/__init__.py": {
      "line_count": 1,
      "comment_ratio": 0.0,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/api/v1/__init__.py": {
      "line_count": 1,
      "comment_ratio": 0.0,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/api/v1/routes/__init__.py": {
      "line_count": 1,
      "comment_ratio": 0.0,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/services/__init__.py": {
      "line_count": 1,
      "comment_ratio": 0.0,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/services/ml/__init__.py": {
      "line_count": 1,
      "comment_ratio": 0.0,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "app/services/domain/data_lake_service.py": {
      "line_count": 16,
      "comment_ratio": 0.0,
      "avg_function_length": 12,
      "complexity_estimate": 0
    },
    "app/services/domain/data_catalog_service.py": {
      "line_count": 18,
      "comment_ratio": 0.0,
      "avg_function_length": 14,
      "complexity_estimate": 0
    },
    "tests/test_data_pipeline_service.py": {
      "line_count": 137,
      "comment_ratio": 0.24817518248175183,
      "avg_function_length": 6,
      "complexity_estimate": 1
    },
    "tests/test_data_lake.py": {
      "line_count": 140,
      "comment_ratio": 0.1357142857142857,
      "avg_function_length": 25,
      "complexity_estimate": 5
    },
    "tests/__init__.py": {
      "line_count": 1,
      "comment_ratio": 0.0,
      "avg_function_length": 0,
      "complexity_estimate": 0
    },
    "tests/test_anomaly_detector.py": {
      "line_count": 35,
      "comment_ratio": 0.05714285714285714,
      "avg_function_length": 9,
      "complexity_estimate": 0
    },
    "tests/performance_test.py": {
      "line_count": 66,
      "comment_ratio": 0.07575757575757576,
      "avg_function_length": 24,
      "complexity_estimate": 0
    },
    "tests/integration_test.py": {
      "line_count": 156,
      "comment_ratio": 0.12179487179487179,
      "avg_function_length": 36,
      "complexity_estimate": 5
    },
    "tests/error_test.py": {
      "line_count": 14,
      "comment_ratio": 0.0,
      "avg_function_length": 4,
      "complexity_estimate": 0
    },
    "tests/test_ad_predictor.py": {
      "line_count": 44,
      "comment_ratio": 0.022727272727272728,
      "avg_function_length": 11,
      "complexity_estimate": 0
    },
    "scripts/digest.py": {
      "line_count": 679,
      "comment_ratio": 0.09572901325478646,
      "avg_function_length": 29,
      "complexity_estimate": 95
    }
  },
  "files": [
    {
      "path": "app/models/ml/prediction/account_health_predictor.py",
      "metadata": {
        "path": "app/models/ml/prediction/account_health_predictor.py",
        "token_count": 5428,
        "size_bytes": 25756,
        "classes": [
          {
            "name": "EnhancedHealthEnsemble",
            "methods": [
              "__init__",
              "fit",
              "predict",
              "_build_private_nn",
              "_optimize_weights"
            ],
            "lineno": 59
          },
          {
            "name": "AdvancedHealthPredictor",
            "methods": [
              "__init__",
              "_get_feature_columns",
              "_build_enhanced_pipeline",
              "train",
              "_preprocess_training_data",
              "_objective",
              "predict_health_score",
              "_extract_enhanced_features",
              "_generate_shap_explanations",
              "_calculate_confidence",
              "_identify_risk_factors",
              "_evaluate_risk_level",
              "_generate_suggestions",
              "_process_historical_data",
              "_calculate_trend",
              "_calculate_velocity",
              "_rule_based_prediction",
              "save_model",
              "load_model",
              "_load_config",
              "_update_model_params",
              "_evaluate_model",
              "_calculate_privacy_budget",
              "_create_model"
            ],
            "lineno": 168
          }
        ],
        "functions": [
          {
            "name": "callback",
            "lineno": 45,
            "args": [
              "study",
              "trial"
            ]
          },
          {
            "name": "__init__",
            "lineno": 62,
            "args": [
              "self",
              "gb_params",
              "nn_params"
            ]
          },
          {
            "name": "fit",
            "lineno": 67,
            "args": [
              "self",
              "X",
              "y"
            ]
          },
          {
            "name": "predict",
            "lineno": 87,
            "args": [
              "self",
              "X"
            ]
          },
          {
            "name": "_build_private_nn",
            "lineno": 92,
            "args": [
              "self",
              "input_dim"
            ]
          },
          {
            "name": "_optimize_weights",
            "lineno": 129,
            "args": [
              "self",
              "X_val",
              "y_val"
            ]
          },
          {
            "name": "__init__",
            "lineno": 171,
            "args": [
              "self",
              "model_path",
              "config_path"
            ]
          },
          {
            "name": "_get_feature_columns",
            "lineno": 183,
            "args": [
              "self"
            ]
          },
          {
            "name": "_build_enhanced_pipeline",
            "lineno": 190,
            "args": [
              "self"
            ]
          },
          {
            "name": "train",
            "lineno": 201,
            "args": [
              "self",
              "training_data",
              "target_column",
              "save_path"
            ]
          },
          {
            "name": "_preprocess_training_data",
            "lineno": 247,
            "args": [
              "self",
              "training_data",
              "target_column"
            ]
          },
          {
            "name": "_objective",
            "lineno": 282,
            "args": [
              "self",
              "trial",
              "X",
              "y"
            ]
          },
          {
            "name": "predict_health_score",
            "lineno": 309,
            "args": [
              "self",
              "performance_metrics",
              "historical_data"
            ]
          },
          {
            "name": "_extract_enhanced_features",
            "lineno": 336,
            "args": [
              "self",
              "metrics",
              "historical_data"
            ]
          },
          {
            "name": "_generate_shap_explanations",
            "lineno": 361,
            "args": [
              "self",
              "shap_values"
            ]
          },
          {
            "name": "_calculate_confidence",
            "lineno": 371,
            "args": [
              "self",
              "features_df"
            ]
          },
          {
            "name": "_identify_risk_factors",
            "lineno": 382,
            "args": [
              "self",
              "features_df"
            ]
          },
          {
            "name": "_evaluate_risk_level",
            "lineno": 415,
            "args": [
              "self",
              "value",
              "thresholds"
            ]
          },
          {
            "name": "_generate_suggestions",
            "lineno": 427,
            "args": [
              "self",
              "risk_factors",
              "metrics"
            ]
          },
          {
            "name": "_process_historical_data",
            "lineno": 458,
            "args": [
              "self",
              "historical_data"
            ]
          },
          {
            "name": "_calculate_trend",
            "lineno": 469,
            "args": [
              "self",
              "series",
              "window"
            ]
          },
          {
            "name": "_calculate_velocity",
            "lineno": 478,
            "args": [
              "self",
              "series"
            ]
          },
          {
            "name": "_rule_based_prediction",
            "lineno": 484,
            "args": [
              "self",
              "metrics",
              "historical_data"
            ]
          },
          {
            "name": "save_model",
            "lineno": 517,
            "args": [
              "self",
              "save_path"
            ]
          },
          {
            "name": "load_model",
            "lineno": 541,
            "args": [
              "self",
              "model_path"
            ]
          },
          {
            "name": "_load_config",
            "lineno": 555,
            "args": [
              "self",
              "config_path"
            ]
          },
          {
            "name": "_update_model_params",
            "lineno": 590,
            "args": [
              "self",
              "params"
            ]
          },
          {
            "name": "_evaluate_model",
            "lineno": 601,
            "args": [
              "self",
              "X",
              "y"
            ]
          },
          {
            "name": "_calculate_privacy_budget",
            "lineno": 628,
            "args": [
              "self"
            ]
          },
          {
            "name": "_create_model",
            "lineno": 635,
            "args": [
              "self",
              "params"
            ]
          },
          {
            "name": "objective",
            "lineno": 138,
            "args": [
              "trial"
            ]
          }
        ],
        "imports": [
          "numpy",
          "pandas",
          "joblib",
          "logging",
          "tensorflow",
          "json",
          "os",
          "optuna",
          "shap",
          "datetime.datetime",
          "typing.Dict",
          "typing.Any",
          "typing.List",
          "typing.Optional",
          "sklearn.ensemble.GradientBoostingRegressor",
          "sklearn.impute.KNNImputer",
          "sklearn.preprocessing.StandardScaler",
          "sklearn.pipeline.Pipeline",
          "sklearn.model_selection.TimeSeriesSplit",
          "sklearn.model_selection.train_test_split",
          "sklearn.metrics.mean_squared_error",
          "sklearn.metrics.r2_score",
          "sklearn.metrics.mean_absolute_error",
          "sklearn.base.BaseEstimator",
          "sklearn.base.RegressorMixin",
          "sklearn.compose.ColumnTransformer",
          "keras.layers",
          "keras.models",
          "keras.callbacks",
          "keras.optimizers"
        ],
        "strings": [
          "\nEnhanced Account Health Predictor with ML Pipeline - Optimized Version\n",
          "\n    Creates a custom early-stopping callback for Optuna.\n    If the trial's value does not improve ",
          "Optimized ensemble with dynamic weights and privacy",
          "Enhanced predictor with explainability and privacy",
          "inf",
          "Differentially private neural network",
          "PSO-based weight optimization with early stopping",
          "Get required feature columns with defaults",
          "Modular preprocessing and modeling pipeline",
          "health_score",
          "Optuna-optimized training with automated feature engineering",
          "Enhanced preprocessing with missing value handling and feature engineering",
          "Optuna optimization objective with early stopping",
          "Enhanced prediction with uncertainty estimation",
          "Extract and validate features from input metrics",
          "Generate SHAP-based feature explanations",
          "Calculate prediction confidence with bootstrapping",
          "Enhanced risk detection with SHAP-based importance",
          "Dynamic risk evaluation with multi-level thresholds",
          "AI-powered suggestions with contextual awareness",
          "Temporal feature engineering from historical data",
          "Calculate linear trend over specified window",
          "Calculate rate of change over last 3 periods",
          "Fallback prediction with business rules",
          "Secure model serialization with privacy checks",
          "Secure model loading with validation",
          "Load configuration with defaults",
          "Update model parameters with optimized values",
          "Comprehensive model evaluation with privacy metrics",
          "Calculate differential privacy budget usage.\n        If 'privacy_spent' is not found in the model's ",
          "Create a model with the specified parameters",
          "callbacks",
          "validation_split",
          "inf",
          "ctr",
          "conversion_rate",
          "cost_per_conversion",
          "impressions",
          "spend",
          "revenue",
          "conversions",
          "engagement_cost_ratio",
          "revenue_per_impression",
          "conversion_cost_ratio",
          "ctr_conversion_interaction",
          "spend_efficiency",
          "gb_max_depth",
          "gb_learning_rate",
          "nn_epochs",
          "nn_batch_size",
          "risk_thresholds",
          "critical",
          "critical",
          "warning",
          "warning",
          "critical",
          "warning",
          "info",
          "7d_avg_ctr",
          "28d_trend_spend",
          "14d_conversion_rate",
          "revenue_velocity",
          "roi",
          "ctr",
          "health_score",
          "confidence_interval",
          "risk_factors",
          "optimization_suggestions",
          "prediction_timestamp",
          "gb_params",
          "nn_params",
          "risk_thresholds",
          "metrics",
          "privacy_report",
          "feature_importance",
          "training_size",
          "n_estimators",
          "max_depth",
          "epochs",
          "batch_size",
          "epochs",
          "epochs",
          "val_loss",
          "mean_squared_error",
          "callbacks",
          "validation_split",
          "w1",
          "w1",
          "clicks",
          "revenue",
          "conversions",
          "ctr",
          "conversion_rate",
          "revenue",
          "gb_max_depth",
          "gb_learning_rate",
          "nn_epochs",
          "nn_batch_size",
          "Model not trained yet. Call train() first.",
          "health_score",
          "confidence_interval",
          "risk_factors",
          "optimization_suggestions",
          "explanations",
          "prediction_timestamp",
          "trend_ctr",
          "trend_conversion",
          "Model must be trained before generating explanations.",
          "Model must be trained before calculating confidence.",
          "Model must be trained before identifying risk factors.",
          "critical_low",
          "critical_high",
          "warning_low",
          "warning_high",
          "critical",
          "Verify data quality",
          "Check system connectivity",
          "Review basic campaign metrics",
          "Cannot save an unfitted model",
          "pipeline.joblib",
          "w",
          "Model saved to ",
          "pipeline.joblib",
          "explainer.joblib",
          "is_fitted",
          "Model loaded from ",
          "n_estimators",
          "max_depth",
          "learning_rate",
          "epochs",
          "batch_size",
          "ctr",
          "conversion_rate",
          "cost_per_conversion",
          "max_depth",
          "learning_rate",
          "epochs",
          "batch_size",
          "mse",
          "mae",
          "r2",
          "epsilon_spent",
          "delta",
          "privacy_spent",
          "epochs",
          "mae",
          "callbacks",
          "imputer",
          "scaler",
          "ensemble",
          "minimize",
          "fc",
          "is_fitted",
          "ctr",
          "ctr",
          "conversion_rate",
          "conversion_rate",
          "critical_low",
          "critical_high",
          "warning_low",
          "warning_high",
          "risk_level",
          "warning",
          "Consider A/B testing ad creatives",
          "Review audience targeting parameters",
          "Optimize campaign scheduling",
          "spend",
          "revenue",
          "explainer.joblib",
          "config.json",
          "config",
          "is_fitted",
          "version",
          "1.0.0",
          "explainer.joblib",
          "config.json",
          "config",
          "critical_low",
          "warning_low",
          "critical_low",
          "warning_low",
          "warning_high",
          "critical_high",
          "r",
          "gb_max_depth",
          "gb_learning_rate",
          "nn_epochs",
          "nn_batch_size",
          "imputer",
          "scaler",
          "ensemble",
          "relu",
          "relu",
          "sigmoid",
          "Small training dataset size (",
          " samples). Consider using more data.",
          "ensemble",
          "Training failed: ",
          "Missing column ",
          " in training data. Filling with zeros.",
          "spend",
          "impressions",
          "spend",
          "spend",
          "Prediction error: ",
          "Missing required feature: ",
          "Feature ",
          " must be numeric, got ",
          "Feature ",
          " cannot be negative, got ",
          "risk_level",
          "ensemble",
          "ensemble",
          "ensemble",
          "Training data size (",
          ") is too small. Need at least 80 samples.",
          "ctr",
          "conversion_rate",
          "metric",
          "value",
          "shap_impact",
          "risk_level",
          "thresholds",
          "metric",
          "gb_params",
          "nn_params",
          "risk_level",
          "metric",
          "ensemble",
          "n_estimators",
          "max_depth",
          "learning_rate",
          "epochs",
          "batch_size",
          "shap_impact",
          "gb_max_depth",
          "gb_learning_rate",
          "nn_epochs",
          "nn_batch_size",
          "ctr",
          "conversion_rate",
          "_"
        ]
      },
      "metrics": {
        "line_count": 652,
        "comment_ratio": 0.12269938650306748,
        "avg_function_length": 18,
        "complexity_estimate": 55
      },
      "content": "\"\"\"\nEnhanced Account Health Predictor with ML Pipeline - Optimized Version\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport logging\nimport tensorflow as tf\nimport json\nimport os\nimport optuna\nimport shap\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import TimeSeriesSplit, train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.compose import ColumnTransformer\nfrom keras import layers, models, callbacks, optimizers\n\n# Remove the import that doesn't work in your environment:\n# from optuna.early_stopping import EarlyStoppingCallback\n\noptimizer = optimizers.Adam(learning_rate=0.001)\n\nlogger = logging.getLogger(__name__)\n\n# ------------------------------\n# Custom Early Stopping Callback\n# ------------------------------\ndef early_stopping_callback_factory(patience: int = 5, min_delta: float = 1e-4):\n    \"\"\"\n    Creates a custom early-stopping callback for Optuna.\n    If the trial's value does not improve by at least `min_delta`\n    for `patience` consecutive trials, the study will be stopped.\n    \"\"\"\n    best_score = float('inf')\n    best_trial_number = 0\n\n    def callback(study, trial):\n        nonlocal best_score, best_trial_number\n        current_value = trial.value\n        if current_value < best_score - min_delta:\n            best_score = current_value\n            best_trial_number = trial.number\n        elif trial.number - best_trial_number >= patience:\n            study.stop()\n\n    return callback\n\n# -----------------------------------------------\n# Begin EnhancedHealthEnsemble and AdvancedHealthPredictor\n# -----------------------------------------------\nclass EnhancedHealthEnsemble(RegressorMixin, BaseEstimator):\n    \"\"\"Optimized ensemble with dynamic weights and privacy\"\"\"\n    \n    def __init__(self, gb_params=None, nn_params=None):\n        self.gb_params = gb_params or {'n_estimators': 100, 'max_depth': 5}\n        self.nn_params = nn_params or {'epochs': 100, 'batch_size': 32}\n        self.ensemble_weights = None\n        \n    def fit(self, X, y):\n        # Split data for weight optimization\n        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Train GB model with more trees\n        self.gb = GradientBoostingRegressor(**self.gb_params).fit(X_train, y_train)\n        \n        # Build and compile the NN model\n        self.nn = self._build_private_nn(X.shape[1])\n        \n        # Increase number of epochs for the NN\n        if 'epochs' in self.nn_params and self.nn_params['epochs'] < 30:\n            self.nn_params['epochs'] = 30\n        \n        # Fit the NN model\n        self.nn.fit(X_train, y_train, **self.nn_params)\n        \n        # Optimize ensemble weights using validation set\n        self._optimize_weights(X_val, y_val)\n        return self\n    def predict(self, X):\n        gb_pred = self.gb.predict(X)\n        nn_pred = self.nn.predict(X, verbose=0).flatten()\n        return self.ensemble_weights[0] * gb_pred + self.ensemble_weights[1] * nn_pred\n\n    def _build_private_nn(self, input_dim):\n        \"\"\"Differentially private neural network\"\"\"\n        model = models.Sequential([\n            layers.Input(shape=(input_dim,)),\n            layers.Dense(64, activation='relu'),\n            layers.BatchNormalization(),\n            layers.Dropout(0.3),\n            layers.Dense(32, activation='relu'),\n            layers.BatchNormalization(),\n            layers.Dense(1, activation='sigmoid')\n        ])\n        \n        # Add early stopping callback\n        early_stopping = callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n        \n        # Compile the model with appropriate loss and metrics\n        model.compile(\n            optimizer=optimizer,\n            loss='mean_squared_error',\n            metrics=['mae']\n        )\n        \n        # Add callbacks to nn_params\n        if 'callbacks' not in self.nn_params:\n            self.nn_params['callbacks'] = []\n        self.nn_params['callbacks'].append(early_stopping)\n        \n        # Add validation split if not present\n        if 'validation_split' not in self.nn_params:\n            self.nn_params['validation_split'] = 0.2\n            \n        return model\n\n    def _optimize_weights(self, X_val, y_val):\n        \"\"\"PSO-based weight optimization with early stopping\"\"\"\n        gb_pred = self.gb.predict(X_val)\n        nn_pred = self.nn.predict(X_val, verbose=0).flatten()\n        \n        best_score = float('inf')\n        patience = 5\n        no_improve_count = 0\n        \n        def objective(trial):\n            nonlocal best_score, no_improve_count\n            w1 = trial.suggest_float('w1', 0, 1)\n            w2 = 1 - w1\n            combined = w1 * gb_pred + w2 * nn_pred\n            score = mean_squared_error(y_val, combined)\n            \n            if score < best_score:\n                best_score = score\n                no_improve_count = 0\n            else:\n                no_improve_count += 1\n                \n            if no_improve_count >= patience:\n                trial.study.stop()\n                \n            return score\n        \n        study = optuna.create_study()\n        study.optimize(objective, n_trials=20)  # Reduced number of trials\n        \n        # Set default weights if optimization fails\n        if len(study.trials) == 0:\n            self.ensemble_weights = [0.5, 0.5]\n        else:\n            w1 = study.best_params['w1']\n            self.ensemble_weights = [w1, 1 - w1]\n        \n        return self\n\nclass AdvancedHealthPredictor:\n    \"\"\"Enhanced predictor with explainability and privacy\"\"\"\n    \n    def __init__(self, model_path=None, config_path=None):\n        self.model_path = model_path\n        self.config = self._load_config(config_path)\n        self.feature_columns = self._get_feature_columns()\n        self.pipeline = self._build_enhanced_pipeline()\n        self.explainer = None\n        self.loaded = False\n        self.is_fitted = False  # Add this line\n\n        if model_path and os.path.exists(model_path):\n            self.load_model(model_path)\n\n    def _get_feature_columns(self):\n        \"\"\"Get required feature columns with defaults\"\"\"\n        return [\n            'ctr', 'conversion_rate', 'cost_per_conversion',\n            'impressions', 'spend', 'revenue', 'conversions'\n        ]\n\n    def _build_enhanced_pipeline(self):\n        \"\"\"Modular preprocessing and modeling pipeline\"\"\"\n        return Pipeline([\n            ('imputer', KNNImputer(n_neighbors=5)),\n            ('scaler', StandardScaler()),\n            ('ensemble', EnhancedHealthEnsemble(\n                gb_params=self.config.get('gb_params', {}),\n                nn_params=self.config.get('nn_params', {})\n            ))\n        ])\n\n    def train(self, training_data, target_column='health_score', save_path=None):\n        \"\"\"Optuna-optimized training with automated feature engineering\"\"\"\n        try:\n            X, y = self._preprocess_training_data(training_data, target_column)\n            \n            # Ensure minimum training data size\n            if len(X) < 100:\n                logger.warning(f\"Small training dataset size ({len(X)} samples). Consider using more data.\")\n                if len(X) < 80:  # Enforce minimum size requirement\n                    raise ValueError(f\"Training data size ({len(X)}) is too small. Need at least 80 samples.\")\n            \n            # Split data for training and validation\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n            \n            # Hyperparameter optimization with early stopping\n            study = optuna.create_study(direction='minimize')\n            study.optimize(\n                lambda trial: self._objective(trial, X_train, y_train),\n                n_trials=20,  # Reduced number of trials\n                callbacks=[early_stopping_callback_factory(patience=5, min_delta=1e-4)]\n            )\n            \n            # Update model with best parameters\n            if study.best_params:\n                self._update_model_params(study.best_params)\n            \n            # Fit the pipeline\n            self.pipeline.fit(X_train, y_train)\n            self.is_fitted = True\n            \n            # Initialize explainer\n            self.explainer = shap.Explainer(self.pipeline.named_steps['ensemble'].gb)\n            \n            # Evaluate on full dataset\n            val_results = self._evaluate_model(X, y)\n            \n            if save_path:\n                self.save_model(save_path)\n                \n            return val_results\n        \n        except Exception as e:\n            logger.error(f\"Training failed: {str(e)}\")\n            self.is_fitted = False  # Ensure is_fitted is False on failure\n            raise\n        \n    def _preprocess_training_data(self, training_data, target_column):\n        \"\"\"Enhanced preprocessing with missing value handling and feature engineering\"\"\"\n        df = pd.DataFrame(training_data)\n        \n        # Ensure all required columns exist with default values\n        for col in self.feature_columns:\n            if col not in df.columns:\n                df[col] = 0.0\n                logger.warning(f\"Missing column {col} in training data. Filling with zeros.\")\n        \n        # Calculate derived features\n        df['engagement_cost_ratio'] = df['clicks'] / df['spend'].replace(0, 1e-6)\n        df['revenue_per_impression'] = df['revenue'] / df['impressions'].replace(0, 1e-6)\n        df['conversion_cost_ratio'] = df['conversions'] / df['spend'].replace(0, 1e-6)\n        \n        # Add interaction terms\n        df['ctr_conversion_interaction'] = df['ctr'] * df['conversion_rate']\n        df['spend_efficiency'] = df['revenue'] / df['spend'].replace(0, 1e-6)\n        \n        # Handle outliers\n        for col in self.feature_columns:\n            if col in df.columns and df[col].dtype.kind in 'fc':  # float or complex\n                q1 = df[col].quantile(0.25)\n                q3 = df[col].quantile(0.75)\n                iqr = q3 - q1\n                lower_bound = q1 - 1.5 * iqr\n                upper_bound = q3 + 1.5 * iqr\n                df[col] = df[col].clip(lower_bound, upper_bound)\n        \n        # Split features and target\n        X = df[self.feature_columns]\n        y = df[target_column]\n        \n        return X, y\n\n    def _objective(self, trial, X, y):\n        \"\"\"Optuna optimization objective with early stopping\"\"\"\n        params = {\n            'gb_max_depth': trial.suggest_int('gb_max_depth', 3, 8),\n            'gb_learning_rate': trial.suggest_float('gb_learning_rate', 1e-3, 0.1, log=True),\n            'nn_epochs': trial.suggest_int('nn_epochs', 10, 50),\n            'nn_batch_size': trial.suggest_categorical('nn_batch_size', [16, 32, 64])\n        }\n        \n        # Use smaller cross-validation\n        tscv = TimeSeriesSplit(n_splits=3)\n        scores = []\n        \n        for train_idx, val_idx in tscv.split(X):\n            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n            \n            model = self._create_model(params)\n            model.fit(X_train, y_train)\n            scores.append(mean_squared_error(y_val, model.predict(X_val)))\n            \n            # Early stopping within cross-validation\n            if len(scores) > 1 and scores[-1] > scores[-2] * 1.1:  # 10% worse\n                break\n                \n        return np.mean(scores)\n\n    def predict_health_score(self, performance_metrics, historical_data=None):\n        \"\"\"Enhanced prediction with uncertainty estimation\"\"\"\n        if not hasattr(self, 'is_fitted') or not self.is_fitted:\n            raise ValueError(\"Model not trained yet. Call train() first.\")\n            \n        try:\n            features = self._extract_enhanced_features(performance_metrics, historical_data)\n            features_df = pd.DataFrame([features], columns=self.feature_columns)\n            \n            # Generate predictions with confidence intervals\n            base_pred = self.pipeline.predict(features_df)[0]\n            shap_values = self.explainer(features_df)\n            \n            return {\n                'health_score': float(np.clip(base_pred, 0, 1)),\n                'confidence_interval': self._calculate_confidence(features_df),\n                'risk_factors': self._identify_risk_factors(features_df),\n                'optimization_suggestions': self._generate_suggestions(\n                    self._identify_risk_factors(features_df), performance_metrics\n                ),\n                'explanations': self._generate_shap_explanations(shap_values),\n                'prediction_timestamp': datetime.now().isoformat()\n            }\n        except Exception as e:\n            logger.error(f\"Prediction error: {str(e)}\")\n            return self._rule_based_prediction(performance_metrics, historical_data)\n\n    def _extract_enhanced_features(self, metrics, historical_data=None):\n        \"\"\"Extract and validate features from input metrics\"\"\"\n        features = {}\n        \n        # Ensure all required features are present\n        for col in self.feature_columns:\n            if col not in metrics:\n                raise ValueError(f\"Missing required feature: {col}\")\n            features[col] = metrics[col]\n            \n        # Add derived features if historical data is available\n        if historical_data is not None and len(historical_data) > 0:\n            df = pd.DataFrame(historical_data)\n            features['trend_ctr'] = df['ctr'].mean() if 'ctr' in df else features['ctr']\n            features['trend_conversion'] = df['conversion_rate'].mean() if 'conversion_rate' in df else features['conversion_rate']\n        \n        # Validate feature values\n        for col, value in features.items():\n            if not isinstance(value, (int, float)):\n                raise ValueError(f\"Feature {col} must be numeric, got {type(value)}\")\n            if value < 0:\n                raise ValueError(f\"Feature {col} cannot be negative, got {value}\")\n                \n        return features\n\n    def _generate_shap_explanations(self, shap_values):\n        \"\"\"Generate SHAP-based feature explanations\"\"\"\n        if not self.is_fitted:\n            raise RuntimeError(\"Model must be trained before generating explanations.\")\n            \n        feature_importance = {}\n        for i, feature in enumerate(self.feature_columns):\n            feature_importance[feature] = float(np.abs(shap_values.values[:, i]).mean())\n        return feature_importance\n\n    def _calculate_confidence(self, features_df):\n        \"\"\"Calculate prediction confidence with bootstrapping\"\"\"\n        if not self.is_fitted:\n            raise RuntimeError(\"Model must be trained before calculating confidence.\")\n            \n        preds = []\n        for _ in range(100):\n            sample = features_df.sample(frac=1, replace=True)\n            preds.append(self.pipeline.predict(sample)[0])\n        return (np.mean(preds), np.std(preds))\n\n    def _identify_risk_factors(self, features_df):\n        \"\"\"Enhanced risk detection with SHAP-based importance\"\"\"\n        if not self.is_fitted:\n            raise RuntimeError(\"Model must be trained before identifying risk factors.\")\n            \n        risk_factors = []\n        feature_values = features_df.iloc[0].to_dict()\n        thresholds = self.config.get('risk_thresholds', {})\n        \n        # Get SHAP values for explanation\n        shap_values = self.explainer(features_df)\n        \n        for i, feature in enumerate(self.feature_columns):\n            value = feature_values.get(feature, 0)\n            shap_impact = shap_values.values[0][i]\n            \n            if feature in thresholds:\n                threshold_config = thresholds[feature]\n                risk_level = self._evaluate_risk_level(value, threshold_config)\n                \n                if risk_level:\n                    risk_factors.append({\n                        'metric': feature,\n                        'value': value,\n                        'shap_impact': float(shap_impact),\n                        'risk_level': risk_level,\n                        'thresholds': threshold_config\n                    })\n        \n        return sorted(risk_factors, \n                     key=lambda x: (abs(x['shap_impact']), x['risk_level']), \n                     reverse=True)[:5]\n\n    def _evaluate_risk_level(self, value, thresholds):\n        \"\"\"Dynamic risk evaluation with multi-level thresholds\"\"\"\n        if 'critical_low' in thresholds and value < thresholds['critical_low']:\n            return 'critical'\n        if 'critical_high' in thresholds and value > thresholds['critical_high']:\n            return 'critical'\n        if 'warning_low' in thresholds and value < thresholds['warning_low']:\n            return 'warning'\n        if 'warning_high' in thresholds and value > thresholds['warning_high']:\n            return 'warning'\n        return None\n\n    def _generate_suggestions(self, risk_factors, metrics):\n        \"\"\"AI-powered suggestions with contextual awareness\"\"\"\n        suggestions = []\n        priority_map = {\n            'critical': 3,\n            'warning': 2,\n            'info': 1\n        }\n        \n        # Generate suggestions based on risk factors\n        for risk in risk_factors:\n            if risk['risk_level'] == 'critical':\n                suggestions.extend(\n                    self._get_critical_suggestions(risk['metric'], metrics))\n            elif risk['risk_level'] == 'warning':\n                suggestions.extend(\n                    self._get_warning_suggestions(risk['metric'], metrics))\n        \n        # Add general optimization suggestions\n        if not suggestions:\n            suggestions.extend([\n                \"Consider A/B testing ad creatives\",\n                \"Review audience targeting parameters\",\n                \"Optimize campaign scheduling\"\n            ])\n            \n        return sorted(list(set(suggestions)),\n                     key=lambda x: priority_map.get(\n                         x.split('_')[0].lower(), 1),\n                     reverse=True)[:5]\n\n    def _process_historical_data(self, historical_data):\n        \"\"\"Temporal feature engineering from historical data\"\"\"\n        df = pd.DataFrame(historical_data)\n        temporal_features = {\n            '7d_avg_ctr': df['ctr'].rolling(7).mean().iloc[-1],\n            '28d_trend_spend': self._calculate_trend(df['spend'], 28),\n            '14d_conversion_rate': df['conversion_rate'].rolling(14).mean().iloc[-1],\n            'revenue_velocity': self._calculate_velocity(df['revenue'])\n        }\n        return {k: v if not pd.isna(v) else 0 for k, v in temporal_features.items()}\n\n    def _calculate_trend(self, series, window):\n        \"\"\"Calculate linear trend over specified window\"\"\"\n        if len(series) < window:\n            return 0\n        x = np.arange(window)\n        y = series.values[-window:]\n        slope = np.polyfit(x, y, 1)[0]\n        return slope / np.mean(y) if np.mean(y) != 0 else 0\n\n    def _calculate_velocity(self, series):\n        \"\"\"Calculate rate of change over last 3 periods\"\"\"\n        if len(series) < 4:\n            return 0\n        return (series.iloc[-1] - series.iloc[-4]) / 3\n\n    def _rule_based_prediction(self, metrics, historical_data=None):\n        \"\"\"Fallback prediction with business rules\"\"\"\n        score = 0.7  # Base score\n        adjustments = []\n        \n        # ROI-based adjustment\n        roi = metrics.get('roi', 0)\n        if roi > 1.5:\n            score += 0.15\n        elif roi < 0:\n            score -= 0.2\n            \n        # CTR-based adjustment\n        ctr = metrics.get('ctr', 0)\n        if ctr > 0.05:\n            score += 0.1\n        elif ctr < 0.01:\n            score -= 0.15\n            \n        # Apply bounds and return\n        final_score = np.clip(score, 0, 1)\n        return {\n            'health_score': float(final_score),\n            'confidence_interval': (final_score - 0.1, final_score + 0.1),\n            'risk_factors': [],\n            'optimization_suggestions': [\n                \"Verify data quality\",\n                \"Check system connectivity\",\n                \"Review basic campaign metrics\"\n            ],\n            'prediction_timestamp': datetime.now().isoformat()\n        }\n\n    def save_model(self, save_path):\n        \"\"\"Secure model serialization with privacy checks\"\"\"\n        if not self.is_fitted:\n            raise RuntimeError(\"Cannot save an unfitted model\")\n            \n        os.makedirs(save_path, exist_ok=True)\n        \n        # Save pipeline without training data\n        joblib.dump(self.pipeline, os.path.join(save_path, 'pipeline.joblib'))\n        \n        # Save SHAP explainer separately\n        if self.explainer:\n            joblib.dump(self.explainer, os.path.join(save_path, 'explainer.joblib'))\n            \n        # Save configuration and fitted state\n        with open(os.path.join(save_path, 'config.json'), 'w') as f:\n            json.dump({\n                'config': self.config,\n                'is_fitted': self.is_fitted,\n                'version': '1.0.0'\n            }, f)\n            \n        logger.info(f\"Model saved to {save_path}\")\n\n    def load_model(self, model_path):\n        \"\"\"Secure model loading with validation\"\"\"\n        self.pipeline = joblib.load(os.path.join(model_path, 'pipeline.joblib'))\n        \n        if os.path.exists(os.path.join(model_path, 'explainer.joblib')):\n            self.explainer = joblib.load(os.path.join(model_path, 'explainer.joblib'))\n            \n        with open(os.path.join(model_path, 'config.json')) as f:\n            saved_state = json.load(f)\n            self.config.update(saved_state['config'])\n            self.is_fitted = saved_state['is_fitted']\n            \n        logger.info(f\"Model loaded from {model_path}\")\n\n    def _load_config(self, config_path=None):\n        \"\"\"Load configuration with defaults\"\"\"\n        default_config = {\n            'gb_params': {\n                'n_estimators': 100,\n                'max_depth': 5,\n                'learning_rate': 0.1\n            },\n            'nn_params': {\n                'epochs': 100,\n                'batch_size': 32\n            },\n            'risk_thresholds': {\n                'ctr': {\n                    'critical_low': 0.01,\n                    'warning_low': 0.02\n                },\n                'conversion_rate': {\n                    'critical_low': 0.005,\n                    'warning_low': 0.01\n                },\n                'cost_per_conversion': {\n                    'warning_high': 100,\n                    'critical_high': 200\n                }\n            }\n        }\n        \n        if config_path and os.path.exists(config_path):\n            with open(config_path, 'r') as f:\n                loaded_config = json.load(f)\n                return {**default_config, **loaded_config}\n        \n        return default_config\n\n    def _update_model_params(self, params):\n        \"\"\"Update model parameters with optimized values\"\"\"\n        self.pipeline.named_steps['ensemble'].gb_params.update({\n            'max_depth': params['gb_max_depth'],\n            'learning_rate': params['gb_learning_rate']\n        })\n        self.pipeline.named_steps['ensemble'].nn_params.update({\n            'epochs': params['nn_epochs'],\n            'batch_size': params['nn_batch_size']\n        })\n\n    def _evaluate_model(self, X, y):\n        \"\"\"Comprehensive model evaluation with privacy metrics\"\"\"\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Don't retrain if already fitted\n        if not self.is_fitted:\n            self.pipeline.fit(X_train, y_train)\n            self.is_fitted = True\n        \n        preds = self.pipeline.predict(X_test)\n        \n        return {\n            'metrics': {\n                'mse': mean_squared_error(y_test, preds),\n                'mae': mean_absolute_error(y_test, preds),\n                'r2': r2_score(y_test, preds)\n            },\n            'privacy_report': {\n                'epsilon_spent': self._calculate_privacy_budget(),\n                'delta': 1e-5\n            },\n            'feature_importance': dict(zip(\n                self.feature_columns,\n                self.pipeline.named_steps['ensemble'].gb.feature_importances_\n            )),\n            'training_size': len(X_train)  # Add training size\n        }\n    def _calculate_privacy_budget(self):\n        \"\"\"Calculate differential privacy budget usage.\n        If 'privacy_spent' is not found in the model's history, return 0.\n        \"\"\"\n        history_dict = self.pipeline.named_steps['ensemble'].nn.history.history\n        return history_dict.get('privacy_spent', [0])[-1]\n\n    def _create_model(self, params):\n        \"\"\"Create a model with the specified parameters\"\"\"\n        return Pipeline([\n            ('imputer', KNNImputer(n_neighbors=5)),\n            ('scaler', StandardScaler()),\n            ('ensemble', EnhancedHealthEnsemble(\n                gb_params={\n                    'n_estimators': 100,\n                    'max_depth': params['gb_max_depth'],\n                    'learning_rate': params['gb_learning_rate']\n                },\n                nn_params={\n                    'epochs': params['nn_epochs'],\n                    'batch_size': params['nn_batch_size']\n                }\n            ))\n        ])\n"
    },
    {
      "path": "app/models/ml/prediction/ad_score_predictor.py",
      "metadata": {
        "path": "app/models/ml/prediction/ad_score_predictor.py",
        "token_count": 3190,
        "size_bytes": 15176,
        "classes": [
          {
            "name": "AttentionLayer",
            "methods": [
              "__init__",
              "forward"
            ],
            "lineno": 22
          },
          {
            "name": "AdScorePredictor",
            "methods": [
              "__init__",
              "_default_config",
              "_build_dynamic_nn",
              "_build_feature_pipeline",
              "forward",
              "fit",
              "_get_feature_dims",
              "_calibrate_ensemble",
              "_learn_health_weights",
              "predict",
              "_generate_explanations",
              "_fallback_prediction",
              "_get_model_metadata",
              "_get_health_impact",
              "_train_torch_model",
              "_optimize_hyperparameters",
              "_torch_predict_proba"
            ],
            "lineno": 37
          }
        ],
        "functions": [
          {
            "name": "__init__",
            "lineno": 24,
            "args": [
              "self",
              "input_dim"
            ]
          },
          {
            "name": "forward",
            "lineno": 33,
            "args": [
              "self",
              "x"
            ]
          },
          {
            "name": "__init__",
            "lineno": 40,
            "args": [
              "self",
              "config"
            ]
          },
          {
            "name": "_default_config",
            "lineno": 64,
            "args": [
              "self"
            ]
          },
          {
            "name": "_build_dynamic_nn",
            "lineno": 92,
            "args": [
              "self"
            ]
          },
          {
            "name": "_build_feature_pipeline",
            "lineno": 114,
            "args": [
              "self"
            ]
          },
          {
            "name": "forward",
            "lineno": 125,
            "args": [
              "self",
              "x"
            ]
          },
          {
            "name": "fit",
            "lineno": 141,
            "args": [
              "self",
              "X",
              "y",
              "account_health"
            ]
          },
          {
            "name": "_get_feature_dims",
            "lineno": 180,
            "args": [
              "self",
              "X"
            ]
          },
          {
            "name": "_calibrate_ensemble",
            "lineno": 188,
            "args": [
              "self",
              "X_val",
              "y_val"
            ]
          },
          {
            "name": "_learn_health_weights",
            "lineno": 205,
            "args": [
              "self",
              "X",
              "y",
              "health_data"
            ]
          },
          {
            "name": "predict",
            "lineno": 219,
            "args": [
              "self",
              "features"
            ]
          },
          {
            "name": "_generate_explanations",
            "lineno": 264,
            "args": [
              "self",
              "processed_input"
            ]
          },
          {
            "name": "_fallback_prediction",
            "lineno": 275,
            "args": [
              "self",
              "features",
              "error"
            ]
          },
          {
            "name": "_get_model_metadata",
            "lineno": 285,
            "args": [
              "self"
            ]
          },
          {
            "name": "_get_health_impact",
            "lineno": 294,
            "args": [
              "self",
              "adjustments"
            ]
          },
          {
            "name": "_train_torch_model",
            "lineno": 301,
            "args": [
              "self",
              "X_train",
              "y_train"
            ]
          },
          {
            "name": "_optimize_hyperparameters",
            "lineno": 355,
            "args": [
              "self",
              "X",
              "y"
            ]
          },
          {
            "name": "_torch_predict_proba",
            "lineno": 368,
            "args": [
              "self",
              "X"
            ]
          },
          {
            "name": "objective",
            "lineno": 195,
            "args": [
              "weights"
            ]
          }
        ],
        "imports": [
          "torch",
          "xgboost",
          "shap",
          "numpy",
          "pandas",
          "logging",
          "typing.Dict",
          "typing.Any",
          "typing.List",
          "typing.Optional",
          "typing.Tuple",
          "datetime.datetime",
          "sklearn.compose.ColumnTransformer",
          "sklearn.preprocessing.StandardScaler",
          "sklearn.preprocessing.OneHotEncoder",
          "sklearn.feature_extraction.text.TfidfVectorizer",
          "sklearn.metrics.mean_squared_error",
          "sklearn.metrics.r2_score",
          "sklearn.metrics.mean_absolute_error",
          "sklearn.model_selection.GridSearchCV",
          "sklearn.model_selection.train_test_split",
          "torch.utils.data.DataLoader",
          "torch.utils.data.TensorDataset",
          "torch.nn",
          "torch.optim",
          "scipy.optimize.minimize"
        ],
        "strings": [
          "Advanced Ad Scoring System with Multi-Modal Integration - Enhanced Version",
          "Attention mechanism for text features",
          "Enhanced multi-modal ad scoring predictor",
          "4.0.0",
          "Default configuration for the predictor",
          "Build neural network with dynamic architecture",
          "Enhanced feature processing pipeline",
          "Enhanced forward pass with dynamic splits",
          "Enhanced training with dynamic components",
          "Determine feature dimensions from fitted preprocessor",
          "Optimize ensemble weights using PSO",
          "Learn health adjustment weights using linear regression",
          "Generate predictions with confidence intervals",
          "Enhanced explanations with interaction effects",
          "Graceful fallback with partial results",
          "Get model metadata for tracking",
          "Calculate health impact on prediction",
          "Train PyTorch model with enhanced error handling",
          "Optimize hyperparameters using grid search",
          "word_count",
          "sentiment_score",
          "complexity_score",
          "target_audience_match",
          "spend_efficiency",
          "platform",
          "ad_type",
          "content_category",
          "ad_content",
          "xgb_params",
          "nn_params",
          "feature_params",
          "ensemble_params",
          "toarray",
          "toarray",
          "num",
          "cat",
          "text",
          "shap_values",
          "interactions",
          "feature_importance",
          "summary_plot",
          "engagement_score",
          "confidence",
          "error",
          "fallback",
          "model_metadata",
          "version",
          "timestamp",
          "ensemble_weights",
          "health_weights",
          "raw_adjustment",
          "relative_impact",
          "max_depth",
          "learning_rate",
          "n_estimators",
          "cuda",
          "cpu",
          "n_estimators",
          "max_depth",
          "learning_rate",
          "objective",
          "eval_metric",
          "use_label_encoder",
          "random_state",
          "binary:logistic",
          "logloss",
          "batch_size",
          "learning_rate",
          "epochs",
          "patience",
          "max_text_features",
          "text_ngram_range",
          "calibration_size",
          "min_weight",
          "numerical",
          "categorical",
          "text",
          "combined",
          "Training completed with version ",
          "risk_factors",
          "spend_efficiency",
          "audience_health",
          "score",
          "confidence",
          "feature_impacts",
          "prediction_time",
          "Unknown error",
          "inf",
          "neg_log_loss",
          "xgb_params",
          "num",
          "cat",
          "text",
          "ad_content",
          "type",
          "fun",
          "eq",
          "Model must be trained before making predictions",
          "score",
          "confidence",
          "error",
          "fallback",
          "bar",
          "min",
          "epochs",
          "numerical",
          "categorical",
          "text",
          "combined",
          "word_count",
          "sentiment_score",
          "complexity_score",
          "target_audience_match",
          "spend_efficiency",
          "platform",
          "ad_type",
          "content_category",
          "num",
          "num",
          "Prediction error: ",
          "batch_size",
          "learning_rate",
          "nn_params",
          "PyTorch training error: ",
          "ignore",
          "num",
          "cat",
          "text",
          "Missing feature ",
          " in prediction input. Using default value 0.0",
          "nn_params",
          "nn_params",
          "patience",
          "Epoch ",
          ", Loss: ",
          "cat",
          "text",
          "nn_params",
          "Early stopping at epoch ",
          "num",
          "cat",
          "text",
          ".4f"
        ]
      },
      "metrics": {
        "line_count": 374,
        "comment_ratio": 0.13101604278074866,
        "avg_function_length": 16,
        "complexity_estimate": 14
      },
      "content": "\"\"\"Advanced Ad Scoring System with Multi-Modal Integration - Enhanced Version\"\"\"\nimport torch\nimport xgboost as xgb\nimport shap\nimport numpy as np\nimport pandas as pd\nimport logging\nfrom typing import Dict, Any, List, Optional, Tuple\nfrom datetime import datetime\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch import nn, optim\nfrom scipy.optimize import minimize\n\n\nlogger = logging.getLogger(__name__)\n\nclass AttentionLayer(nn.Module):\n    \"\"\"Attention mechanism for text features\"\"\"\n    def __init__(self, input_dim: int):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.Tanh(),\n            nn.Linear(64, 1),\n            nn.Softmax(dim=1)\n        )\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        attn_weights = self.attention(x)\n        return (x * attn_weights).sum(dim=1)    \n\nclass AdScorePredictor(nn.Module):\n    \"\"\"Enhanced multi-modal ad scoring predictor\"\"\"\n    \n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        super().__init__()\n        self.config = config or self._default_config()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Model components\n        self.tree_model = xgb.XGBClassifier(**self.config['xgb_params'])\n        self.nn_model = None  # Built dynamically during training\n        self.preprocessor = self._build_feature_pipeline()\n        self.explainer = None\n        self.version = \"4.0.0\"\n        self.ensemble_weights = None\n        \n        # Feature columns\n        self.feature_columns = [\n            'word_count', 'sentiment_score', 'complexity_score',\n            'target_audience_match', 'spend_efficiency',\n            'platform', 'ad_type', 'content_category', 'ad_content'\n        ]\n        \n        # Health integration\n        self.health_weights = None\n        self.feature_dims = {}  # Stores num/cat/text feature dimensions\n\n    def _default_config(self) -> Dict[str, Any]:\n        \"\"\"Default configuration for the predictor\"\"\"\n        return {\n            'xgb_params': {\n                'n_estimators': 200,\n                'max_depth': 6,\n                'learning_rate': 0.1,\n                'objective': 'binary:logistic',\n                'eval_metric': 'logloss',\n                'use_label_encoder': False,\n                'random_state': 42\n            },\n            'nn_params': {\n                'batch_size': 64,\n                'learning_rate': 0.001,\n                'epochs': 100,\n                'patience': 10\n            },\n            'feature_params': {\n                'max_text_features': 500,\n                'text_ngram_range': (1, 2)\n            },\n            'ensemble_params': {\n                'calibration_size': 0.2,\n                'min_weight': 0.1\n            }\n        }\n\n    def _build_dynamic_nn(self) -> None:\n        \"\"\"Build neural network with dynamic architecture\"\"\"\n        self.nn_model = nn.ModuleDict({\n            'numerical': nn.Sequential(\n                nn.Linear(self.feature_dims['num'], 64),\n                nn.ReLU(),\n                nn.BatchNorm1d(64)),\n            'categorical': nn.Sequential(\n                nn.Linear(self.feature_dims['cat'], 64),\n                nn.ReLU(),\n                nn.BatchNorm1d(64)),\n            'text': nn.Sequential(\n                nn.Linear(self.feature_dims['text'], 64),\n                nn.ReLU(),\n                nn.BatchNorm1d(64)),\n            'combined': nn.Sequential(\n                nn.Linear(64 * 3, 128),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(128, 2))\n        }).to(self.device)\n\n    def _build_feature_pipeline(self) -> ColumnTransformer:\n        \"\"\"Enhanced feature processing pipeline\"\"\"\n        return ColumnTransformer([\n            ('num', StandardScaler(), [\n                'word_count', 'sentiment_score', 'complexity_score',\n                'target_audience_match', 'spend_efficiency'\n            ]),\n            ('cat', OneHotEncoder(handle_unknown='ignore'), ['platform', 'ad_type', 'content_category']),\n            ('text', TfidfVectorizer(max_features=500), 'ad_content')\n        ])\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Enhanced forward pass with dynamic splits\"\"\"\n        # Split input tensor into feature groups\n        num = x[:, :self.feature_dims['num']]\n        cat = x[:, self.feature_dims['num']:self.feature_dims['num']+self.feature_dims['cat']]\n        text = x[:, -self.feature_dims['text']:]\n        \n        # Process each modality\n        num_out = self.nn_model['numerical'](num)\n        cat_out = self.nn_model['categorical'](cat)\n        text_out = self.nn_model['text'](text)\n        \n        # Combine and final processing\n        combined = torch.cat([num_out, cat_out, text_out], dim=1)\n        return self.nn_model['combined'](combined)\n\n    def fit(self, X: pd.DataFrame, y: pd.Series, \n           account_health: Optional[pd.DataFrame] = None):\n        \"\"\"Enhanced training with dynamic components\"\"\"\n        # Preprocess data and determine feature dimensions\n        X_processed = self.preprocessor.fit_transform(X)\n        self._get_feature_dims(X)\n        \n        # Build dynamic neural network\n        self._build_dynamic_nn()\n        \n        # Hyperparameter tuning\n        self._optimize_hyperparameters(X_processed, y)\n        \n        # Split data for ensemble calibration\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_processed, y, test_size=0.2, random_state=42)\n        \n        # Convert sparse matrices to dense for PyTorch\n        if hasattr(X_train, 'toarray'):\n            X_train = X_train.toarray()\n        if hasattr(X_val, 'toarray'):\n            X_val = X_val.toarray()\n        \n        # Train models\n        self.tree_model.fit(X_train, y_train)\n        self._train_torch_model(X_train, y_train)\n        \n        # Calibrate ensemble weights\n        self._calibrate_ensemble(X_val, y_val)\n        \n        # Learn health weights\n        if account_health is not None:\n            self._learn_health_weights(X, y, account_health)\n        \n        # Initialize explainer\n        self.explainer = shap.TreeExplainer(self.tree_model)\n        logger.info(f\"Training completed with version {self.version}\")\n        return self\n\n    def _get_feature_dims(self, X: pd.DataFrame) -> None:\n        \"\"\"Determine feature dimensions from fitted preprocessor\"\"\"\n        self.feature_dims = {\n            'num': len(self.preprocessor.transformers_[0][2]),\n            'cat': self.preprocessor.named_transformers_['cat'].get_feature_names_out().shape[0],\n            'text': self.preprocessor.named_transformers_['text'].get_feature_names_out().shape[0]\n        }\n\n    def _calibrate_ensemble(self, X_val: np.ndarray, y_val: pd.Series) -> None:\n        \"\"\"Optimize ensemble weights using PSO\"\"\"\n        # Get predictions from both models\n        xgb_preds = self.tree_model.predict_proba(X_val)[:, 1]\n        nn_preds = self._torch_predict_proba(X_val)[:, 1]\n        \n        # Define objective function\n        def objective(weights):\n            combined = weights[0]*xgb_preds + weights[1]*nn_preds\n            return np.mean((combined - y_val)**2)\n        \n        # Optimize weights\n        result = minimize(objective, [0.5, 0.5], \n                         bounds=[(0,1), (0,1)],\n                         constraints={'type': 'eq', 'fun': lambda w: w.sum()-1})\n        self.ensemble_weights = result.x\n\n    def _learn_health_weights(self, X: pd.DataFrame, y: pd.Series, \n                            health_data: pd.DataFrame) -> None:\n        \"\"\"Learn health adjustment weights using linear regression\"\"\"\n        # Get base predictions\n        raw_scores = self.tree_model.predict_proba(self.preprocessor.transform(X))[:, 1]\n        \n        # Prepare health features\n        health_features = health_data[['risk_factors', 'spend_efficiency', 'audience_health']]\n        \n        # Solve for optimal weights\n        X_health = np.column_stack([raw_scores*(health_features[col]) for col in health_features])\n        y_diff = y - raw_scores\n        self.health_weights = np.linalg.lstsq(X_health, y_diff, rcond=None)[0]\n\n    def predict(self, features: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Generate predictions with confidence intervals\"\"\"\n        try:\n            # Convert single sample to DataFrame\n            df = pd.DataFrame([features])\n            \n            # Ensure all required features are present\n            for col in self.feature_columns:\n                if col not in df.columns:\n                    df[col] = 0.0\n                    logger.warning(f\"Missing feature {col} in prediction input. Using default value 0.0\")\n            \n            # Make prediction\n            X = df[self.feature_columns]\n            if not self.is_fitted:\n                raise RuntimeError(\"Model must be trained before making predictions\")\n            \n            prediction = self.pipeline.predict(X)[0]\n            \n            # Calculate confidence using feature importance and data distribution\n            confidence = self._calculate_prediction_confidence(X)\n            \n            # Generate SHAP values for explainability\n            if self.explainer is not None:\n                shap_values = self.explainer.shap_values(X)\n                feature_impacts = dict(zip(self.feature_columns, shap_values[0]))\n            else:\n                feature_impacts = {}\n            \n            return {\n                'score': float(prediction),  # Changed from 'engagement_score' to 'score'\n                'confidence': float(confidence),\n                'feature_impacts': feature_impacts,\n                'prediction_time': datetime.utcnow().isoformat()\n            }\n            \n        except Exception as e:\n            logger.error(f\"Prediction error: {str(e)}\")\n            return {\n                'score': 0.5,  # Changed from 'engagement_score' to 'score'\n                'confidence': 0.0,\n                'error': str(e),\n                'fallback': True\n            }\n\n    def _generate_explanations(self, processed_input) -> Dict[str, Any]:\n        \"\"\"Enhanced explanations with interaction effects\"\"\"\n        shap_values = self.explainer.shap_values(processed_input)\n        interactions = self.explainer.shap_interaction_values(processed_input)\n        return {\n            'shap_values': shap_values,\n            'interactions': interactions,\n            'feature_importance': self.tree_model.feature_importances_,\n            'summary_plot': shap.summary_plot(shap_values, processed_input, plot_type='bar')\n        }\n\n    def _fallback_prediction(self, features: Dict[str, Any], error: Exception = None) -> Dict[str, Any]:\n        \"\"\"Graceful fallback with partial results\"\"\"\n        return {\n            'engagement_score': 0.5,\n            'confidence': 0.0,\n            'error': str(error) if error else \"Unknown error\",\n            'fallback': True,\n            'model_metadata': self._get_model_metadata()\n        }\n\n    def _get_model_metadata(self) -> Dict[str, Any]:\n        \"\"\"Get model metadata for tracking\"\"\"\n        return {\n            'version': self.version,\n            'timestamp': datetime.utcnow().isoformat(),\n            'ensemble_weights': self.ensemble_weights.tolist() if self.ensemble_weights is not None else None,\n            'health_weights': self.health_weights.tolist() if self.health_weights is not None else None\n        }\n\n    def _get_health_impact(self, adjustments: float) -> Dict[str, float]:\n        \"\"\"Calculate health impact on prediction\"\"\"\n        return {\n            'raw_adjustment': float(adjustments),\n            'relative_impact': float(abs(adjustments) / (1 + abs(adjustments)))\n        }\n\n    def _train_torch_model(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n        \"\"\"Train PyTorch model with enhanced error handling\"\"\"\n        try:\n            # Convert data to PyTorch tensors\n            X_tensor = torch.FloatTensor(X_train).to(self.device)\n            y_tensor = torch.LongTensor(y_train).to(self.device)  # Remove .values since it's already numpy\n            \n            # Create data loader\n            dataset = TensorDataset(X_tensor, y_tensor)\n            loader = DataLoader(dataset, batch_size=self.config['nn_params']['batch_size'], shuffle=True)\n            \n            # Training setup\n            criterion = nn.CrossEntropyLoss()\n            optimizer = optim.Adam(self.parameters(), lr=self.config['nn_params']['learning_rate'])\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)\n            \n            # Training loop\n            best_loss = float('inf')\n            patience_counter = 0\n            \n            for epoch in range(self.config['nn_params']['epochs']):\n                self.train()  # Set model to training mode\n                epoch_loss = 0\n                \n                for batch_X, batch_y in loader:\n                    optimizer.zero_grad()\n                    outputs = self(batch_X)\n                    loss = criterion(outputs, batch_y)\n                    loss.backward()\n                    optimizer.step()\n                    epoch_loss += loss.item()\n                \n                avg_loss = epoch_loss / len(loader)\n                scheduler.step(avg_loss)\n                \n                # Early stopping\n                if avg_loss < best_loss:\n                    best_loss = avg_loss\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                    if patience_counter >= self.config['nn_params']['patience']:\n                        logger.info(f\"Early stopping at epoch {epoch}\")\n                        break\n                \n                if epoch % 10 == 0:\n                    logger.debug(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n            \n            self.eval()  # Set model to evaluation mode\n            \n        except Exception as e:\n            logger.error(f\"PyTorch training error: {str(e)}\")\n            raise\n\n    def _optimize_hyperparameters(self, X: np.ndarray, y: pd.Series) -> None:\n        \"\"\"Optimize hyperparameters using grid search\"\"\"\n        param_grid = {\n            'max_depth': [4, 6, 8],\n            'learning_rate': [0.01, 0.1],\n            'n_estimators': [100, 200]\n        }\n        grid_search = GridSearchCV(\n            self.tree_model, param_grid, cv=3, scoring='neg_log_loss'\n        )\n        grid_search.fit(X, y)\n        self.tree_model.set_params(**grid_search.best_params_)\n\n    def _torch_predict_proba(self, X: np.ndarray) -> np.ndarray:\n        self.eval()\n        with torch.no_grad():\n            outputs = self(torch.FloatTensor(X).to(self.device))\n            return torch.softmax(outputs, dim=1).cpu().numpy()\n            \n"
    },
    {
      "path": "app/models/ml/prediction/anomaly_detector.py",
      "metadata": {
        "path": "app/models/ml/prediction/anomaly_detector.py",
        "token_count": 2751,
        "size_bytes": 13356,
        "classes": [
          {
            "name": "AutoEncoder",
            "methods": [
              "__init__",
              "forward"
            ],
            "lineno": 22
          },
          {
            "name": "EnhancedAnomalyDetector",
            "methods": [
              "__init__",
              "train",
              "detect",
              "_preprocess_data",
              "_train_autoencoder",
              "_calculate_threshold",
              "_calculate_dynamic_thresholds",
              "_ensemble_predict",
              "_generate_explanations",
              "_default_config",
              "_calculate_nn_anomaly_scores",
              "_fallback_detection",
              "_predict_proba",
              "_build_temporary_autoencoder",
              "_validate_autoencoder"
            ],
            "lineno": 60
          }
        ],
        "functions": [
          {
            "name": "__init__",
            "lineno": 24,
            "args": [
              "self",
              "input_dim",
              "hidden_dims",
              "dropout_rate"
            ]
          },
          {
            "name": "forward",
            "lineno": 55,
            "args": [
              "self",
              "x"
            ]
          },
          {
            "name": "__init__",
            "lineno": 64,
            "args": [
              "self",
              "config"
            ]
          },
          {
            "name": "train",
            "lineno": 89,
            "args": [
              "self",
              "metrics"
            ]
          },
          {
            "name": "detect",
            "lineno": 118,
            "args": [
              "self",
              "current",
              "history"
            ]
          },
          {
            "name": "_preprocess_data",
            "lineno": 144,
            "args": [
              "self",
              "metrics"
            ]
          },
          {
            "name": "_train_autoencoder",
            "lineno": 165,
            "args": [
              "self",
              "X"
            ]
          },
          {
            "name": "_calculate_threshold",
            "lineno": 211,
            "args": [
              "self",
              "X"
            ]
          },
          {
            "name": "_calculate_dynamic_thresholds",
            "lineno": 222,
            "args": [
              "self",
              "X"
            ]
          },
          {
            "name": "_ensemble_predict",
            "lineno": 231,
            "args": [
              "self",
              "X"
            ]
          },
          {
            "name": "_generate_explanations",
            "lineno": 246,
            "args": [
              "self",
              "X"
            ]
          },
          {
            "name": "_default_config",
            "lineno": 255,
            "args": [
              "self"
            ]
          },
          {
            "name": "_calculate_nn_anomaly_scores",
            "lineno": 281,
            "args": [
              "self",
              "X"
            ]
          },
          {
            "name": "_fallback_detection",
            "lineno": 290,
            "args": [
              "self",
              "current",
              "error"
            ]
          },
          {
            "name": "_predict_proba",
            "lineno": 300,
            "args": [
              "self",
              "X"
            ]
          },
          {
            "name": "_build_temporary_autoencoder",
            "lineno": 304,
            "args": [
              "self",
              "params"
            ]
          },
          {
            "name": "_validate_autoencoder",
            "lineno": 341,
            "args": [
              "self",
              "model",
              "X",
              "val_size"
            ]
          }
        ],
        "imports": [
          "torch",
          "numpy",
          "pandas",
          "logging",
          "typing.Dict",
          "typing.Any",
          "typing.List",
          "typing.Optional",
          "typing.Tuple",
          "datetime.datetime",
          "sklearn.ensemble.IsolationForest",
          "sklearn.preprocessing.StandardScaler",
          "sklearn.feature_selection.VarianceThreshold",
          "sklearn.svm.OneClassSVM",
          "sklearn.linear_model.ElasticNetCV",
          "sklearn.decomposition.PCA",
          "sklearn.pipeline.Pipeline",
          "torch.utils.data.DataLoader",
          "torch.utils.data.TensorDataset",
          "torch.nn",
          "optuna",
          "shap"
        ],
        "strings": [
          "Enhanced Anomaly Detection System with Hybrid Approach",
          "Enhanced autoencoder with regularization and dropout",
          "Enhanced anomaly detector with improved training and evaluation",
          "Initialize the anomaly detector with default or custom configuration",
          "Train the anomaly detector on the provided metrics",
          "Detect anomalies in current metrics",
          "Convert metrics to normalized numpy array",
          "Optimized training with learning rate scheduling",
          "Calculate reconstruction error threshold",
          "Adaptive thresholding using quantiles",
          "Stacked ensemble predictions",
          "SHAP explanations with feature importance",
          "Default configuration for the anomaly detector",
          "Calculate anomaly scores using autoencoder reconstruction error",
          "Enhanced fallback with partial results",
          "SHAP-compatible prediction function",
          "Build a temporary autoencoder for hyperparameter optimization",
          "Validate autoencoder performance",
          "learning_rate",
          "batch_size",
          "max_epochs",
          "early_stopping_patience",
          "hidden_dims",
          "dropout_rate",
          "l2_reg",
          "threshold_percentile",
          "is_anomaly",
          "reconstruction_error",
          "threshold",
          "confidence",
          "clicks",
          "conversions",
          "spend",
          "revenue",
          "impressions",
          "ctr",
          "min",
          "inf",
          "metrics",
          "feature_importance",
          "summary_plot",
          "decision_plot",
          "metrics",
          "iso_forest",
          "ocsvm",
          "autoencoder",
          "max_epochs",
          "early_stopping_patience",
          "hidden_dims",
          "dropout_rate",
          "l2_reg",
          "threshold_percentile",
          "anomalies",
          "detection_time",
          "error",
          "fallback",
          "reduced_dim",
          "cuda",
          "cpu",
          "Detector must be trained before use",
          "max_epochs",
          "threshold_percentile",
          "ctr",
          "conversion_rate",
          "cost_per_conversion",
          "impressions",
          "clicks",
          "spend",
          "n_estimators",
          "contamination",
          "random_state",
          "auto",
          "kernel",
          "nu",
          "rbf",
          "learning_rate",
          "batch_size",
          "epochs",
          "Unknown error",
          "num_layers",
          "hidden_dim",
          "num_layers",
          "autoencoder",
          "scaler",
          "autoencoder",
          "autoencoder",
          "Missing required columns: ",
          "batch_size",
          "learning_rate",
          "l2_reg",
          "autoencoder",
          "autoencoder",
          "threshold_quantile",
          "iso_forest",
          "ocsvm",
          "autoencoder",
          "autoencoder",
          "is_anomaly",
          "hidden_dim",
          "Data validation error: ",
          "scaler",
          "scaler",
          "autoencoder",
          "autoencoder",
          "autoencoder",
          "early_stopping_patience",
          "ocsvm",
          "metrics",
          "metrics",
          "Early stopping at epoch ",
          "iso",
          "nn",
          "weights",
          "hidden_dim",
          "hidden_dim",
          "num_layers",
          "weights",
          "weights",
          "num_layers",
          "hidden_dims",
          "dropout_rate"
        ]
      },
      "metrics": {
        "line_count": 358,
        "comment_ratio": 0.10893854748603352,
        "avg_function_length": 18,
        "complexity_estimate": 15
      },
      "content": "\"\"\"Enhanced Anomaly Detection System with Hybrid Approach\"\"\"\nimport torch\nimport numpy as np\nimport pandas as pd\nimport logging\nfrom typing import Dict, Any, List, Optional, Tuple\nfrom datetime import datetime\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch import nn\nimport optuna\nimport shap\n\nlogger = logging.getLogger(__name__)\n\nclass AutoEncoder(nn.Module):\n    \"\"\"Enhanced autoencoder with regularization and dropout\"\"\"\n    def __init__(self, input_dim, hidden_dims=[64, 32, 16], dropout_rate=0.2):\n        super().__init__()\n        \n        # Encoder layers\n        encoder_layers = []\n        prev_dim = input_dim\n        for dim in hidden_dims:\n            encoder_layers.extend([\n                nn.Linear(prev_dim, dim),\n                nn.BatchNorm1d(dim),\n                nn.ReLU(),\n                nn.Dropout(dropout_rate)\n            ])\n            prev_dim = dim\n        self.encoder = nn.Sequential(*encoder_layers)\n        \n        # Decoder layers\n        decoder_layers = []\n        hidden_dims_reversed = hidden_dims[::-1]\n        prev_dim = hidden_dims[-1]\n        for dim in hidden_dims_reversed[1:]:\n            decoder_layers.extend([\n                nn.Linear(prev_dim, dim),\n                nn.BatchNorm1d(dim),\n                nn.ReLU(),\n                nn.Dropout(dropout_rate)\n            ])\n            prev_dim = dim\n        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n        self.decoder = nn.Sequential(*decoder_layers)\n    \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\nclass EnhancedAnomalyDetector:\n    \"\"\"Enhanced anomaly detector with improved training and evaluation\"\"\"\n    INPUT_DIM = 6  # Number of input features\n    \n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        \"\"\"Initialize the anomaly detector with default or custom configuration\"\"\"\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Default configuration\n        self.config = {\n            'learning_rate': 0.001,\n            'batch_size': 2048,\n            'max_epochs': 100,\n            'early_stopping_patience': 10,\n            'hidden_dims': [64, 32, 16],\n            'dropout_rate': 0.2,\n            'l2_reg': 1e-5,\n            'threshold_percentile': 95\n        }\n        \n        # Update with custom config if provided\n        if config:\n            self.config.update(config)\n        \n        self.models = {}\n        self.reconstruction_errors = None\n        self.threshold = None\n        self.is_fitted = False\n        \n    def train(self, metrics: List[Dict[str, float]]) -> None:\n        \"\"\"Train the anomaly detector on the provided metrics\"\"\"\n        try:\n            # Convert metrics to numpy array and normalize\n            X = self._preprocess_data(metrics)\n            \n            # Initialize models if not already done\n            if not self.models:\n                self.models = {\n                    'autoencoder': AutoEncoder(\n                        input_dim=X.shape[1],\n                        hidden_dims=self.config['hidden_dims'],\n                        dropout_rate=self.config['dropout_rate']\n                    ).to(self.device),\n                    'scaler': StandardScaler()\n                }\n            \n            # Train autoencoder\n            self._train_autoencoder(X)\n            \n            # Calculate reconstruction error distribution\n            self._calculate_threshold(X)\n            \n            self.is_fitted = True\n            \n        except Exception as e:\n            logger.error(f\"Data validation error: {str(e)}\")\n            raise\n\n    def detect(self, current: Dict[str, float], \n              history: List[Dict[str, float]]) -> Dict[str, Any]:\n        \"\"\"Detect anomalies in current metrics\"\"\"\n        if not self.is_fitted:\n            raise RuntimeError(\"Detector must be trained before use\")\n            \n        # Preprocess current data\n        X = self._preprocess_data([current])\n        \n        # Get reconstruction error\n        self.models['autoencoder'].eval()\n        with torch.no_grad():\n            x = torch.FloatTensor(X).to(self.device)\n            reconstructed = self.models['autoencoder'](x)\n            error = torch.mean(torch.pow(x - reconstructed, 2), dim=1).cpu().numpy()[0]\n        \n        # Determine if anomalous\n        is_anomaly = error > self.threshold\n        \n        return {\n            'is_anomaly': bool(is_anomaly),\n            'reconstruction_error': float(error),\n            'threshold': float(self.threshold),\n            'confidence': float(1.0 - error / (error + self.threshold))\n        }\n\n    def _preprocess_data(self, metrics: List[Dict[str, float]]) -> np.ndarray:\n        \"\"\"Convert metrics to normalized numpy array\"\"\"\n        df = pd.DataFrame(metrics)\n        required_columns = ['clicks', 'conversions', 'spend', 'revenue', 'impressions', 'ctr']\n        \n        # Ensure all required columns exist\n        missing_cols = set(required_columns) - set(df.columns)\n        if missing_cols:\n            raise ValueError(f\"Missing required columns: {missing_cols}\")\n        \n        # Extract features\n        X = df[required_columns].values\n        \n        # Scale features\n        if not self.is_fitted:\n            X = self.models['scaler'].fit_transform(X)\n        else:\n            X = self.models['scaler'].transform(X)\n        \n        return X\n\n    def _train_autoencoder(self, X: np.ndarray) -> None:\n        \"\"\"Optimized training with learning rate scheduling\"\"\"\n        dataset = TensorDataset(torch.FloatTensor(X))\n        loader = DataLoader(dataset, batch_size=self.config['batch_size'], shuffle=True)\n        \n        optimizer = torch.optim.AdamW(\n            self.models['autoencoder'].parameters(),\n            lr=self.config['learning_rate'],\n            weight_decay=self.config['l2_reg']\n        )\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n        criterion = nn.MSELoss()\n        \n        best_loss = float('inf')\n        patience_counter = 0\n        \n        for epoch in range(self.config['max_epochs']):\n            epoch_loss = 0.0\n            num_batches = 0\n            \n            self.models['autoencoder'].train()\n            for batch in loader:\n                x = batch[0].to(self.device)\n                \n                optimizer.zero_grad()\n                reconstructed = self.models['autoencoder'](x)\n                loss = criterion(reconstructed, x)\n                loss.backward()\n                optimizer.step()\n                \n                epoch_loss += loss.item()\n                num_batches += 1\n            \n            avg_loss = epoch_loss / num_batches\n            scheduler.step(avg_loss)\n            \n            # Early stopping\n            if avg_loss < best_loss:\n                best_loss = avg_loss\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter >= self.config['early_stopping_patience']:\n                    logger.info(f\"Early stopping at epoch {epoch}\")\n                    break\n    \n    def _calculate_threshold(self, X: np.ndarray) -> None:\n        \"\"\"Calculate reconstruction error threshold\"\"\"\n        self.models['autoencoder'].eval()\n        with torch.no_grad():\n            x = torch.FloatTensor(X).to(self.device)\n            reconstructed = self.models['autoencoder'](x)\n            errors = torch.mean(torch.pow(x - reconstructed, 2), dim=1).cpu().numpy()\n            \n        self.reconstruction_errors = errors\n        self.threshold = np.percentile(errors, self.config['threshold_percentile'])\n\n    def _calculate_dynamic_thresholds(self, X: np.ndarray) -> None:\n        \"\"\"Adaptive thresholding using quantiles\"\"\"\n        scores = self._ensemble_predict(X)\n        for metric in self.config['metrics']:\n            self.thresholds[metric] = np.quantile(\n                scores[metric], \n                self.config['threshold_quantile']\n            )\n\n    def _ensemble_predict(self, X: np.ndarray) -> Dict[str, float]:\n        \"\"\"Stacked ensemble predictions\"\"\"\n        iso_scores = self.models['iso_forest'].decision_function(X)\n        nn_scores = self._calculate_nn_anomaly_scores(X)\n        oc_scores = self.models['ocsvm'].decision_function(X)\n        \n        return {\n            metric: (\n                iso_scores[i] * self.config['weights']['iso'] +\n                nn_scores[i] * self.config['weights']['nn'] +\n                oc_scores[i] * self.config['weights']['ocsvm']\n            )\n            for i, metric in enumerate(self.config['metrics'])\n        }\n\n    def _generate_explanations(self, X: np.ndarray) -> Dict[str, Any]:\n        \"\"\"SHAP explanations with feature importance\"\"\"\n        shap_values = self.explainer.shap_values(X)\n        return {\n            'feature_importance': np.abs(shap_values).mean(0),\n            'summary_plot': shap.summary_plot(shap_values, X),\n            'decision_plot': shap.decision_plot(self.explainer.expected_value, shap_values, X)\n        }\n\n    def _default_config(self) -> Dict[str, Any]:\n        \"\"\"Default configuration for the anomaly detector\"\"\"\n        return {\n            'metrics': ['ctr', 'conversion_rate', 'cost_per_conversion', 'impressions', 'clicks', 'spend'],\n            'iso_forest': {\n                'n_estimators': 100,\n                'contamination': 'auto',\n                'random_state': 42\n            },\n            'ocsvm': {\n                'kernel': 'rbf',\n                'nu': 0.1\n            },\n            'autoencoder': {\n                'learning_rate': 0.001,\n                'batch_size': 32,\n                'epochs': 100\n            },\n            'max_epochs': 100,\n            'early_stopping_patience': 10,\n            'hidden_dims': [64, 32, 16],\n            'dropout_rate': 0.2,\n            'l2_reg': 1e-5,\n            'threshold_percentile': 95\n        }\n\n    def _calculate_nn_anomaly_scores(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Calculate anomaly scores using autoencoder reconstruction error\"\"\"\n        self.models['autoencoder'].eval()\n        with torch.no_grad():\n            X_tensor = torch.FloatTensor(X).to(self.device)\n            reconstructed = self.models['autoencoder'](X_tensor)\n            errors = torch.mean((X_tensor - reconstructed) ** 2, dim=1)\n            return errors.cpu().numpy()\n\n    def _fallback_detection(self, current: Dict[str, float], \n                           error: Exception = None) -> Dict[str, Any]:\n        \"\"\"Enhanced fallback with partial results\"\"\"\n        return {\n            'anomalies': {metric: {'is_anomaly': False} for metric in self.config['metrics']},\n            'detection_time': datetime.utcnow().isoformat(),\n            'error': str(error) if error else 'Unknown error',\n            'fallback': True\n        }\n\n    def _predict_proba(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"SHAP-compatible prediction function\"\"\"\n        return self._ensemble_predict(X)\n\n    def _build_temporary_autoencoder(self, params: Dict[str, Any]) -> nn.Module:\n        \"\"\"Build a temporary autoencoder for hyperparameter optimization\"\"\"\n        layers = []\n        input_dim = self.config['reduced_dim']\n        current_dim = input_dim\n        \n        # Encoder layers\n        for _ in range(params['num_layers']):\n            layers.extend([\n                nn.Linear(current_dim, params['hidden_dim']),\n                nn.BatchNorm1d(params['hidden_dim']),\n                nn.ReLU()\n            ])\n            current_dim = params['hidden_dim']\n        \n        # Bottleneck layer\n        bottleneck_dim = max(2, current_dim // 2)\n        layers.extend([\n            nn.Linear(current_dim, bottleneck_dim),\n            nn.BatchNorm1d(bottleneck_dim),\n            nn.ReLU()\n        ])\n        \n        # Decoder layers\n        current_dim = bottleneck_dim\n        for _ in range(params['num_layers']):\n            next_dim = params['hidden_dim'] if _ < params['num_layers'] - 1 else input_dim\n            layers.extend([\n                nn.Linear(current_dim, next_dim),\n                nn.BatchNorm1d(next_dim),\n                nn.ReLU() if _ < params['num_layers'] - 1 else nn.Identity()\n            ])\n            current_dim = next_dim\n        \n        model = nn.Sequential(*layers).to(self.device)\n        return model\n\n    def _validate_autoencoder(self, model: nn.Module, X: np.ndarray, \n                            val_size: float = 0.2) -> float:\n        \"\"\"Validate autoencoder performance\"\"\"\n        # Split data\n        n_val = int(len(X) * val_size)\n        indices = np.random.permutation(len(X))\n        val_indices = indices[:n_val]\n        \n        # Convert to tensor\n        X_val = torch.FloatTensor(X[val_indices]).to(self.device)\n        \n        # Validation\n        model.eval()\n        with torch.no_grad():\n            reconstructed = model(X_val)\n            loss = torch.nn.MSELoss()(reconstructed, X_val)\n        \n        return loss.item()"
    },
    {
      "path": "app/models/ad_score_model.py",
      "metadata": {
        "path": "app/models/ad_score_model.py",
        "token_count": 425,
        "size_bytes": 2070,
        "classes": [
          {
            "name": "AdScoreModel",
            "methods": [],
            "lineno": 12
          },
          {
            "name": "AdScoreAnalysisModel",
            "methods": [],
            "lineno": 34
          }
        ],
        "functions": [],
        "imports": [
          "datetime.datetime",
          "typing.Dict",
          "typing.Any",
          "typing.List",
          "typing.Optional",
          "uuid.uuid4",
          "sqlalchemy.Column",
          "sqlalchemy.String",
          "sqlalchemy.JSON",
          "sqlalchemy.Float",
          "sqlalchemy.DateTime",
          "sqlalchemy.orm.relationship",
          "app.core.database.BaseModel"
        ],
        "strings": [
          "Model for storing ad scoring results.",
          "Model for storing ad scoring results.",
          "ad_scores",
          "Model for storing detailed ad score analysis results.",
          "ad_score_analysis"
        ]
      },
      "metrics": {
        "line_count": 47,
        "comment_ratio": 0.0851063829787234,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": "\"\"\"Model for storing ad scoring results.\"\"\"\n\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional\nfrom uuid import uuid4\n\nfrom sqlalchemy import Column, String, JSON, Float, DateTime\nfrom sqlalchemy.orm import relationship\n\nfrom app.core.database import BaseModel\n\nclass AdScoreModel(BaseModel):\n    \"\"\"Model for storing ad scoring results.\"\"\"\n    \n    __tablename__ = \"ad_scores\"\n\n    id = Column(String(36), primary_key=True, default=lambda: str(uuid4()))\n    ad_id = Column(String, nullable=False)\n    advertiser_id = Column(String, nullable=False)\n    ad_content = Column(String, nullable=False)\n    engagement_score = Column(Float, nullable=False)\n    sentiment_score = Column(Float, nullable=False)\n    complexity_score = Column(Float, nullable=False)\n    topics = Column(JSON, nullable=False, default=list)\n    target_audience_match = Column(Float, nullable=False)\n    predicted_ctr = Column(Float, nullable=False)\n    confidence_score = Column(Float, nullable=False)\n    processed_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)\n    \n    # Rename 'metadata' to something else, like 'meta_info'\n    meta_info = Column(JSON, nullable=False, default=dict)\n\nclass AdScoreAnalysisModel(BaseModel):\n    \"\"\"Model for storing detailed ad score analysis results.\"\"\"\n    \n    __tablename__ = \"ad_score_analysis\"\n\n    id = Column(String(36), primary_key=True, default=lambda: str(uuid4()))\n    ad_score_id = Column(String(36), nullable=False)\n    content_features = Column(JSON, nullable=False, default=dict)\n    linguistic_analysis = Column(JSON, nullable=False, default=dict)\n    visual_elements = Column(JSON, nullable=False, default=list)\n    performance_projections = Column(JSON, nullable=False, default=dict)\n    similar_successful_ads = Column(JSON, nullable=False, default=list)\n    improvement_suggestions = Column(JSON, nullable=False, default=list)\n    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)"
    },
    {
      "path": "app/models/ad_account_health_model.py",
      "metadata": {
        "path": "app/models/ad_account_health_model.py",
        "token_count": 394,
        "size_bytes": 1933,
        "classes": [
          {
            "name": "AdAccountHealthModel",
            "methods": [],
            "lineno": 12
          },
          {
            "name": "PerformanceMetricModel",
            "methods": [],
            "lineno": 29
          }
        ],
        "functions": [],
        "imports": [
          "datetime.datetime",
          "typing.Dict",
          "typing.Any",
          "typing.List",
          "typing.Optional",
          "uuid.uuid4",
          "sqlalchemy.Column",
          "sqlalchemy.String",
          "sqlalchemy.JSON",
          "sqlalchemy.Float",
          "sqlalchemy.DateTime",
          "sqlalchemy.Boolean",
          "sqlalchemy.orm.relationship",
          "app.core.database.BaseModel"
        ],
        "strings": [
          "Model for storing ad account health metrics.",
          "Model for storing ad account health metrics.",
          "ad_account_health",
          "Model for storing time-series performance metrics.",
          "performance_metrics"
        ]
      },
      "metrics": {
        "line_count": 44,
        "comment_ratio": 0.06818181818181818,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": "\"\"\"Model for storing ad account health metrics.\"\"\"\n\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional\nfrom uuid import uuid4\n\nfrom sqlalchemy import Column, String, JSON, Float, DateTime, Boolean\nfrom sqlalchemy.orm import relationship\n\nfrom app.core.database import BaseModel\n\nclass AdAccountHealthModel(BaseModel):\n    \"\"\"Model for storing ad account health metrics.\"\"\"\n    \n    __tablename__ = \"ad_account_health\"\n\n    id = Column(String(36), primary_key=True, default=lambda: str(uuid4()))\n    account_id = Column(String, nullable=False)\n    health_score = Column(Float, nullable=False)\n    engagement_trends = Column(JSON, nullable=False)\n    risk_factors = Column(JSON, nullable=False, default=list)\n    optimization_suggestions = Column(JSON, nullable=False, default=list)\n    historical_performance = Column(JSON, nullable=False)\n    spend_efficiency = Column(Float, nullable=False)\n    audience_health = Column(JSON, nullable=False, default=dict)\n    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)\n\nclass PerformanceMetricModel(BaseModel):\n    \"\"\"Model for storing time-series performance metrics.\"\"\"\n    \n    __tablename__ = \"performance_metrics\"\n\n    id = Column(String(36), primary_key=True, default=lambda: str(uuid4()))\n    account_id = Column(String, nullable=False)\n    metric_name = Column(String, nullable=False)\n    metric_value = Column(Float, nullable=False)\n    time_period = Column(String, nullable=False)  # daily, weekly, monthly\n    start_date = Column(DateTime, nullable=False)\n    end_date = Column(DateTime, nullable=False)\n    is_anomaly = Column(Boolean, default=False)\n    anomaly_score = Column(Float, nullable=True)\n    meta_info = Column(JSON, nullable=False, default=dict)\n    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)"
    },
    {
      "path": "scripts/digest.py",
      "metadata": {
        "path": "scripts/digest.py",
        "token_count": 5318,
        "size_bytes": 28223,
        "classes": [
          {
            "name": "CodebaseDigest",
            "methods": [
              "__init__",
              "process_codebase",
              "build_hierarchy",
              "process_file",
              "_extract_python_metadata",
              "_extract_js_metadata",
              "_should_ignore",
              "generate_output",
              "_count_extensions"
            ],
            "lineno": 19
          },
          {
            "name": "AdvancedCodebaseDigest",
            "methods": [
              "__init__",
              "process_codebase",
              "analyze_dependencies",
              "calculate_code_metrics",
              "_calculate_comment_ratio",
              "_calculate_avg_function_length",
              "_estimate_complexity",
              "generate_enhanced_output",
              "_identify_complexity_hotspots",
              "_identify_central_modules",
              "_calculate_importance_score"
            ],
            "lineno": 380
          }
        ],
        "functions": [
          {
            "name": "__init__",
            "lineno": 22,
            "args": [
              "self",
              "input_path",
              "output_path",
              "ignore_patterns"
            ]
          },
          {
            "name": "process_codebase",
            "lineno": 40,
            "args": [
              "self"
            ]
          },
          {
            "name": "build_hierarchy",
            "lineno": 99,
            "args": [
              "self"
            ]
          },
          {
            "name": "process_file",
            "lineno": 136,
            "args": [
              "self",
              "file_path",
              "relative_path"
            ]
          },
          {
            "name": "_extract_python_metadata",
            "lineno": 174,
            "args": [
              "self",
              "content"
            ]
          },
          {
            "name": "_extract_js_metadata",
            "lineno": 265,
            "args": [
              "self",
              "content"
            ]
          },
          {
            "name": "_should_ignore",
            "lineno": 296,
            "args": [
              "self",
              "path"
            ]
          },
          {
            "name": "generate_output",
            "lineno": 328,
            "args": [
              "self"
            ]
          },
          {
            "name": "_count_extensions",
            "lineno": 370,
            "args": [
              "self"
            ]
          },
          {
            "name": "__init__",
            "lineno": 383,
            "args": [
              "self",
              "input_path",
              "output_path",
              "ignore_patterns"
            ]
          },
          {
            "name": "process_codebase",
            "lineno": 389,
            "args": [
              "self"
            ]
          },
          {
            "name": "analyze_dependencies",
            "lineno": 406,
            "args": [
              "self"
            ]
          },
          {
            "name": "calculate_code_metrics",
            "lineno": 440,
            "args": [
              "self"
            ]
          },
          {
            "name": "_calculate_comment_ratio",
            "lineno": 465,
            "args": [
              "self",
              "content",
              "file_path"
            ]
          },
          {
            "name": "_calculate_avg_function_length",
            "lineno": 491,
            "args": [
              "self",
              "content",
              "file_path"
            ]
          },
          {
            "name": "_estimate_complexity",
            "lineno": 520,
            "args": [
              "self",
              "content",
              "file_path"
            ]
          },
          {
            "name": "generate_enhanced_output",
            "lineno": 540,
            "args": [
              "self"
            ]
          },
          {
            "name": "_identify_complexity_hotspots",
            "lineno": 592,
            "args": [
              "self"
            ]
          },
          {
            "name": "_identify_central_modules",
            "lineno": 608,
            "args": [
              "self"
            ]
          },
          {
            "name": "_calculate_importance_score",
            "lineno": 623,
            "args": [
              "self",
              "file_path"
            ]
          }
        ],
        "imports": [
          "os",
          "json",
          "tiktoken",
          "typing.Dict",
          "typing.List",
          "typing.Any",
          "typing.Optional",
          "ast",
          "re",
          "logging",
          "time",
          "datetime.datetime"
        ],
        "strings": [
          "CodebaseDigest",
          "Tool to consolidate codebase into LLM-friendly format with hierarchical structure and metadata.",
          "Advanced codebase digest with additional analysis features.",
          "__main__",
          "/Users/alecposner/WITHIN",
          "/Users/alecposner/WITHIN/codebase_digest.json",
          "%(asctime)s - %(levelname)s - %(message)s",
          "%Y-%m-%d %H:%M:%S",
          "Process the entire codebase and generate structured output.",
          "Build a directory structure representation.",
          "Process an individual file and extract metadata.",
          "Extract metadata from Python files.",
          "Extract metadata from JavaScript/TypeScript files.",
          "Check if a path should be included using an allowlist approach.",
          "Generate the consolidated output.",
          "Count the number of files by extension.",
          "Process codebase with advanced metrics.",
          "Analyze import dependencies between files.",
          "Calculate various code quality and complexity metrics.",
          "Calculate the ratio of comments to code.",
          "Calculate average function length in lines.",
          "Estimate code complexity based on control structures.",
          "Generate enhanced output with all collected metrics.",
          "Identify the most complex parts of the codebase.",
          "Identify central modules based on import relationships.",
          "Calculate an importance score for a file based on various metrics.",
          "Starting codebase digest process",
          ".benchmarks",
          ".pytest_cache",
          "htmlcov",
          "venv",
          "__pycache__",
          "node_modules",
          ".git",
          ".env",
          "*.pyc",
          "*.pyo",
          "*.pyd",
          ".DS_Store",
          "*.log",
          ".idea",
          ".vscode",
          "tiktoken",
          "lib",
          "bin",
          "include",
          "site-packages",
          "cl100k_base",
          "Starting codebase processing...",
          "Building directory hierarchy...",
          "Directory hierarchy built successfully",
          "Generating final output...",
          "classes",
          "functions",
          "imports",
          "strings",
          "function\\s+(\\w+)\\s*\\(",
          "class\\s+(\\w+)",
          "import\\s+.*?from\\s+[\\'\"](.+?)[\\'\"]",
          "functions",
          "classes",
          "imports",
          "app",
          "tests",
          "scripts",
          "Generating output file...",
          "codebase_summary",
          "files",
          "Sorting files by importance...",
          "Building final file entries...",
          "Advanced codebase digest initialized with additional metrics",
          "Beginning advanced analysis...",
          "Analyzing dependencies...",
          "Calculating code metrics...",
          "Generating enhanced output...",
          "Calculating code metrics...",
          ".py",
          ".py",
          ".py",
          "Generating enhanced output...",
          "Identifying complexity hotspots...",
          "Identifying central modules...",
          "codebase_summary",
          "dependencies",
          "code_metrics",
          "files",
          "Calculating importance scores and sorting files...",
          "token_count",
          "__pycache__",
          "node_modules",
          ".git",
          ".env",
          "*.pyc",
          "*.pyo",
          "*.pyd",
          ".DS_Store",
          "*.log",
          "Initializing CodebaseDigest for path: ",
          "Output will be written to: ",
          "Ignore patterns: ",
          "Found ",
          " relevant files to process",
          "Codebase processing completed in ",
          " seconds",
          ".",
          "__files__",
          "Directory hierarchy built with ",
          " directories",
          "Processing file: ",
          "path",
          "token_count",
          "size_bytes",
          ".py",
          "metadata",
          "content",
          "venv",
          "site-packages",
          "total_files",
          "total_tokens",
          "file_extensions",
          "directory_structure",
          "generation_time",
          "processing_time_seconds",
          "path",
          "metadata",
          "content",
          "Writing ",
          " files to output JSON...",
          "w",
          "Codebase digest written to ",
          "Total tokens: ",
          "Total files processed: ",
          "Advanced codebase digest completed in ",
          " seconds",
          "Analyzing dependencies across ",
          " files...",
          "imports",
          "Building dependency relationships for ",
          " files...",
          "imports",
          "Dependency analysis complete. Found ",
          " dependency relationships",
          "content",
          "line_count",
          "comment_ratio",
          "avg_function_length",
          "complexity_estimate",
          "Code metrics calculated for ",
          " files",
          "\n",
          "\n",
          "total_files",
          "total_tokens",
          "file_extensions",
          "directory_structure",
          "complexity_hotspots",
          "central_modules",
          "generation_time",
          "processing_time_seconds",
          "Adding ",
          " files to output...",
          "path",
          "metadata",
          "metrics",
          "content",
          "Writing enhanced output to ",
          "...",
          "w",
          "Enhanced codebase digest written to ",
          "Total files included: ",
          "Identified ",
          " complexity hotspots",
          "Identified ",
          " central modules",
          "metadata",
          "Digest completed successfully in ",
          " seconds",
          "Digest process failed",
          "r",
          "*",
          "metadata",
          "content",
          "utf-8",
          "metadata",
          "imports",
          "imported_by",
          "end_lineno",
          "metadata",
          "content",
          "utf-8",
          "complexity_estimate",
          "imported_by",
          "complexity_estimate",
          "Error during processing: ",
          ".1f",
          "Processed ",
          " directories for hierarchy",
          "utf-8",
          "ignore",
          "Extracted Python metadata: ",
          " classes, ",
          " functions",
          ".js",
          ".ts",
          "Error processing ",
          ": ",
          "Syntax error in Python file during metadata extraction",
          "Error extracting Python metadata: ",
          "name",
          "name",
          "Error extracting JS/TS metadata: ",
          "*",
          "token_count",
          "files",
          ".1f",
          "imports",
          "Dependency analysis progress: ",
          "/",
          " files",
          "\n",
          "Metrics calculation: ",
          "% complete (",
          "/",
          ")",
          "#",
          "Error calculating function length for ",
          ": ",
          "Error estimating complexity for ",
          ": ",
          "files",
          "file",
          "complexity",
          "imported_by",
          "file",
          "imported_by_count",
          ".2f",
          "Extracted JS/TS metadata",
          "name",
          "methods",
          "lineno",
          "metadata",
          "files",
          "imports",
          "\"\"\"",
          "'''",
          "files",
          "complexity_estimate",
          "complexity",
          "imported_by_count",
          "Progress: ",
          "% (",
          "/",
          ") - Elapsed time: ",
          "s",
          "Current tokens: ",
          "Error processing ",
          ": ",
          "metadata",
          ".1f",
          "\"\"\"",
          "'''",
          "imported_by",
          "classes",
          "functions",
          "classes",
          "Error processing class: ",
          "name",
          "lineno",
          "args",
          ".1f",
          ".1f",
          "arg",
          "\"\"\"",
          "'''",
          "functions",
          "Error processing function: ",
          "Error processing import: ",
          "imported_by",
          "module",
          "strings",
          "imports",
          ".",
          "strings",
          "imports",
          "imports"
        ]
      },
      "metrics": {
        "line_count": 679,
        "comment_ratio": 0.09572901325478646,
        "avg_function_length": 29,
        "complexity_estimate": 95
      },
      "content": "import os\nimport json\nimport tiktoken\nfrom typing import Dict, List, Any, Optional\nimport ast\nimport re\nimport logging\nimport time\nfrom datetime import datetime\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\nlogger = logging.getLogger('CodebaseDigest')\n\nclass CodebaseDigest:\n    \"\"\"Tool to consolidate codebase into LLM-friendly format with hierarchical structure and metadata.\"\"\"\n    \n    def __init__(self, input_path: str, output_path: str, ignore_patterns: Optional[List[str]] = None):\n        self.input_path = input_path\n        self.output_path = output_path\n        self.ignore_patterns = ignore_patterns or [\n            \"__pycache__\", \"node_modules\", \".git\", \".env\", \n            \"*.pyc\", \"*.pyo\", \"*.pyd\", \".DS_Store\", \"*.log\"\n        ]\n        logger.info(f\"Initializing CodebaseDigest for path: {input_path}\")\n        logger.info(f\"Output will be written to: {output_path}\")\n        logger.info(f\"Ignore patterns: {self.ignore_patterns}\")\n        \n        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n        self.total_tokens = 0\n        self.file_data = {}\n        self.hierarchy = {}\n        self.files_processed = 0\n        self.start_time = time.time()\n        \n    def process_codebase(self):\n        \"\"\"Process the entire codebase and generate structured output.\"\"\"\n        logger.info(\"Starting codebase processing...\")\n        \n        # Count total files for progress reporting\n        total_files = 0\n        for root, _, files in os.walk(self.input_path):\n            if self._should_ignore(root):\n                continue\n            for file in files:\n                if not self._should_ignore(file):\n                    total_files += 1\n        \n        logger.info(f\"Found {total_files} relevant files to process\")\n        \n        # Build directory hierarchy\n        logger.info(\"Building directory hierarchy...\")\n        self.build_hierarchy()\n        logger.info(\"Directory hierarchy built successfully\")\n        \n        # Process each file\n        self.files_processed = 0\n        for root, _, files in os.walk(self.input_path):\n            if self._should_ignore(root):\n                continue\n                \n            for file in files:\n                if self._should_ignore(file):\n                    continue\n                    \n                file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(file_path, self.input_path)\n                \n                try:\n                    self.process_file(file_path, relative_path)\n                    self.files_processed += 1\n                    \n                    # Log progress every 10 files or at 25%, 50%, 75% milestones\n                    if (self.files_processed % 10 == 0) or (\n                        total_files > 0 and self.files_processed in [\n                            total_files // 4, total_files // 2, \n                            (total_files * 3) // 4, total_files\n                        ]\n                    ):\n                        elapsed = time.time() - self.start_time\n                        progress = (self.files_processed / total_files) * 100 if total_files > 0 else 0\n                        logger.info(f\"Progress: {progress:.1f}% ({self.files_processed}/{total_files}) - Elapsed time: {elapsed:.1f}s\")\n                        logger.info(f\"Current tokens: {self.total_tokens}\")\n                        \n                except Exception as e:\n                    logger.error(f\"Error processing {file_path}: {str(e)}\")\n        \n        # Generate and save output\n        logger.info(\"Generating final output...\")\n        self.generate_output()\n        \n        elapsed = time.time() - self.start_time\n        logger.info(f\"Codebase processing completed in {elapsed:.1f} seconds\")\n        \n    def build_hierarchy(self):\n        \"\"\"Build a directory structure representation.\"\"\"\n        hierarchy = {}\n        dirs_processed = 0\n        \n        for root, dirs, files in os.walk(self.input_path):\n            # Skip directories that should be ignored\n            if self._should_ignore(root):\n                dirs[:] = []  # Don't process subdirectories\n                continue\n\n            # Filter out ignored directories and files\n            dirs[:] = [d for d in dirs if not self._should_ignore(d)]\n            files = [f for f in files if not self._should_ignore(f)]\n            \n            if not files:\n                continue\n                \n            relative_path = os.path.relpath(root, self.input_path)\n            current = hierarchy\n            \n            if relative_path != '.':\n                path_parts = relative_path.split(os.sep)\n                for part in path_parts:\n                    if part not in current:\n                        current[part] = {}\n                    current = current[part]\n            \n            current['__files__'] = files\n            dirs_processed += 1\n            \n            if dirs_processed % 20 == 0:\n                logger.debug(f\"Processed {dirs_processed} directories for hierarchy\")\n            \n        self.hierarchy = hierarchy\n        logger.info(f\"Directory hierarchy built with {dirs_processed} directories\")\n        \n    def process_file(self, file_path: str, relative_path: str):\n        \"\"\"Process an individual file and extract metadata.\"\"\"\n        _, file_ext = os.path.splitext(file_path)\n        \n        # Log processing more detailed info in debug mode\n        logger.debug(f\"Processing file: {relative_path}\")\n        \n        try:\n            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                content = f.read()\n                \n            # Use disallowed_special=() to prevent errors with special tokens like <|endoftext|>\n            tokens = len(self.encoding.encode(content, disallowed_special=()))\n            self.total_tokens += tokens\n            \n            # Extract function/class information based on file type\n            metadata = {\n                'path': relative_path,\n                'token_count': tokens,\n                'size_bytes': os.path.getsize(file_path),\n            }\n            \n            if file_ext == '.py':\n                metadata.update(self._extract_python_metadata(content))\n                logger.debug(f\"Extracted Python metadata: {len(metadata.get('classes', []))} classes, \"\n                          f\"{len(metadata.get('functions', []))} functions\")\n            elif file_ext in ['.js', '.ts']:\n                metadata.update(self._extract_js_metadata(content))\n                logger.debug(f\"Extracted JS/TS metadata\")\n                \n            self.file_data[relative_path] = {\n                'metadata': metadata,\n                'content': content\n            }\n        except Exception as e:\n            logger.error(f\"Error processing {file_path}: {str(e)}\")\n            raise\n        \n    def _extract_python_metadata(self, content: str) -> Dict[str, Any]:\n        \"\"\"Extract metadata from Python files.\"\"\"\n        metadata = {\n            'classes': [],\n            'functions': [],\n            'imports': [],\n            'strings': []\n        }\n        \n        try:\n            tree = ast.parse(content)\n            \n            # Extract classes and functions while handling potential AST issues\n            for node in ast.walk(tree):\n                # Extract class definitions and methods\n                if isinstance(node, ast.ClassDef):\n                    try:\n                        class_info = {\n                            'name': node.name,\n                            'methods': [m.name for m in node.body if isinstance(m, ast.FunctionDef)],\n                            'lineno': node.lineno\n                        }\n                        metadata['classes'].append(class_info)\n                    except Exception as e:\n                        logger.debug(f\"Error processing class: {str(e)}\")\n                \n                # Extract standalone function definitions\n                elif isinstance(node, ast.FunctionDef):\n                    # Skip methods already counted in classes\n                    parent_class = False\n                    try:\n                        for parent in ast.iter_fields(tree):\n                            if isinstance(parent[1], list) and node in parent[1]:\n                                if any(isinstance(n, ast.ClassDef) for n in parent[1]):\n                                    parent_class = True\n                                    break\n                    except:\n                        pass\n                    \n                    if not parent_class:\n                        try:\n                            args = []\n                            for arg in node.args.args:\n                                if hasattr(arg, 'arg'):\n                                    args.append(arg.arg)\n                            \n                            func_info = {\n                                'name': node.name,\n                                'lineno': node.lineno,\n                                'args': args\n                            }\n                            metadata['functions'].append(func_info)\n                        except Exception as e:\n                            logger.debug(f\"Error processing function: {str(e)}\")\n                \n                # Extract imports\n                elif isinstance(node, ast.Import) or isinstance(node, ast.ImportFrom):\n                    try:\n                        if isinstance(node, ast.Import):\n                            for name in node.names:\n                                metadata['imports'].append(name.name)\n                        else:\n                            for name in node.names:\n                                if hasattr(node, 'module') and node.module:\n                                    metadata['imports'].append(f\"{node.module}.{name.name}\")\n                                else:\n                                    metadata['imports'].append(name.name)\n                    except Exception as e:\n                        logger.debug(f\"Error processing import: {str(e)}\")\n                \n                # Extract string literals\n                elif isinstance(node, ast.Constant) and isinstance(node.value, str):\n                    # For Python 3.8+: handle string constants\n                    try:\n                        metadata['strings'].append(node.value[:100])  # Truncate long strings\n                    except:\n                        pass\n                \n                elif isinstance(node, ast.Str):  # For older Python versions\n                    try:\n                        metadata['strings'].append(node.s[:100])  # Truncate long strings\n                    except:\n                        pass\n                        \n        except SyntaxError:\n            logger.warning(f\"Syntax error in Python file during metadata extraction\")\n        except Exception as e:\n            logger.warning(f\"Error extracting Python metadata: {str(e)}\")\n            \n        return metadata\n        \n    def _extract_js_metadata(self, content: str) -> Dict[str, Any]:\n        \"\"\"Extract metadata from JavaScript/TypeScript files.\"\"\"\n        # Simplified extraction using regex patterns\n        functions = []\n        classes = []\n        imports = []\n        \n        try:\n            # Find function declarations \n            func_pattern = r'function\\s+(\\w+)\\s*\\('\n            for match in re.finditer(func_pattern, content):\n                functions.append({'name': match.group(1)})\n                \n            # Find class declarations\n            class_pattern = r'class\\s+(\\w+)'\n            for match in re.finditer(class_pattern, content):\n                classes.append({'name': match.group(1)})\n                \n            # Find import statements\n            import_pattern = r'import\\s+.*?from\\s+[\\'\"](.+?)[\\'\"]'\n            for match in re.finditer(import_pattern, content):\n                imports.append(match.group(1))\n        except Exception as e:\n            logger.warning(f\"Error extracting JS/TS metadata: {str(e)}\")\n            \n        return {\n            'functions': functions,\n            'classes': classes,\n            'imports': imports\n        }\n        \n    def _should_ignore(self, path: str) -> bool:\n        \"\"\"Check if a path should be included using an allowlist approach.\"\"\"\n        # Skip virtual environment and site-packages directories\n        if 'venv' in path or 'site-packages' in path:\n            return True\n        \n        # Get relative path from input directory\n        rel_path = os.path.relpath(path, self.input_path)\n        path_parts = rel_path.split(os.sep)\n        \n        # Always include files directly in the root directory\n        if len(path_parts) == 1 and os.path.isfile(path):\n            # Still check for ignored file patterns\n            for pattern in self.ignore_patterns:\n                if pattern.startswith('*') and path.endswith(pattern[1:]):\n                    return True\n            return False\n        \n        # Only allow specific directories\n        allowed_dirs = ['app', 'tests', 'scripts']\n        if path_parts[0] not in allowed_dirs:\n            return True\n        \n        # Still check for ignored file patterns\n        for pattern in self.ignore_patterns:\n            if pattern.startswith('*') and path.endswith(pattern[1:]):\n                return True\n            elif pattern in path_parts:\n                return True\n        \n        return False        \n\n    def generate_output(self):\n        \"\"\"Generate the consolidated output.\"\"\"\n        logger.info(\"Generating output file...\")\n        \n        output = {\n            'codebase_summary': {\n                'total_files': len(self.file_data),\n                'total_tokens': self.total_tokens,\n                'file_extensions': self._count_extensions(),\n                'directory_structure': self.hierarchy,\n                'generation_time': datetime.now().isoformat(),\n                'processing_time_seconds': time.time() - self.start_time\n            },\n            'files': []\n        }\n        \n        # Sort files by token count (descending) to prioritize important files\n        logger.info(\"Sorting files by importance...\")\n        sorted_files = sorted(\n            self.file_data.items(), \n            key=lambda x: x[1]['metadata']['token_count'], \n            reverse=True\n        )\n        \n        logger.info(\"Building final file entries...\")\n        for file_path, data in sorted_files:\n            file_entry = {\n                'path': file_path,\n                'metadata': data['metadata'],\n                'content': data['content']\n            }\n            output['files'].append(file_entry)\n        \n        # Write output to file\n        logger.info(f\"Writing {len(output['files'])} files to output JSON...\")\n        with open(self.output_path, 'w', encoding='utf-8') as f:\n            json.dump(output, f, indent=2)\n            \n        logger.info(f\"Codebase digest written to {self.output_path}\")\n        logger.info(f\"Total tokens: {self.total_tokens}\")\n        logger.info(f\"Total files processed: {len(self.file_data)}\")\n        \n    def _count_extensions(self) -> Dict[str, int]:\n        \"\"\"Count the number of files by extension.\"\"\"\n        extensions = {}\n        for file_path in self.file_data:\n            _, ext = os.path.splitext(file_path)\n            if ext:\n                extensions[ext] = extensions.get(ext, 0) + 1\n        return extensions\n\n\nclass AdvancedCodebaseDigest(CodebaseDigest):\n    \"\"\"Advanced codebase digest with additional analysis features.\"\"\"\n    \n    def __init__(self, input_path: str, output_path: str, ignore_patterns: Optional[List[str]] = None):\n        super().__init__(input_path, output_path, ignore_patterns)\n        self.dependencies = {}\n        self.code_metrics = {}\n        logger.info(\"Advanced codebase digest initialized with additional metrics\")\n        \n    def process_codebase(self):\n        \"\"\"Process codebase with advanced metrics.\"\"\"\n        super().process_codebase()\n        \n        logger.info(\"Beginning advanced analysis...\")\n        logger.info(\"Analyzing dependencies...\")\n        self.analyze_dependencies()\n        \n        logger.info(\"Calculating code metrics...\")\n        self.calculate_code_metrics()\n        \n        logger.info(\"Generating enhanced output...\")\n        self.generate_enhanced_output()\n        \n        total_time = time.time() - self.start_time\n        logger.info(f\"Advanced codebase digest completed in {total_time:.1f} seconds\")\n        \n    def analyze_dependencies(self):\n        \"\"\"Analyze import dependencies between files.\"\"\"\n        logger.info(f\"Analyzing dependencies across {len(self.file_data)} files...\")\n        dependency_count = 0\n        \n        for file_path, data in self.file_data.items():\n            if 'imports' in data['metadata']:\n                self.dependencies[file_path] = {\n                    'imports': data['metadata'].get('imports', []),\n                    'imported_by': []\n                }\n        \n        # Build the 'imported_by' relationships\n        total_dependencies = len(self.dependencies)\n        files_processed = 0\n        \n        logger.info(f\"Building dependency relationships for {total_dependencies} files...\")\n        \n        for file_path, deps in self.dependencies.items():\n            for imp in deps['imports']:\n                for other_file, other_data in self.dependencies.items():\n                    if file_path != other_file:\n                        for other_imp in other_data.get('imports', []):\n                            if other_imp.endswith(imp) or imp.endswith(other_imp):\n                                self.dependencies[other_file]['imported_by'].append(file_path)\n                                dependency_count += 1\n                                break\n                            \n            files_processed += 1\n            if files_processed % 20 == 0 or files_processed == total_dependencies:\n                logger.info(f\"Dependency analysis progress: {files_processed}/{total_dependencies} files\")\n                \n        logger.info(f\"Dependency analysis complete. Found {dependency_count} dependency relationships\")\n                            \n    def calculate_code_metrics(self):\n        \"\"\"Calculate various code quality and complexity metrics.\"\"\"\n        logger.info(\"Calculating code metrics...\")\n        files_processed = 0\n        total_files = len(self.file_data)\n        \n        for file_path, data in self.file_data.items():\n            content = data['content']\n            \n            metrics = {\n                'line_count': content.count('\\n') + 1,\n                'comment_ratio': self._calculate_comment_ratio(content, file_path),\n                'avg_function_length': self._calculate_avg_function_length(content, file_path),\n                'complexity_estimate': self._estimate_complexity(content, file_path)\n            }\n            \n            self.code_metrics[file_path] = metrics\n            \n            files_processed += 1\n            if files_processed % 20 == 0 or files_processed == total_files:\n                percent_complete = (files_processed / total_files) * 100\n                logger.info(f\"Metrics calculation: {percent_complete:.1f}% complete ({files_processed}/{total_files})\")\n            \n        logger.info(f\"Code metrics calculated for {files_processed} files\")\n            \n    def _calculate_comment_ratio(self, content: str, file_path: str) -> float:\n        \"\"\"Calculate the ratio of comments to code.\"\"\"\n        _, ext = os.path.splitext(file_path)\n        comment_lines = 0\n        \n        if ext == '.py':\n            # Count # comments and docstrings\n            lines = content.split('\\n')\n            in_multiline = False\n            \n            for line in lines:\n                stripped = line.strip()\n                if in_multiline:\n                    comment_lines += 1\n                    if '\"\"\"' in stripped or \"'''\" in stripped:\n                        in_multiline = False\n                elif stripped.startswith('#'):\n                    comment_lines += 1\n                elif stripped.startswith('\"\"\"') or stripped.startswith(\"'''\"):\n                    comment_lines += 1\n                    if not (stripped.endswith('\"\"\"') or stripped.endswith(\"'''\")):\n                        in_multiline = True\n        \n        total_lines = content.count('\\n') + 1\n        return comment_lines / total_lines if total_lines > 0 else 0\n        \n    def _calculate_avg_function_length(self, content: str, file_path: str) -> int:\n        \"\"\"Calculate average function length in lines.\"\"\"\n        _, ext = os.path.splitext(file_path)\n        \n        if ext != '.py':\n            return 0\n            \n        try:\n            tree = ast.parse(content)\n            functions = [node for node in ast.walk(tree) \n                        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef))]\n            \n            if not functions:\n                return 0\n                \n            # Work around end_lineno attribute issue in some Python versions\n            function_lines = []\n            for func in functions:\n                if hasattr(func, 'end_lineno'):\n                    function_lines.append(func.end_lineno - func.lineno + 1)\n                else:\n                    # Estimate function length based on body size\n                    function_lines.append(len(func.body) + 2)  # +2 for def line and potential docstring\n                    \n            return sum(function_lines) // len(functions) if function_lines else 0\n        except Exception as e:\n            logger.debug(f\"Error calculating function length for {file_path}: {str(e)}\")\n            return 0\n            \n    def _estimate_complexity(self, content: str, file_path: str) -> int:\n        \"\"\"Estimate code complexity based on control structures.\"\"\"\n        _, ext = os.path.splitext(file_path)\n        \n        if ext != '.py':\n            return 0\n            \n        # Count control flow statements as a simple complexity metric\n        complexity = 0\n        \n        try:\n            tree = ast.parse(content)\n            for node in ast.walk(tree):\n                if isinstance(node, (ast.If, ast.For, ast.While, ast.Try)):\n                    complexity += 1\n        except Exception as e:\n            logger.debug(f\"Error estimating complexity for {file_path}: {str(e)}\")\n            \n        return complexity\n        \n    def generate_enhanced_output(self):\n        \"\"\"Generate enhanced output with all collected metrics.\"\"\"\n        logger.info(\"Generating enhanced output...\")\n        \n        logger.info(\"Identifying complexity hotspots...\")\n        hotspots = self._identify_complexity_hotspots()\n        \n        logger.info(\"Identifying central modules...\")\n        central_modules = self._identify_central_modules()\n        \n        output = {\n            'codebase_summary': {\n                'total_files': len(self.file_data),\n                'total_tokens': self.total_tokens,\n                'file_extensions': self._count_extensions(),\n                'directory_structure': self.hierarchy,\n                'complexity_hotspots': hotspots,\n                'central_modules': central_modules,\n                'generation_time': datetime.now().isoformat(),\n                'processing_time_seconds': time.time() - self.start_time\n            },\n            'dependencies': self.dependencies,\n            'code_metrics': self.code_metrics,\n            'files': []\n        }\n        \n        # Sort files by importance score\n        logger.info(\"Calculating importance scores and sorting files...\")\n        sorted_files = sorted(\n            self.file_data.items(), \n            key=lambda x: self._calculate_importance_score(x[0]),\n            reverse=True\n        )\n        \n        logger.info(f\"Adding {len(sorted_files)} files to output...\")\n        for file_path, data in sorted_files:\n            file_entry = {\n                'path': file_path,\n                'metadata': data['metadata'],\n                'metrics': self.code_metrics.get(file_path, {}),\n                'content': data['content']\n            }\n            output['files'].append(file_entry)\n        \n        # Write output to file\n        logger.info(f\"Writing enhanced output to {self.output_path}...\")\n        with open(self.output_path, 'w', encoding='utf-8') as f:\n            json.dump(output, f, indent=2)\n            \n        logger.info(f\"Enhanced codebase digest written to {self.output_path}\")\n        logger.info(f\"Total files included: {len(output['files'])}\")\n        \n    def _identify_complexity_hotspots(self) -> List[Dict[str, Any]]:\n        \"\"\"Identify the most complex parts of the codebase.\"\"\"\n        complexity_threshold = 5  # Arbitrary threshold\n        \n        hotspots = []\n        for file_path, metrics in self.code_metrics.items():\n            if metrics['complexity_estimate'] > complexity_threshold:\n                hotspots.append({\n                    'file': file_path,\n                    'complexity': metrics['complexity_estimate']\n                })\n                \n        result = sorted(hotspots, key=lambda x: x['complexity'], reverse=True)[:5]\n        logger.info(f\"Identified {len(result)} complexity hotspots\")\n        return result\n        \n    def _identify_central_modules(self) -> List[Dict[str, Any]]:\n        \"\"\"Identify central modules based on import relationships.\"\"\"\n        central_modules = []\n        \n        for file_path, deps in self.dependencies.items():\n            if len(deps.get('imported_by', [])) > 2:  # Arbitrary threshold\n                central_modules.append({\n                    'file': file_path,\n                    'imported_by_count': len(deps.get('imported_by', []))\n                })\n                \n        result = sorted(central_modules, key=lambda x: x['imported_by_count'], reverse=True)[:5]\n        logger.info(f\"Identified {len(result)} central modules\")\n        return result\n        \n    def _calculate_importance_score(self, file_path: str) -> float:\n        \"\"\"Calculate an importance score for a file based on various metrics.\"\"\"\n        importance = 0\n        \n        # Factor 1: Token count (more tokens = more important)\n        tokens = self.file_data[file_path]['metadata']['token_count']\n        importance += min(tokens / 1000, 5)  # Cap at 5 points for token count\n        \n        # Factor 2: Dependency centrality\n        if file_path in self.dependencies:\n            imported_by_count = len(self.dependencies[file_path].get('imported_by', []))\n            importance += imported_by_count * 2\n            \n        # Factor 3: Complexity\n        if file_path in self.code_metrics:\n            importance += min(self.code_metrics[file_path]['complexity_estimate'], 10)\n            \n        return importance\n\n\n# Usage for your WITHIN codebase\nif __name__ == \"__main__\":\n    start_time = time.time()\n    logger.info(\"Starting codebase digest process\")\n    \n    # Define paths\n    root_path = \"/Users/alecposner/WITHIN\"\n    output_path = \"/Users/alecposner/WITHIN/codebase_digest.json\"\n    \n    # Define directories to explicitly ignore\n    ignore_dirs = [\".benchmarks\", \".pytest_cache\", \"htmlcov\", \"venv\"]\n    \n    # Base ignore patterns\n    ignore_patterns = [\n        \"__pycache__\", \"node_modules\", \".git\", \".env\", \n        \"*.pyc\", \"*.pyo\", \"*.pyd\", \".DS_Store\", \"*.log\",\n        \".idea\", \".vscode\", \"tiktoken\",\n        \"lib\", \"bin\", \"include\", \"site-packages\"\n    ]\n    \n    # Add directory patterns\n    for dir_pattern in ignore_dirs:\n        ignore_patterns.append(dir_pattern)\n    \n    digest = AdvancedCodebaseDigest(\n        input_path=root_path,\n        output_path=output_path,\n        ignore_patterns=ignore_patterns\n    )\n    \n    try:\n        digest.process_codebase()\n        total_time = time.time() - start_time\n        logger.info(f\"Digest completed successfully in {total_time:.2f} seconds\")\n    except Exception as e:\n        logger.error(f\"Error during processing: {str(e)}\")\n        logger.error(\"Digest process failed\")"
    },
    {
      "path": "app/schemas/ad_score_schema.py",
      "metadata": {
        "path": "app/schemas/ad_score_schema.py",
        "token_count": 1139,
        "size_bytes": 5152,
        "classes": [
          {
            "name": "AdScoreRequestSchema",
            "methods": [],
            "lineno": 9
          },
          {
            "name": "AdScoreResponseSchema",
            "methods": [],
            "lineno": 37
          },
          {
            "name": "AdScoreAnalysisRequestSchema",
            "methods": [],
            "lineno": 74
          },
          {
            "name": "AdScoreAnalysisResponseSchema",
            "methods": [],
            "lineno": 90
          },
          {
            "name": "Config",
            "methods": [],
            "lineno": 20
          },
          {
            "name": "Config",
            "methods": [],
            "lineno": 52
          },
          {
            "name": "Config",
            "methods": [],
            "lineno": 81
          },
          {
            "name": "Config",
            "methods": [],
            "lineno": 103
          }
        ],
        "functions": [],
        "imports": [
          "datetime.datetime",
          "typing.Dict",
          "typing.Any",
          "typing.List",
          "typing.Optional",
          "uuid.UUID",
          "pydantic.BaseModel",
          "pydantic.Field"
        ],
        "strings": [
          "Schemas for ad score models and requests/responses.",
          "Schema for ad score request.",
          "Schema for ad score response.",
          "Schema for requesting detailed ad score analysis.",
          "Schema for detailed ad score analysis response.",
          "standard",
          "Unique identifier for the ad",
          "ID of the advertiser",
          "Full ad content text",
          "Type of ad (image, video, carousel, etc.)",
          "Target audience segments",
          "Platform where ad will be displayed",
          "Historical performance data if available",
          "example",
          "Unique identifier for the ad score record",
          "Ad identifier",
          "Overall predicted engagement score (0-1)",
          "Sentiment analysis score (-1 to 1)",
          "Content complexity score (0-10)",
          "Main topics identified in the ad",
          "Match score with target audience (0-1)",
          "Predicted click-through rate",
          "Model confidence in prediction (0-1)",
          "Suggestions to improve ad performance",
          "Timestamp when the ad was scored",
          "example",
          "ID of the previously scored ad",
          "Depth of analysis: 'basic', 'standard', or 'comprehensive'",
          "Whether to include similar successful ads",
          "example",
          "Analysis record ID",
          "Referenced ad score ID",
          "Extracted content features",
          "Linguistic analysis results",
          "Visual element analysis if applicable",
          "Projected performance metrics",
          "Similar high-performing ads",
          "Detailed improvement suggestions",
          "Timestamp of analysis",
          "ad_id",
          "advertiser_id",
          "ad_content",
          "ad_type",
          "target_audience",
          "platform",
          "historical_performance",
          "ad123456",
          "adv789012",
          "Experience the future of fitness with TechFit Pro. Advanced workout tracking, personalized coaching,",
          "image",
          "instagram",
          "id",
          "ad_id",
          "engagement_score",
          "sentiment_score",
          "complexity_score",
          "topics",
          "target_audience_match",
          "predicted_ctr",
          "confidence_score",
          "improvement_suggestions",
          "processed_at",
          "f47ac10b-58cc-4372-a567-0e02b2c3d479",
          "ad123456",
          "2025-02-20T10:15:30Z",
          "ad_score_id",
          "analysis_depth",
          "include_similar_ads",
          "f47ac10b-58cc-4372-a567-0e02b2c3d479",
          "comprehensive",
          "fitness_enthusiasts",
          "tech_savvy",
          "25-40_age_group",
          "avg_ctr",
          "avg_conversion_rate",
          "previous_engagement_rate",
          "fitness",
          "technology",
          "health",
          "innovation",
          "Include a stronger call-to-action",
          "Emphasize cost savings more prominently",
          "Add social proof elements"
        ]
      },
      "metrics": {
        "line_count": 104,
        "comment_ratio": 0.04807692307692308,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": "\"\"\"Schemas for ad score models and requests/responses.\"\"\"\n\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional\nfrom uuid import UUID\n\nfrom pydantic import BaseModel, Field\n\nclass AdScoreRequestSchema(BaseModel):\n    \"\"\"Schema for ad score request.\"\"\"\n    \n    ad_id: str = Field(..., description=\"Unique identifier for the ad\")\n    advertiser_id: str = Field(..., description=\"ID of the advertiser\")\n    ad_content: str = Field(..., description=\"Full ad content text\")\n    ad_type: str = Field(..., description=\"Type of ad (image, video, carousel, etc.)\")\n    target_audience: Optional[List[str]] = Field(None, description=\"Target audience segments\")\n    platform: str = Field(..., description=\"Platform where ad will be displayed\")\n    historical_performance: Optional[Dict[str, Any]] = Field(None, description=\"Historical performance data if available\")\n    \n    class Config:\n        schema_extra = {\n            \"example\": {\n                \"ad_id\": \"ad123456\",\n                \"advertiser_id\": \"adv789012\",\n                \"ad_content\": \"Experience the future of fitness with TechFit Pro. Advanced workout tracking, personalized coaching, and real-time feedback.\",\n                \"ad_type\": \"image\",\n                \"target_audience\": [\"fitness_enthusiasts\", \"tech_savvy\", \"25-40_age_group\"],\n                \"platform\": \"instagram\",\n                \"historical_performance\": {\n                    \"avg_ctr\": 0.025,\n                    \"avg_conversion_rate\": 0.03,\n                    \"previous_engagement_rate\": 0.045\n                }\n            }\n        }\n\nclass AdScoreResponseSchema(BaseModel):\n    \"\"\"Schema for ad score response.\"\"\"\n    \n    id: UUID = Field(..., description=\"Unique identifier for the ad score record\")\n    ad_id: str = Field(..., description=\"Ad identifier\")\n    engagement_score: float = Field(..., description=\"Overall predicted engagement score (0-1)\")\n    sentiment_score: float = Field(..., description=\"Sentiment analysis score (-1 to 1)\")\n    complexity_score: float = Field(..., description=\"Content complexity score (0-10)\")\n    topics: List[str] = Field(..., description=\"Main topics identified in the ad\")\n    target_audience_match: float = Field(..., description=\"Match score with target audience (0-1)\")\n    predicted_ctr: float = Field(..., description=\"Predicted click-through rate\")\n    confidence_score: float = Field(..., description=\"Model confidence in prediction (0-1)\")\n    improvement_suggestions: List[str] = Field(default_factory=list, description=\"Suggestions to improve ad performance\")\n    processed_at: datetime = Field(..., description=\"Timestamp when the ad was scored\")\n    \n    class Config:\n        orm_mode = True\n        schema_extra = {\n            \"example\": {\n                \"id\": \"f47ac10b-58cc-4372-a567-0e02b2c3d479\",\n                \"ad_id\": \"ad123456\",\n                \"engagement_score\": 0.78,\n                \"sentiment_score\": 0.65,\n                \"complexity_score\": 3.2,\n                \"topics\": [\"fitness\", \"technology\", \"health\", \"innovation\"],\n                \"target_audience_match\": 0.82,\n                \"predicted_ctr\": 0.032,\n                \"confidence_score\": 0.89,\n                \"improvement_suggestions\": [\n                    \"Include a stronger call-to-action\",\n                    \"Emphasize cost savings more prominently\",\n                    \"Add social proof elements\"\n                ],\n                \"processed_at\": \"2025-02-20T10:15:30Z\"\n            }\n        }\n\nclass AdScoreAnalysisRequestSchema(BaseModel):\n    \"\"\"Schema for requesting detailed ad score analysis.\"\"\"\n    \n    ad_score_id: UUID = Field(..., description=\"ID of the previously scored ad\")\n    analysis_depth: str = Field(\"standard\", description=\"Depth of analysis: 'basic', 'standard', or 'comprehensive'\")\n    include_similar_ads: bool = Field(True, description=\"Whether to include similar successful ads\")\n    \n    class Config:\n        schema_extra = {\n            \"example\": {\n                \"ad_score_id\": \"f47ac10b-58cc-4372-a567-0e02b2c3d479\",\n                \"analysis_depth\": \"comprehensive\",\n                \"include_similar_ads\": True\n            }\n        }\n\nclass AdScoreAnalysisResponseSchema(BaseModel):\n    \"\"\"Schema for detailed ad score analysis response.\"\"\"\n    \n    id: UUID = Field(..., description=\"Analysis record ID\")\n    ad_score_id: UUID = Field(..., description=\"Referenced ad score ID\")\n    content_features: Dict[str, Any] = Field(..., description=\"Extracted content features\")\n    linguistic_analysis: Dict[str, Any] = Field(..., description=\"Linguistic analysis results\")\n    visual_elements: List[Dict[str, Any]] = Field(..., description=\"Visual element analysis if applicable\")\n    performance_projections: Dict[str, Any] = Field(..., description=\"Projected performance metrics\")\n    similar_successful_ads: List[Dict[str, Any]] = Field(..., description=\"Similar high-performing ads\")\n    improvement_suggestions: List[Dict[str, Any]] = Field(..., description=\"Detailed improvement suggestions\")\n    created_at: datetime = Field(..., description=\"Timestamp of analysis\")\n    \n    class Config:\n        orm_mode = True"
    },
    {
      "path": "app/schemas/ad_account_health_schema.py",
      "metadata": {
        "path": "app/schemas/ad_account_health_schema.py",
        "token_count": 998,
        "size_bytes": 4603,
        "classes": [
          {
            "name": "AdAccountHealthRequestSchema",
            "methods": [],
            "lineno": 9
          },
          {
            "name": "AdAccountHealthResponseSchema",
            "methods": [],
            "lineno": 27
          },
          {
            "name": "PerformanceMetricSchema",
            "methods": [],
            "lineno": 79
          },
          {
            "name": "Config",
            "methods": [],
            "lineno": 17
          },
          {
            "name": "Config",
            "methods": [],
            "lineno": 41
          },
          {
            "name": "Config",
            "methods": [],
            "lineno": 92
          }
        ],
        "functions": [],
        "imports": [
          "datetime.datetime",
          "typing.Dict",
          "typing.Any",
          "typing.List",
          "typing.Optional",
          "uuid.UUID",
          "pydantic.BaseModel",
          "pydantic.Field"
        ],
        "strings": [
          "Schemas for ad account health models and requests/responses.",
          "Schema for ad account health analysis request.",
          "Schema for ad account health analysis response.",
          "Schema for performance metrics.",
          "last_30_days",
          "Advertiser account ID",
          "Time period for analysis",
          "Specific metrics to include",
          "Period to compare against",
          "example",
          "Health record ID",
          "Advertiser account ID",
          "Overall account health score (0-100)",
          "Engagement trend analysis",
          "Identified risk factors",
          "Optimization recommendations",
          "Ad spend efficiency score",
          "Audience health metrics",
          "Comparison to previous period",
          "Analysis timestamp",
          "example",
          "Metric record ID",
          "Advertiser account ID",
          "Name of the performance metric",
          "Value of the metric",
          "Time period (daily, weekly, monthly)",
          "Start of measurement period",
          "End of measurement period",
          "Whether this metric is flagged as anomalous",
          "Anomaly detection score if applicable",
          "account_id",
          "time_period",
          "include_metrics",
          "comparison_period",
          "adv789012",
          "last_90_days",
          "previous_90_days",
          "id",
          "account_id",
          "health_score",
          "engagement_trends",
          "risk_factors",
          "optimization_suggestions",
          "spend_efficiency",
          "audience_health",
          "comparison_results",
          "created_at",
          "e47ac10b-58cc-4372-a567-0e02b2c3d479",
          "adv789012",
          "2025-02-20T09:30:15Z",
          "engagement",
          "conversion",
          "spend_efficiency",
          "audience_growth",
          "direction",
          "rate_of_change",
          "metrics",
          "improving",
          "growth_rate",
          "engagement_quality",
          "retention_rate",
          "high",
          "health_score_change",
          "significant_improvements",
          "significant_declines",
          "ctr",
          "conversion_rate",
          "type",
          "severity",
          "affected_segments",
          "audience_fatigue",
          "medium",
          "type",
          "severity",
          "affected_campaigns",
          "decreasing_roas",
          "low",
          "type",
          "impact",
          "description",
          "audience_refresh",
          "high",
          "Refresh creative for returning visitors segment",
          "type",
          "impact",
          "description",
          "budget_reallocation",
          "medium",
          "Shift 15% budget from underperforming campaigns",
          "audience_growth",
          "conversion_rate",
          "current",
          "trend",
          "current",
          "trend",
          "returning_visitors",
          "summer_promo"
        ]
      },
      "metrics": {
        "line_count": 93,
        "comment_ratio": 0.043010752688172046,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": "\"\"\"Schemas for ad account health models and requests/responses.\"\"\"\n\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional\nfrom uuid import UUID\n\nfrom pydantic import BaseModel, Field\n\nclass AdAccountHealthRequestSchema(BaseModel):\n    \"\"\"Schema for ad account health analysis request.\"\"\"\n    \n    account_id: str = Field(..., description=\"Advertiser account ID\")\n    time_period: str = Field(\"last_30_days\", description=\"Time period for analysis\")\n    include_metrics: List[str] = Field(default_factory=list, description=\"Specific metrics to include\")\n    comparison_period: Optional[str] = Field(None, description=\"Period to compare against\")\n    \n    class Config:\n        schema_extra = {\n            \"example\": {\n                \"account_id\": \"adv789012\",\n                \"time_period\": \"last_90_days\",\n                \"include_metrics\": [\"engagement\", \"conversion\", \"spend_efficiency\", \"audience_growth\"],\n                \"comparison_period\": \"previous_90_days\"\n            }\n        }\n\nclass AdAccountHealthResponseSchema(BaseModel):\n    \"\"\"Schema for ad account health analysis response.\"\"\"\n    \n    id: UUID = Field(..., description=\"Health record ID\")\n    account_id: str = Field(..., description=\"Advertiser account ID\")\n    health_score: float = Field(..., description=\"Overall account health score (0-100)\")\n    engagement_trends: Dict[str, Any] = Field(..., description=\"Engagement trend analysis\")\n    risk_factors: List[Dict[str, Any]] = Field(..., description=\"Identified risk factors\")\n    optimization_suggestions: List[Dict[str, Any]] = Field(..., description=\"Optimization recommendations\")\n    spend_efficiency: float = Field(..., description=\"Ad spend efficiency score\")\n    audience_health: Dict[str, Any] = Field(..., description=\"Audience health metrics\")\n    comparison_results: Optional[Dict[str, Any]] = Field(None, description=\"Comparison to previous period\")\n    created_at: datetime = Field(..., description=\"Analysis timestamp\")\n    \n    class Config:\n        orm_mode = True\n        schema_extra = {\n            \"example\": {\n                \"id\": \"e47ac10b-58cc-4372-a567-0e02b2c3d479\",\n                \"account_id\": \"adv789012\",\n                \"health_score\": 78.5,\n                \"engagement_trends\": {\n                    \"direction\": \"improving\",\n                    \"rate_of_change\": 0.12,\n                    \"metrics\": {\n                        \"ctr\": {\"current\": 0.028, \"trend\": 0.003},\n                        \"conversion_rate\": {\"current\": 0.032, \"trend\": 0.005}\n                    }\n                },\n                \"risk_factors\": [\n                    {\"type\": \"audience_fatigue\", \"severity\": \"medium\", \"affected_segments\": [\"returning_visitors\"]},\n                    {\"type\": \"decreasing_roas\", \"severity\": \"low\", \"affected_campaigns\": [\"summer_promo\"]}\n                ],\n                \"optimization_suggestions\": [\n                    {\"type\": \"audience_refresh\", \"impact\": \"high\", \"description\": \"Refresh creative for returning visitors segment\"},\n                    {\"type\": \"budget_reallocation\", \"impact\": \"medium\", \"description\": \"Shift 15% budget from underperforming campaigns\"}\n                ],\n                \"spend_efficiency\": 0.92,\n                \"audience_health\": {\n                    \"growth_rate\": 0.08,\n                    \"engagement_quality\": \"high\",\n                    \"retention_rate\": 0.76\n                },\n                \"comparison_results\": {\n                    \"health_score_change\": 5.3,\n                    \"significant_improvements\": [\"audience_growth\", \"conversion_rate\"],\n                    \"significant_declines\": []\n                },\n                \"created_at\": \"2025-02-20T09:30:15Z\"\n            }\n        }\n\nclass PerformanceMetricSchema(BaseModel):\n    \"\"\"Schema for performance metrics.\"\"\"\n    \n    id: Optional[UUID] = Field(None, description=\"Metric record ID\")\n    account_id: str = Field(..., description=\"Advertiser account ID\")\n    metric_name: str = Field(..., description=\"Name of the performance metric\")\n    metric_value: float = Field(..., description=\"Value of the metric\")\n    time_period: str = Field(..., description=\"Time period (daily, weekly, monthly)\")\n    start_date: datetime = Field(..., description=\"Start of measurement period\")\n    end_date: datetime = Field(..., description=\"End of measurement period\")\n    is_anomaly: bool = Field(False, description=\"Whether this metric is flagged as anomalous\")\n    anomaly_score: Optional[float] = Field(None, description=\"Anomaly detection score if applicable\")\n    \n    class Config:\n        orm_mode = True"
    },
    {
      "path": "app/schemas/__init__.py",
      "metadata": {
        "path": "app/schemas/__init__.py",
        "token_count": 382,
        "size_bytes": 1560,
        "classes": [],
        "functions": [],
        "imports": [
          "datetime.datetime",
          "typing.Dict",
          "typing.Any",
          "typing.List",
          "typing.Optional",
          "uuid.UUID",
          "sqlalchemy.ext.declarative.declarative_base",
          "ad_score_schema.AdScoreRequestSchema",
          "ad_score_schema.AdScoreResponseSchema",
          "ad_score_schema.AdScoreAnalysisRequestSchema",
          "ad_score_schema.AdScoreAnalysisResponseSchema",
          "ad_account_health_schema.AdAccountHealthRequestSchema",
          "ad_account_health_schema.AdAccountHealthResponseSchema",
          "ad_account_health_schema.PerformanceMetricSchema"
        ],
        "strings": [
          "WITHIN ML module for ad scoring and account health monitoring.",
          "1.0.0",
          "feature_extraction",
          "scoring",
          "monitoring",
          "MLBase",
          "ML_MODULE_VERSION",
          "ML_CONFIG",
          "AdScoreRequestSchema",
          "AdScoreResponseSchema",
          "AdScoreAnalysisRequestSchema",
          "AdScoreAnalysisResponseSchema",
          "AdAccountHealthRequestSchema",
          "AdAccountHealthResponseSchema",
          "PerformanceMetricSchema",
          "nlp_model",
          "sentiment_analyzer",
          "min_confidence_threshold",
          "en_core_web_lg",
          "vader_lexicon",
          "model_path",
          "scaler_path",
          "default_threshold",
          "models/ad_score_v1.pkl",
          "models/scaler_v1.pkl",
          "drift_detection_window",
          "retraining_frequency_days",
          "minimum_samples_for_retraining"
        ]
      },
      "metrics": {
        "line_count": 63,
        "comment_ratio": 0.12698412698412698,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": "\"\"\"WITHIN ML module for ad scoring and account health monitoring.\"\"\"\n\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional\nfrom uuid import UUID\n\n# SQLAlchemy base for models\nfrom sqlalchemy.ext.declarative import declarative_base\nMLBase = declarative_base()\n\n# Pydantic schemas\nfrom .ad_score_schema import (\n    AdScoreRequestSchema,\n    AdScoreResponseSchema,\n    AdScoreAnalysisRequestSchema,\n    AdScoreAnalysisResponseSchema\n)\n\nfrom .ad_account_health_schema import (\n    AdAccountHealthRequestSchema,\n    AdAccountHealthResponseSchema,\n    PerformanceMetricSchema\n)\n\n# Model version and configuration\nML_MODULE_VERSION = \"1.0.0\"\nML_CONFIG = {\n    \"feature_extraction\": {\n        \"nlp_model\": \"en_core_web_lg\",\n        \"sentiment_analyzer\": \"vader_lexicon\",\n        \"min_confidence_threshold\": 0.6\n    },\n    \"scoring\": {\n        \"model_path\": \"models/ad_score_v1.pkl\",\n        \"scaler_path\": \"models/scaler_v1.pkl\",\n        \"default_threshold\": 0.5\n    },\n    \"monitoring\": {\n        \"drift_detection_window\": 1000,\n        \"retraining_frequency_days\": 14,\n        \"minimum_samples_for_retraining\": 5000\n    }\n}\n\n__all__ = [\n    # Base classes\n    \"MLBase\",\n    \n    # Module configuration\n    \"ML_MODULE_VERSION\",\n    \"ML_CONFIG\",\n    \n    # Ad Score Schemas\n    \"AdScoreRequestSchema\",\n    \"AdScoreResponseSchema\",\n    \"AdScoreAnalysisRequestSchema\",\n    \"AdScoreAnalysisResponseSchema\",\n    \n    # Ad Account Health Schemas\n    \"AdAccountHealthRequestSchema\",\n    \"AdAccountHealthResponseSchema\",\n    \"PerformanceMetricSchema\"\n]"
    },
    {
      "path": "app/models/domain/data_lake_model.py",
      "metadata": {
        "path": "app/models/domain/data_lake_model.py",
        "token_count": 432,
        "size_bytes": 1893,
        "classes": [
          {
            "name": "DataLakeModel",
            "methods": [
              "validate_name",
              "validate_meta_info"
            ],
            "lineno": 18
          }
        ],
        "functions": [
          {
            "name": "validate_name",
            "lineno": 42,
            "args": [
              "self",
              "key",
              "name_value"
            ]
          },
          {
            "name": "validate_meta_info",
            "lineno": 49,
            "args": [
              "self",
              "key",
              "meta"
            ]
          }
        ],
        "imports": [
          "datetime.datetime",
          "uuid.uuid4",
          "sqlalchemy.Column",
          "sqlalchemy.String",
          "sqlalchemy.JSON",
          "sqlalchemy.DateTime",
          "sqlalchemy.LargeBinary",
          "sqlalchemy.CheckConstraint",
          "sqlalchemy.orm.relationship",
          "sqlalchemy.orm.validates",
          "app.core.database.BaseModel"
        ],
        "strings": [
          "Model for storing raw data in the data lake.",
          "data_lake",
          "Ensure 'name' is non-empty and <= 255 chars.",
          "Name cannot be empty.",
          "Name too long.",
          "name",
          "Ensure 'meta_info' is a dictionary.",
          "meta_info must be a dictionary.",
          "meta_info",
          "DataCatalogModel",
          "length(name) > 0",
          "length(data) <= 10485760",
          "data_lake_entry",
          "check_name_not_empty",
          "check_data_size_limit"
        ]
      },
      "metrics": {
        "line_count": 56,
        "comment_ratio": 0.16071428571428573,
        "avg_function_length": 4,
        "complexity_estimate": 0
      },
      "content": "# /Users/alecposner/WITHIN/app/models/domain/data_lake_model.py\n\nfrom datetime import datetime\nfrom uuid import uuid4\n\nfrom sqlalchemy import (\n    Column,\n    String,\n    JSON,\n    DateTime,\n    LargeBinary,\n    CheckConstraint\n)\nfrom sqlalchemy.orm import relationship, validates\n\nfrom app.core.database import BaseModel\n\nclass DataLakeModel(BaseModel):\n    \"\"\"Model for storing raw data in the data lake.\"\"\"\n\n    __tablename__ = \"data_lake\"\n\n    # Store UUID as a 36-char string for broader DB compatibility\n    id = Column(String(36), primary_key=True, default=lambda: str(uuid4()))\n    name = Column(String(255), nullable=False)\n    description = Column(String(500), nullable=True)\n    data = Column(LargeBinary, nullable=False)\n    meta_info = Column(JSON, nullable=False, default=dict)\n\n    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)\n\n    # Example constraints\n    __table_args__ = (\n        CheckConstraint('length(name) > 0', name='check_name_not_empty'),\n        # 10MB limit on 'data'\n        CheckConstraint('length(data) <= 10485760', name='check_data_size_limit'),\n    )\n\n    # Validate fields\n    @validates('name')\n    def validate_name(self, key, name_value):\n        \"\"\"Ensure 'name' is non-empty and <= 255 chars.\"\"\"\n        assert len(name_value) > 0, \"Name cannot be empty.\"\n        assert len(name_value) <= 255, \"Name too long.\"\n        return name_value\n\n    @validates('meta_info')\n    def validate_meta_info(self, key, meta):\n        \"\"\"Ensure 'meta_info' is a dictionary.\"\"\"\n        assert isinstance(meta, dict), \"meta_info must be a dictionary.\"\n        return meta\n\n    # Relationship to DataCatalogModel (string-based to avoid circular import)\n    catalog_entries = relationship(\"DataCatalogModel\", back_populates=\"data_lake_entry\")\n"
    },
    {
      "path": "app/models/domain/data_catalog_model.py",
      "metadata": {
        "path": "app/models/domain/data_catalog_model.py",
        "token_count": 252,
        "size_bytes": 1149,
        "classes": [
          {
            "name": "DataCatalogModel",
            "methods": [],
            "lineno": 16
          }
        ],
        "functions": [],
        "imports": [
          "datetime.datetime",
          "uuid.uuid4",
          "sqlalchemy.Column",
          "sqlalchemy.String",
          "sqlalchemy.JSON",
          "sqlalchemy.DateTime",
          "sqlalchemy.ForeignKey",
          "sqlalchemy.orm.relationship",
          "app.core.database.BaseModel"
        ],
        "strings": [
          "Model for storing metadata about data assets in the data lake.",
          "data_catalog",
          "DataLakeModel",
          "data_lake.id",
          "catalog_entries"
        ]
      },
      "metrics": {
        "line_count": 36,
        "comment_ratio": 0.1111111111111111,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": "# /Users/alecposner/WITHIN/app/models/domain/data_catalog_model.py\n\nfrom datetime import datetime\nfrom uuid import uuid4\n\nfrom sqlalchemy import (\n    Column,\n    String,\n    JSON,\n    DateTime,\n    ForeignKey\n)\nfrom sqlalchemy.orm import relationship\nfrom app.core.database import BaseModel\n\nclass DataCatalogModel(BaseModel):\n    \"\"\"Model for storing metadata about data assets in the data lake.\"\"\"\n\n    __tablename__ = \"data_catalog\"\n\n    id = Column(String(36), primary_key=True, default=lambda: str(uuid4()))\n    name = Column(String(255), nullable=False)\n    description = Column(String(500), nullable=True)\n\n    # Link to 'data_lake.id'\n    data_lake_id = Column(String(36), ForeignKey(\"data_lake.id\"), nullable=False)\n\n    usage_guidelines = Column(String(500), nullable=True)\n    meta_info = Column(JSON, nullable=False, default=dict)\n\n    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)\n\n    # Relationship back to DataLakeModel\n    data_lake_entry = relationship(\"DataLakeModel\", back_populates=\"catalog_entries\")\n"
    },
    {
      "path": "tests/integration_test.py",
      "metadata": {
        "path": "tests/integration_test.py",
        "token_count": 1578,
        "size_bytes": 6504,
        "classes": [],
        "functions": [
          {
            "name": "predictor_with_limited_trials",
            "lineno": 12,
            "args": [
              "monkeypatch"
            ]
          },
          {
            "name": "synthetic_training_data",
            "lineno": 30,
            "args": []
          },
          {
            "name": "test_full_health_pipeline",
            "lineno": 62,
            "args": [
              "synthetic_training_data",
              "predictor_with_limited_trials",
              "capsys"
            ]
          },
          {
            "name": "limited_trials_optimize",
            "lineno": 19,
            "args": [
              "self",
              "func",
              "n_trials"
            ]
          }
        ],
        "imports": [
          "pytest",
          "pandas",
          "numpy",
          "optuna",
          "app.models.ml.prediction.account_health_predictor.AdvancedHealthPredictor"
        ],
        "strings": [
          "\n    Fixture that creates an AdvancedHealthPredictor and patches\n    optuna.study.Study.optimize to ",
          "\n    End-to-end test that ensures:\n      - No more than 500 trials occur (due to the monkey-patch).\n",
          "Model should not be fitted before training",
          "Model should be fitted after training",
          "Expected training start message.",
          "No 'metrics' in training results.",
          "No 'r2' metric in training results.",
          "R2 score should not be too negative",
          "MSE should be reasonably low",
          "No 'feature_importance' in results.",
          "No 'training_size' in results",
          "Training size should be at least 80",
          "Missing 'health_score' in prediction.",
          "Health score out of [0,1] range.",
          "Missing 'risk_factors' in prediction.",
          "Missing 'optimization_suggestions'.",
          "Missing 'confidence_interval' in prediction.",
          "Confidence interval should be a tuple",
          "Confidence interval should have lower and upper bounds",
          "Expected prediction message.",
          "optimize",
          "function",
          "module",
          "[TEST] Starting training...",
          "nn_params",
          "epochs",
          "batch_size",
          "validation_split",
          "verbose",
          "gb_params",
          "n_estimators",
          "max_depth",
          "learning_rate",
          "[TEST] Model fitted status:",
          "[TEST] Pipeline components state:",
          "pipeline",
          "[TEST] Starting training...",
          "metrics",
          "r2",
          "feature_importance",
          "training_size",
          "[TEST] Making a test prediction...",
          "ctr",
          "conversion_rate",
          "spend",
          "clicks",
          "impressions",
          "conversions",
          "cost_per_conversion",
          "revenue",
          "[TEST] Pre-prediction pipeline state:",
          "pipeline",
          "health_score",
          "risk_factors",
          "optimization_suggestions",
          "confidence_interval",
          "[TEST] Making a test prediction...",
          "[TEST] Integration test finished successfully.",
          "ctr",
          "conversion_rate",
          "spend",
          "clicks",
          "impressions",
          "conversions",
          "cost_per_conversion",
          "revenue",
          "health_score",
          "  No pipeline found",
          "metrics",
          "r2",
          "mse",
          "training_size",
          "  No pipeline found",
          "health_score",
          "confidence_interval",
          "metrics",
          "metrics",
          "confidence_interval",
          "  - ",
          ": ",
          "  - ",
          ": ",
          "fitted",
          "not fitted",
          "fitted",
          "not fitted",
          "predict",
          "transform",
          "predict",
          "transform"
        ]
      },
      "metrics": {
        "line_count": 156,
        "comment_ratio": 0.12179487179487179,
        "avg_function_length": 36,
        "complexity_estimate": 5
      },
      "content": "# tests/integration_test.py\n\nimport pytest\nimport pandas as pd\nimport numpy as np\nimport optuna\n\nfrom app.models.ml.prediction.account_health_predictor import AdvancedHealthPredictor\n\n\n@pytest.fixture(scope='function')\ndef predictor_with_limited_trials(monkeypatch) -> AdvancedHealthPredictor:\n    \"\"\"\n    Fixture that creates an AdvancedHealthPredictor and patches\n    optuna.study.Study.optimize to never exceed 500 trials.\n    \"\"\"\n    original_optimize = optuna.study.Study.optimize\n\n    def limited_trials_optimize(self, func, n_trials=None, *args, **kwargs):\n        if n_trials is None or n_trials > 500:\n            n_trials = 500\n        return original_optimize(self, func, n_trials=n_trials, *args, **kwargs)\n\n    monkeypatch.setattr(optuna.study.Study, \"optimize\", limited_trials_optimize)\n\n    return AdvancedHealthPredictor()\n\n\n@pytest.fixture(scope='module')\ndef synthetic_training_data():\n    np.random.seed(42)\n    n = 100\n    ctr = np.random.uniform(0.01, 0.1, n)\n    conversion_rate = np.random.uniform(0.01, 0.2, n)\n    spend = np.random.normal(500, 200, n)\n    clicks = np.random.randint(10, 1000, n)\n    impressions = np.random.randint(1000, 10000, n)\n    conversions = np.random.randint(1, 100, n)\n    # Create additional features the predictor expects:\n    cost_per_conversion = spend / (conversions + 1)\n    revenue = spend * 1.2  # assume revenue is 20% higher than spend\n\n    # Generate a health score with a clear relationship:\n    # (For example, assume health_score increases with ctr and conversion_rate,\n    # decreases with spend, and add some small noise)\n    health_score = 0.3 * ctr + 0.5 * conversion_rate - 0.0001 * spend + np.random.normal(0, 0.01, n)\n    # Clip the health_score to the [0,1] range.\n    health_score = np.clip(health_score, 0, 1)\n\n    return pd.DataFrame({\n        'ctr': ctr,\n        'conversion_rate': conversion_rate,\n        'spend': spend,\n        'clicks': clicks,\n        'impressions': impressions,\n        'conversions': conversions,\n        'cost_per_conversion': cost_per_conversion,\n        'revenue': revenue,\n        'health_score': health_score\n    })\n\ndef test_full_health_pipeline(\n    synthetic_training_data: pd.DataFrame,\n    predictor_with_limited_trials: AdvancedHealthPredictor,\n    capsys: pytest.CaptureFixture\n) -> None:\n    \"\"\"\n    End-to-end test that ensures:\n      - No more than 500 trials occur (due to the monkey-patch).\n      - The model trains on synthetic data.\n      - Predictions are returned as expected.\n    \"\"\"\n    print(\"[TEST] Starting training...\")\n\n    # Verify model is not fitted initially\n    assert not predictor_with_limited_trials.is_fitted, \"Model should not be fitted before training\"\n\n    # Use more training data (at least 100 samples)\n    sample_size = max(min(len(synthetic_training_data), 200), 100)\n    test_data = synthetic_training_data.sample(n=sample_size, random_state=42)\n\n    # Configure model for faster training but with enough epochs\n    predictor_with_limited_trials.config['nn_params'] = {\n        'epochs': 20,  # Increased from 10\n        'batch_size': 32,  # Increased from 16\n        'validation_split': 0.2,\n        'verbose': 0\n    }\n    predictor_with_limited_trials.config['gb_params'] = {\n        'n_estimators': 100,  # Increased from 50\n        'max_depth': 4,  # Increased from 3\n        'learning_rate': 0.1\n    }\n\n    # Train (capped at 500 trials by monkey-patch)\n    results = predictor_with_limited_trials.train(test_data)\n\n    # Verify model is fitted after training\n    assert predictor_with_limited_trials.is_fitted, \"Model should be fitted after training\"\n    \n    # NEW LOGGING - Model state after training\n    print(\"[TEST] Model fitted status:\", predictor_with_limited_trials.is_fitted)\n    print(\"[TEST] Pipeline components state:\")\n    if hasattr(predictor_with_limited_trials, 'pipeline'):\n        for step_name, step in predictor_with_limited_trials.pipeline.named_steps.items():\n            print(f\"  - {step_name}: {'fitted' if hasattr(step, 'predict') or hasattr(step, 'transform') else 'not fitted'}\")\n    else:\n        print(\"  No pipeline found\")\n\n    captured = capsys.readouterr()\n    assert \"[TEST] Starting training...\" in captured.out, \"Expected training start message.\"\n\n    # Enhanced checks on training results\n    assert 'metrics' in results, \"No 'metrics' in training results.\"\n    assert 'r2' in results['metrics'], \"No 'r2' metric in training results.\"\n    assert results['metrics']['r2'] > -0.1, \"R2 score should not be too negative\"\n    assert results['metrics']['mse'] < 0.5, \"MSE should be reasonably low\"\n    assert 'feature_importance' in results, \"No 'feature_importance' in results.\"\n    assert 'training_size' in results, \"No 'training_size' in results\"\n    assert results['training_size'] >= 80, \"Training size should be at least 80\"\n\n    print(\"[TEST] Making a test prediction...\")\n    test_input = {\n        'ctr': 0.05,\n        'conversion_rate': 0.1,\n        'spend': 600,\n        'clicks': 50,\n        'impressions': 1000,\n        'conversions': 5,\n        'cost_per_conversion': 120,\n        'revenue': 720\n    }\n    \n    # NEW LOGGING - Pipeline state before prediction\n    print(\"[TEST] Pre-prediction pipeline state:\")\n    if hasattr(predictor_with_limited_trials, 'pipeline'):\n        for step_name, step in predictor_with_limited_trials.pipeline.named_steps.items():\n            print(f\"  - {step_name}: {'fitted' if hasattr(step, 'predict') or hasattr(step, 'transform') else 'not fitted'}\")\n    else:\n        print(\"  No pipeline found\")\n    \n    prediction = predictor_with_limited_trials.predict_health_score(test_input)\n\n    # Enhanced checks on prediction output\n    assert 'health_score' in prediction, \"Missing 'health_score' in prediction.\"\n    assert 0 <= prediction['health_score'] <= 1, \"Health score out of [0,1] range.\"\n    assert 'risk_factors' in prediction, \"Missing 'risk_factors' in prediction.\"\n    assert 'optimization_suggestions' in prediction, \"Missing 'optimization_suggestions'.\"\n    assert 'confidence_interval' in prediction, \"Missing 'confidence_interval' in prediction.\"\n    assert isinstance(prediction['confidence_interval'], tuple), \"Confidence interval should be a tuple\"\n    assert len(prediction['confidence_interval']) == 2, \"Confidence interval should have lower and upper bounds\"\n\n    captured = capsys.readouterr()\n    assert \"[TEST] Making a test prediction...\" in captured.out, \"Expected prediction message.\"\n\n    print(\"[TEST] Integration test finished successfully.\")"
    },
    {
      "path": "tests/performance_test.py",
      "metadata": {
        "path": "tests/performance_test.py",
        "token_count": 693,
        "size_bytes": 2564,
        "classes": [],
        "functions": [
          {
            "name": "test_training_performance",
            "lineno": 8,
            "args": [
              "benchmark"
            ]
          },
          {
            "name": "test_prediction_latency",
            "lineno": 35,
            "args": [
              "benchmark"
            ]
          },
          {
            "name": "setup",
            "lineno": 11,
            "args": []
          }
        ],
        "imports": [
          "pytest",
          "pandas",
          "numpy",
          "app.models.ml.prediction.ad_score_predictor.AdScorePredictor"
        ],
        "strings": [
          "word_count",
          "sentiment_score",
          "complexity_score",
          "target_audience_match",
          "spend_efficiency",
          "platform",
          "ad_type",
          "content_category",
          "ad_content",
          "mobile",
          "video",
          "fashion",
          "Special offer!",
          "word_count",
          "sentiment_score",
          "complexity_score",
          "target_audience_match",
          "spend_efficiency",
          "platform",
          "ad_type",
          "content_category",
          "ad_content",
          "engagement",
          "engagement",
          "word_count",
          "sentiment_score",
          "complexity_score",
          "target_audience_match",
          "spend_efficiency",
          "platform",
          "ad_type",
          "content_category",
          "ad_content",
          "engagement",
          "engagement",
          "mobile",
          "desktop",
          "tablet",
          "video",
          "banner",
          "text",
          "fashion",
          "tech",
          "food",
          "Ad content ",
          "mobile",
          "desktop",
          "tablet",
          "video",
          "banner",
          "text",
          "fashion",
          "tech",
          "food",
          "Ad content "
        ]
      },
      "metrics": {
        "line_count": 66,
        "comment_ratio": 0.07575757575757576,
        "avg_function_length": 24,
        "complexity_estimate": 0
      },
      "content": "# tests/performance_test.py\nimport pytest\nimport pandas as pd\nimport numpy as np\nfrom app.models.ml.prediction.ad_score_predictor import AdScorePredictor\n\n@pytest.mark.benchmark\ndef test_training_performance(benchmark):\n    model = AdScorePredictor()\n\n    def setup():\n        # Generate large dataset\n        data = pd.DataFrame({\n            'word_count': np.random.randint(50, 500, 1000),\n            'sentiment_score': np.random.uniform(0, 1, 1000),\n            'complexity_score': np.random.uniform(0, 1, 1000),\n            'target_audience_match': np.random.uniform(0, 1, 1000),\n            'spend_efficiency': np.random.uniform(0.5, 2.0, 1000),\n            'platform': np.random.choice(['mobile', 'desktop', 'tablet'], 1000),\n            'ad_type': np.random.choice(['video', 'banner', 'text'], 1000),\n            'content_category': np.random.choice(['fashion', 'tech', 'food'], 1000),\n            'ad_content': [f'Ad content {i}' for i in range(1000)],\n            'engagement': np.random.randint(0, 2, 1000)  # Binary engagement\n        })\n        return data, data['engagement'].values  # Return y as numpy array\n\n    # Run benchmark with proper argument passing\n    benchmark.pedantic(\n        model.fit,\n        args=setup(),  # Pass args as tuple from setup\n        rounds=3\n    )\n\n@pytest.mark.benchmark\ndef test_prediction_latency(benchmark):\n    model = AdScorePredictor()\n    \n    # Generate training data\n    train_data = pd.DataFrame({\n        'word_count': np.random.randint(50, 500, 100),\n        'sentiment_score': np.random.uniform(0, 1, 100),\n        'complexity_score': np.random.uniform(0, 1, 100),\n        'target_audience_match': np.random.uniform(0, 1, 100),\n        'spend_efficiency': np.random.uniform(0.5, 2.0, 100),\n        'platform': np.random.choice(['mobile', 'desktop', 'tablet'], 100),\n        'ad_type': np.random.choice(['video', 'banner', 'text'], 100),\n        'content_category': np.random.choice(['fashion', 'tech', 'food'], 100),\n        'ad_content': [f'Ad content {i}' for i in range(100)],\n        'engagement': np.random.randint(0, 2, 100)\n    })\n    model.fit(train_data, train_data['engagement'])\n    \n    # Sample input for prediction\n    sample_input = {\n        'word_count': 180,\n        'sentiment_score': 0.7,\n        'complexity_score': 0.65,\n        'target_audience_match': 0.8,\n        'spend_efficiency': 1.0,\n        'platform': 'mobile',\n        'ad_type': 'video',\n        'content_category': 'fashion',\n        'ad_content': 'Special offer!'\n    }\n    \n    benchmark(model.predict, sample_input)"
    },
    {
      "path": "tests/test_ad_predictor.py",
      "metadata": {
        "path": "tests/test_ad_predictor.py",
        "token_count": 461,
        "size_bytes": 1767,
        "classes": [],
        "functions": [
          {
            "name": "sample_ad_data",
            "lineno": 8,
            "args": []
          },
          {
            "name": "test_ad_predictor_train",
            "lineno": 23,
            "args": [
              "sample_ad_data"
            ]
          },
          {
            "name": "test_ad_prediction",
            "lineno": 29,
            "args": [
              "sample_ad_data"
            ]
          }
        ],
        "imports": [
          "pytest",
          "app.models.ml.prediction.ad_score_predictor.AdScorePredictor",
          "pandas",
          "numpy"
        ],
        "strings": [
          "score",
          "explanation",
          "word_count",
          "sentiment_score",
          "complexity_score",
          "target_audience_match",
          "spend_efficiency",
          "platform",
          "ad_type",
          "content_category",
          "ad_content",
          "engagement",
          "engagement",
          "engagement",
          "word_count",
          "sentiment_score",
          "complexity_score",
          "target_audience_match",
          "spend_efficiency",
          "platform",
          "ad_type",
          "content_category",
          "ad_content",
          "mobile",
          "video",
          "fashion",
          "Special offer!",
          "mobile",
          "desktop",
          "tablet",
          "video",
          "banner",
          "text",
          "fashion",
          "tech",
          "food",
          "Ad content "
        ]
      },
      "metrics": {
        "line_count": 44,
        "comment_ratio": 0.022727272727272728,
        "avg_function_length": 11,
        "complexity_estimate": 0
      },
      "content": "# tests/test_ad_predictor.py\nimport pytest\nfrom app.models.ml.prediction.ad_score_predictor import AdScorePredictor\nimport pandas as pd\nimport numpy as np\n\n@pytest.fixture\ndef sample_ad_data():\n    n_samples = 30  # Increased sample size for 3-fold CV\n    return pd.DataFrame({\n        'word_count': np.random.randint(50, 500, n_samples),\n        'sentiment_score': np.random.uniform(0, 1, n_samples),\n        'complexity_score': np.random.uniform(0, 1, n_samples),\n        'target_audience_match': np.random.uniform(0, 1, n_samples),\n        'spend_efficiency': np.random.uniform(0.5, 2.0, n_samples),\n        'platform': np.random.choice(['mobile', 'desktop', 'tablet'], n_samples),\n        'ad_type': np.random.choice(['video', 'banner', 'text'], n_samples),\n        'content_category': np.random.choice(['fashion', 'tech', 'food'], n_samples),\n        'ad_content': [f'Ad content {i}' for i in range(n_samples)],\n        'engagement': np.random.randint(0, 2, n_samples)  # Binary engagement\n    })\n\ndef test_ad_predictor_train(sample_ad_data):\n    model = AdScorePredictor()\n    model.fit(sample_ad_data, sample_ad_data['engagement'])\n    assert model.ensemble_weights is not None\n    assert model.explainer is not None\n\ndef test_ad_prediction(sample_ad_data):\n    model = AdScorePredictor()\n    model.fit(sample_ad_data, sample_ad_data['engagement'])\n    prediction = model.predict({\n        'word_count': 180,\n        'sentiment_score': 0.7,\n        'complexity_score': 0.65,\n        'target_audience_match': 0.8,\n        'spend_efficiency': 1.0,\n        'platform': 'mobile',\n        'ad_type': 'video',\n        'content_category': 'fashion',\n        'ad_content': 'Special offer!'\n    })\n    assert 'score' in prediction\n    assert 'explanation' in prediction"
    },
    {
      "path": "tests/test_data_lake.py",
      "metadata": {
        "path": "tests/test_data_lake.py",
        "token_count": 984,
        "size_bytes": 4577,
        "classes": [],
        "functions": [
          {
            "name": "test_successful_data_lake_entry_creation",
            "lineno": 10,
            "args": []
          },
          {
            "name": "test_uuid_generation",
            "lineno": 53,
            "args": []
          },
          {
            "name": "test_duplicate_id_prevention",
            "lineno": 68,
            "args": []
          },
          {
            "name": "test_data_size_limits",
            "lineno": 104,
            "args": []
          },
          {
            "name": "setup_database",
            "lineno": 136,
            "args": []
          }
        ],
        "imports": [
          "pytest",
          "uuid",
          "uuid.uuid4",
          "sqlalchemy.exc.IntegrityError",
          "sqlalchemy.exc.DataError",
          "app.models.domain.data_lake_model.DataLakeModel",
          "app.core.database.SessionLocal",
          "app.core.database.engine",
          "app.core.database.Base",
          "sqlalchemy"
        ],
        "strings": [
          "Test successful data lake entry creation with various scenarios.",
          "Verify UUID generation is unique and follows UUID4 standard.",
          "DataLakeModel must have generate_uuid method",
          "Test preventing duplicate ID entries with robust error handling.",
          "Test data size constraints.",
          "Create all tables before tests and drop them after.",
          "generate_uuid",
          "session",
          "test_data_auto_id",
          "test_data_explicit_id",
          "test_data_auto_id",
          "test_data_explicit_id",
          "source",
          "test_auto",
          "source",
          "test_explicit",
          "first_entry",
          "valid_data",
          "source",
          "test_auto",
          "source",
          "test_explicit",
          "source",
          "first",
          "second_entry",
          "source",
          "size_test",
          "oversized_data",
          "Invalid UUID generated: ",
          "source",
          "second",
          "source",
          "size_test"
        ]
      },
      "metrics": {
        "line_count": 140,
        "comment_ratio": 0.1357142857142857,
        "avg_function_length": 25,
        "complexity_estimate": 5
      },
      "content": "import pytest\nimport uuid\nfrom uuid import uuid4\nfrom sqlalchemy.exc import IntegrityError, DataError\nfrom app.models.domain.data_lake_model import DataLakeModel\nfrom app.core.database import SessionLocal, engine, Base\nimport sqlalchemy\n\n# Use function-based tests instead of class-based tests\ndef test_successful_data_lake_entry_creation():\n    \"\"\"Test successful data lake entry creation with various scenarios.\"\"\"\n    db = SessionLocal()\n    \n    try:\n        # Test auto-generated ID\n        entry_auto_id = DataLakeModel(\n            name=\"test_data_auto_id\", \n            data=b\"test data auto\",\n            meta_info={\"source\": \"test_auto\"}\n        )\n        db.add(entry_auto_id)\n        db.commit()\n        db.refresh(entry_auto_id)\n\n        # Test explicitly provided UUID\n        explicit_uuid = str(uuid4())\n        entry_explicit_id = DataLakeModel(\n            id=explicit_uuid,\n            name=\"test_data_explicit_id\", \n            data=b\"test data explicit\",\n            meta_info={\"source\": \"test_explicit\"}\n        )\n        db.add(entry_explicit_id)\n        db.commit()\n        db.refresh(entry_explicit_id)\n\n        # Assertions for auto-generated ID entry\n        assert entry_auto_id.id is not None\n        assert len(entry_auto_id.id) > 0\n        assert entry_auto_id.name == \"test_data_auto_id\"\n        assert entry_auto_id.data == b\"test data auto\"\n        assert entry_auto_id.meta_info == {\"source\": \"test_auto\"}\n\n        # Assertions for explicitly provided ID entry\n        assert entry_explicit_id.id == explicit_uuid\n        assert entry_explicit_id.name == \"test_data_explicit_id\"\n        assert entry_explicit_id.data == b\"test data explicit\"\n        assert entry_explicit_id.meta_info == {\"source\": \"test_explicit\"}\n\n    finally:\n        db.close()\n\ndef test_uuid_generation():\n    \"\"\"Verify UUID generation is unique and follows UUID4 standard.\"\"\"\n    # Ensure DataLakeModel has a generate_uuid method\n    assert hasattr(DataLakeModel, 'generate_uuid'), \"DataLakeModel must have generate_uuid method\"\n    \n    uuids = [DataLakeModel.generate_uuid() for _ in range(100)]\n    assert len(set(uuids)) == 100  # All UUIDs are unique\n    \n    # Validate UUID4 format\n    for uid in uuids:\n        try:\n            uuid.UUID(uid, version=4)\n        except ValueError:\n            pytest.fail(f\"Invalid UUID generated: {uid}\")\n\ndef test_duplicate_id_prevention():\n    \"\"\"Test preventing duplicate ID entries with robust error handling.\"\"\"\n    db = SessionLocal()\n\n    try:\n        # Generate a fixed UUID for testing\n        test_uuid = DataLakeModel.generate_uuid()\n        \n        # First entry\n        first_entry = DataLakeModel(\n            id=test_uuid,\n            name=\"first_entry\", \n            data=b\"first data\",\n            meta_info={\"source\": \"first\"}\n        )\n        db.add(first_entry)\n        db.commit()\n        \n        # Clear SQLAlchemy's identity map\n        db.expunge_all()\n\n        # Attempt to create second entry with same ID\n        with pytest.raises((IntegrityError, sqlalchemy.exc.IntegrityError)):\n            second_entry = DataLakeModel(\n                id=test_uuid,\n                name=\"second_entry\", \n                data=b\"second data\",\n                meta_info={\"source\": \"second\"}\n            )\n            db.add(second_entry)\n            db.flush()  # This will trigger the database constraint check\n\n    finally:\n        db.rollback()  # Always rollback after the test\n        db.close()\n\ndef test_data_size_limits():\n    \"\"\"Test data size constraints.\"\"\"\n    db = SessionLocal()\n    \n    try:\n        # Test valid data size\n        valid_entry = DataLakeModel(\n            name=\"valid_data\",\n            data=b\"x\" * (10 * 1024 * 1024),  # 10MB\n            meta_info={\"source\": \"size_test\"}\n        )\n        db.add(valid_entry)\n        db.commit()\n\n        # Test oversized data\n        with pytest.raises(Exception):  # Adjust to specific constraint exception\n            oversized_entry = DataLakeModel(\n                name=\"oversized_data\",\n                data=b\"x\" * (10 * 1024 * 1024 + 1),  # Exceeds 10MB\n                meta_info={\"source\": \"size_test\"}\n            )\n            db.add(oversized_entry)\n            db.commit()\n\n    except Exception as e:\n        db.rollback()\n        raise\n    finally:\n        db.close()\n\n# Fixture to set up database before tests\n@pytest.fixture(scope=\"session\", autouse=True)\ndef setup_database():\n    \"\"\"Create all tables before tests and drop them after.\"\"\"\n    Base.metadata.create_all(bind=engine)\n    yield\n    Base.metadata.drop_all(bind=engine)"
    },
    {
      "path": "app/core/data_lake/security_manager.py",
      "metadata": {
        "path": "app/core/data_lake/security_manager.py",
        "token_count": 1406,
        "size_bytes": 6920,
        "classes": [
          {
            "name": "PolicyEngine",
            "methods": [
              "__init__",
              "evaluate"
            ],
            "lineno": 9
          },
          {
            "name": "EncryptionService",
            "methods": [
              "__init__",
              "encrypt",
              "decrypt"
            ],
            "lineno": 65
          },
          {
            "name": "AuditLogger",
            "methods": [
              "record_access_attempt",
              "log_event"
            ],
            "lineno": 93
          },
          {
            "name": "SecurityManager",
            "methods": [
              "__init__",
              "check_access",
              "encrypt_data",
              "decrypt_data",
              "log_general_event"
            ],
            "lineno": 116
          }
        ],
        "functions": [
          {
            "name": "__init__",
            "lineno": 15,
            "args": [
              "self",
              "role_assignments",
              "resource_policies"
            ]
          },
          {
            "name": "evaluate",
            "lineno": 36,
            "args": [
              "self",
              "user_id",
              "resource",
              "action"
            ]
          },
          {
            "name": "__init__",
            "lineno": 71,
            "args": [
              "self",
              "secret_key"
            ]
          },
          {
            "name": "encrypt",
            "lineno": 77,
            "args": [
              "self",
              "data"
            ]
          },
          {
            "name": "decrypt",
            "lineno": 83,
            "args": [
              "self",
              "data"
            ]
          },
          {
            "name": "record_access_attempt",
            "lineno": 99,
            "args": [
              "self",
              "user_id",
              "resource",
              "action",
              "allowed"
            ]
          },
          {
            "name": "log_event",
            "lineno": 106,
            "args": [
              "self",
              "user_id",
              "event_description"
            ]
          },
          {
            "name": "__init__",
            "lineno": 136,
            "args": [
              "self",
              "policy_engine",
              "encryption_service",
              "audit_logger"
            ]
          },
          {
            "name": "check_access",
            "lineno": 144,
            "args": [
              "self",
              "user_id",
              "resource",
              "action"
            ]
          },
          {
            "name": "encrypt_data",
            "lineno": 154,
            "args": [
              "self",
              "data"
            ]
          },
          {
            "name": "decrypt_data",
            "lineno": 165,
            "args": [
              "self",
              "data"
            ]
          },
          {
            "name": "log_general_event",
            "lineno": 176,
            "args": [
              "self",
              "user_id",
              "description"
            ]
          }
        ],
        "imports": [
          "logging",
          "typing.Dict",
          "cryptography.fernet.Fernet"
        ],
        "strings": [
          "\n    Evaluates user permissions based on roles and resource-specific rules.\n    In a real system, th",
          "\n    Handles data encryption and decryption using symmetric encryption (Fernet).\n    In production, ",
          "\n    Records security-relevant events (access attempts, data changes) for compliance and auditing.\n ",
          "\n    High-level manager that brings together:\n      - PolicyEngine for RBAC\n      - EncryptionServic",
          "\n        :param role_assignments: A dict of user_id -> role, e.g. {\"user123\": \"admin\"}\n        :para",
          "\n        Evaluate whether a user can perform the given action on the resource.\n        'action' coul",
          "\n        :param secret_key: Symmetric encryption key (Fernet key must be 32 url-safe base64-encoded ",
          "\n        Encrypt raw bytes. For strings, you can do data.encode('utf-8') first.\n        ",
          "\n        Decrypt bytes and return plain text or binary data.\n        ",
          "\n        Logs whenever a user tries to perform an action on a resource.\n        ",
          "\n        Records a general security event (e.g., user logged in, data encryption error, etc.).\n     ",
          "\n        1. Evaluate whether user can perform the action on the resource via policy engine\n        2",
          "\n        Encrypt the data. You might also log encryption events or handle exceptions here.\n        ",
          "\n        Decrypt the data. You might also log decryption events or handle exceptions here.\n        ",
          "\n        Convenience method to log arbitrary security or system events.\n        ",
          "guest",
          "ALLOWED",
          "DENIED",
          "[AUDIT] User '",
          "' attempted '",
          "' on '",
          "' \u2192 ",
          "[AUDIT EVENT] ",
          " (User: ",
          ")",
          "system",
          "system",
          "Encryption error: ",
          "Decryption error: "
        ]
      },
      "metrics": {
        "line_count": 181,
        "comment_ratio": 0.27071823204419887,
        "avg_function_length": 9,
        "complexity_estimate": 4
      },
      "content": "# /Users/alecposner/WITHIN/app/core/data_lake/security_manager.py\n\nimport logging\nfrom typing import Dict\n\n##################################\n# 1. Policy Engine (Role-Based)\n##################################\nclass PolicyEngine:\n    \"\"\"\n    Evaluates user permissions based on roles and resource-specific rules.\n    In a real system, these might be stored in a DB or external IAM service.\n    \"\"\"\n\n    def __init__(self, role_assignments: Dict[str, str], resource_policies: Dict[str, Dict[str, bool]]):\n        \"\"\"\n        :param role_assignments: A dict of user_id -> role, e.g. {\"user123\": \"admin\"}\n        :param resource_policies: A dict describing which roles can perform which actions on a resource.\n            Example structure:\n              {\n                  \"data_catalog\": {\n                      \"admin\": True,   # admin can do any action\n                      \"editor\": True,  # editor can do any action on resource\n                      \"viewer\": False  # viewer cannot modify resource\n                  },\n                  \"data_lake\": {\n                      \"admin\": True,\n                      \"editor\": False,\n                      \"viewer\": False\n                  }\n              }\n        \"\"\"\n        self.role_assignments = role_assignments\n        self.resource_policies = resource_policies\n\n    def evaluate(self, user_id: str, resource: str, action: str) -> bool:\n        \"\"\"\n        Evaluate whether a user can perform the given action on the resource.\n        'action' could be something like 'read' or 'write' or 'delete'.\n        \n        For simplicity, this example uses a boolean from 'resource_policies' to decide if the role is allowed or not.\n        In a real scenario, you'd refine or store more detailed rules for read/write actions specifically.\n        \"\"\"\n        user_role = self.role_assignments.get(user_id, \"guest\")\n        # If resource is not in policies, default to no access\n        if resource not in self.resource_policies:\n            return False\n\n        # If user_role is not recognized for this resource, default to no\n        role_permissions = self.resource_policies[resource]\n        if user_role not in role_permissions:\n            return False\n\n        # This simplistic approach always returns the same boolean for any action.\n        # For a more robust approach, you'd store 'read/write/delete' distinctions per role.\n        can_access = role_permissions[user_role]\n        return can_access\n\n\n##################################\n# 2. Encryption Service\n##################################\nfrom cryptography.fernet import Fernet\n\nclass EncryptionService:\n    \"\"\"\n    Handles data encryption and decryption using symmetric encryption (Fernet).\n    In production, store the secret key securely (e.g. in a vault or environment variable).\n    \"\"\"\n\n    def __init__(self, secret_key: bytes):\n        \"\"\"\n        :param secret_key: Symmetric encryption key (Fernet key must be 32 url-safe base64-encoded bytes).\n        \"\"\"\n        self._fernet = Fernet(secret_key)\n\n    def encrypt(self, data: bytes) -> bytes:\n        \"\"\"\n        Encrypt raw bytes. For strings, you can do data.encode('utf-8') first.\n        \"\"\"\n        return self._fernet.encrypt(data)\n\n    def decrypt(self, data: bytes) -> bytes:\n        \"\"\"\n        Decrypt bytes and return plain text or binary data.\n        \"\"\"\n        return self._fernet.decrypt(data)\n\n\n##################################\n# 3. Audit Logger\n##################################\nclass AuditLogger:\n    \"\"\"\n    Records security-relevant events (access attempts, data changes) for compliance and auditing.\n    This example logs to stdout via Python's 'logging', but you might also store these events in a DB.\n    \"\"\"\n\n    def record_access_attempt(self, user_id: str, resource: str, action: str, allowed: bool):\n        \"\"\"\n        Logs whenever a user tries to perform an action on a resource.\n        \"\"\"\n        outcome = \"ALLOWED\" if allowed else \"DENIED\"\n        logging.info(f\"[AUDIT] User '{user_id}' attempted '{action}' on '{resource}' \u2192 {outcome}\")\n\n    def log_event(self, user_id: str, event_description: str):\n        \"\"\"\n        Records a general security event (e.g., user logged in, data encryption error, etc.).\n        \"\"\"\n        logging.info(f\"[AUDIT EVENT] {event_description} (User: {user_id})\")\n\n\n##################################\n# 4. Security Manager\n##################################\nclass SecurityManager:\n    \"\"\"\n    High-level manager that brings together:\n      - PolicyEngine for RBAC\n      - EncryptionService for data encryption\n      - AuditLogger for event logging\n\n    Example usage in code:\n        secret_key = Fernet.generate_key()  # or load from secure source\n        policy_engine = PolicyEngine(role_assignments, resource_policies)\n        encryption_service = EncryptionService(secret_key)\n        audit_logger = AuditLogger()\n\n        security_manager = SecurityManager(policy_engine, encryption_service, audit_logger)\n\n        # Check access\n        if security_manager.check_access(user_id='user123', resource='data_catalog', action='write'):\n            # do something...\n    \"\"\"\n\n    def __init__(self, \n                 policy_engine: PolicyEngine, \n                 encryption_service: EncryptionService, \n                 audit_logger: AuditLogger):\n        self.policy_engine = policy_engine\n        self.encryption_service = encryption_service\n        self.audit_logger = audit_logger\n\n    def check_access(self, user_id: str, resource: str, action: str) -> bool:\n        \"\"\"\n        1. Evaluate whether user can perform the action on the resource via policy engine\n        2. Log the attempt in the audit logger\n        3. Return whether it's allowed\n        \"\"\"\n        allowed = self.policy_engine.evaluate(user_id, resource, action)\n        self.audit_logger.record_access_attempt(user_id, resource, action, allowed)\n        return allowed\n\n    def encrypt_data(self, data: bytes) -> bytes:\n        \"\"\"\n        Encrypt the data. You might also log encryption events or handle exceptions here.\n        \"\"\"\n        try:\n            encrypted = self.encryption_service.encrypt(data)\n            return encrypted\n        except Exception as e:\n            self.audit_logger.log_event(\"system\", f\"Encryption error: {str(e)}\")\n            raise\n\n    def decrypt_data(self, data: bytes) -> bytes:\n        \"\"\"\n        Decrypt the data. You might also log decryption events or handle exceptions here.\n        \"\"\"\n        try:\n            decrypted = self.encryption_service.decrypt(data)\n            return decrypted\n        except Exception as e:\n            self.audit_logger.log_event(\"system\", f\"Decryption error: {str(e)}\")\n            raise\n\n    def log_general_event(self, user_id: str, description: str):\n        \"\"\"\n        Convenience method to log arbitrary security or system events.\n        \"\"\"\n        self.audit_logger.log_event(user_id, description)\n"
    },
    {
      "path": "app/models/__init__.py",
      "metadata": {
        "path": "app/models/__init__.py",
        "token_count": 381,
        "size_bytes": 1431,
        "classes": [],
        "functions": [],
        "imports": [
          "app.core.database.BaseModel",
          "ad_score_model.AdScoreModel",
          "ad_score_model.AdScoreAnalysisModel",
          "ad_account_health_model.AdAccountHealthModel",
          "ad_account_health_model.PerformanceMetricModel",
          "ml.prediction.ad_score_predictor.AdScorePredictor",
          "ml.prediction.account_health_predictor.AdvancedHealthPredictor",
          "ml.prediction.anomaly_detector.EnhancedAnomalyDetector"
        ],
        "strings": [
          "ML models for WITHIN ad scoring and account health system.",
          "ad_score",
          "account_health",
          "anomaly_detection",
          "BaseModel",
          "AdScoreModel",
          "AdScoreAnalysisModel",
          "AdAccountHealthModel",
          "PerformanceMetricModel",
          "AdScorePredictor",
          "AdvancedHealthPredictor",
          "EnhancedAnomalyDetector",
          "MODEL_REGISTRY",
          "version",
          "last_trained",
          "metrics",
          "1.0.0",
          "2025-02-01",
          "version",
          "last_trained",
          "metrics",
          "1.0.0",
          "2025-02-05",
          "version",
          "last_trained",
          "metrics",
          "1.0.0",
          "2025-02-10",
          "precision",
          "recall",
          "f1",
          "rmse",
          "r2",
          "accuracy",
          "precision"
        ]
      },
      "metrics": {
        "line_count": 56,
        "comment_ratio": 0.10714285714285714,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": "\"\"\"ML models for WITHIN ad scoring and account health system.\"\"\"\n\nfrom app.core.database import BaseModel\nfrom .ad_score_model import AdScoreModel, AdScoreAnalysisModel\nfrom .ad_account_health_model import AdAccountHealthModel, PerformanceMetricModel\nfrom .ml.prediction.ad_score_predictor import AdScorePredictor\nfrom .ml.prediction.account_health_predictor import AdvancedHealthPredictor\nfrom .ml.prediction.anomaly_detector import EnhancedAnomalyDetector\n\n# Model registry for versioning\nMODEL_REGISTRY = {\n    \"ad_score\": {\n        \"version\": \"1.0.0\",\n        \"last_trained\": \"2025-02-01\",\n        \"metrics\": {\n            \"precision\": 0.89,\n            \"recall\": 0.87,\n            \"f1\": 0.88\n        }\n    },\n    \"account_health\": {\n        \"version\": \"1.0.0\",\n        \"last_trained\": \"2025-02-05\",\n        \"metrics\": {\n            \"rmse\": 0.12,\n            \"r2\": 0.85\n        }\n    },\n    \"anomaly_detection\": {\n        \"version\": \"1.0.0\",\n        \"last_trained\": \"2025-02-10\",\n        \"metrics\": {\n            \"accuracy\": 0.93,\n            \"precision\": 0.91\n        }\n    }\n}\n\n__all__ = [\n    # Base class\n    \"BaseModel\",\n    \n    # SQLAlchemy models\n    \"AdScoreModel\",\n    \"AdScoreAnalysisModel\",\n    \"AdAccountHealthModel\",\n    \"PerformanceMetricModel\",\n    \n    # ML prediction models \n    \"AdScorePredictor\",\n    \"AdvancedHealthPredictor\",\n    \"EnhancedAnomalyDetector\",\n    \n    # Configuration\n    \"MODEL_REGISTRY\"\n]"
    },
    {
      "path": "app/core/data_lake/data_pipeline_service.py",
      "metadata": {
        "path": "app/core/data_lake/data_pipeline_service.py",
        "token_count": 1125,
        "size_bytes": 5249,
        "classes": [
          {
            "name": "DataPipelineService",
            "methods": [
              "__init__",
              "ingest_data",
              "process_data",
              "curate_data",
              "_transform_data",
              "_finalize_data"
            ],
            "lineno": 13
          }
        ],
        "functions": [
          {
            "name": "__init__",
            "lineno": 19,
            "args": [
              "self",
              "security_manager",
              "db_session"
            ]
          },
          {
            "name": "ingest_data",
            "lineno": 27,
            "args": [
              "self",
              "catalog_id",
              "data",
              "user_id"
            ]
          },
          {
            "name": "process_data",
            "lineno": 52,
            "args": [
              "self",
              "catalog_id",
              "user_id"
            ]
          },
          {
            "name": "curate_data",
            "lineno": 80,
            "args": [
              "self",
              "catalog_id",
              "user_id"
            ]
          },
          {
            "name": "_transform_data",
            "lineno": 106,
            "args": [
              "self",
              "raw_data"
            ]
          },
          {
            "name": "_finalize_data",
            "lineno": 119,
            "args": [
              "self",
              "processed_data"
            ]
          }
        ],
        "imports": [
          "logging",
          "sqlalchemy.orm.Session",
          "app.core.data_lake.security_manager.SecurityManager",
          "app.models.domain.data_catalog_model.DataCatalogModel"
        ],
        "strings": [
          "raw",
          "processed",
          "curated",
          "\n    A layered pipeline service that references DataCatalogModel entries\n    to track the 'layer' (r",
          "\n        :param security_manager: Manages access checks, encryption, etc.\n        :param db_session:",
          "\n        Ingest raw data for a given catalog entry ID. This sets the layer to \"raw\"\n        in the d",
          "\n        Example of moving from 'raw' to 'processed': \n        e.g., cleaning text, removing stopwor",
          "\n        Moves from 'processed' to 'curated', indicating final or validated data.\n        ",
          "\n        Example data transformation logic. For text, you might do:\n          - Remove stopwords\n   ",
          "\n        Final curation step\u2014maybe topic modeling or summary stats. \n        ",
          "layer",
          "raw_data",
          "layer",
          "raw_data",
          "layer",
          "processed_data",
          "layer",
          "processed_data",
          "layer",
          "curated_data",
          "User not allowed to ingest data.",
          "[INGEST] Catalog ",
          " set to layer: ",
          "User not allowed to process data.",
          "[PROCESS] Catalog ",
          " moved from ",
          " \u2192 ",
          "User not allowed to curate data.",
          "[CURATE] Catalog ",
          " moved from ",
          " \u2192 ",
          "transformed",
          "curated",
          "ingest",
          "Catalog entry ",
          " not found.",
          "process",
          "Catalog entry ",
          " not found.",
          "Cannot process data from layer '",
          "'. Must be '",
          "' first.",
          "curate",
          "Catalog entry ",
          " not found.",
          "Cannot curate data from layer '",
          "'. Must be '",
          "' first.",
          "catalog:",
          "catalog:",
          "catalog:"
        ]
      },
      "metrics": {
        "line_count": 126,
        "comment_ratio": 0.1984126984126984,
        "avg_function_length": 17,
        "complexity_estimate": 10
      },
      "content": "# /Users/alecposner/WITHIN/app/core/data_lake/data_pipeline_service.py\n\nimport logging\nfrom sqlalchemy.orm import Session\nfrom app.core.data_lake.security_manager import SecurityManager\nfrom app.models.domain.data_catalog_model import DataCatalogModel\n\n# For demonstration, define the three layer names:\nRAW_LAYER = \"raw\"\nPROCESSED_LAYER = \"processed\"\nCURATED_LAYER = \"curated\"\n\nclass DataPipelineService:\n    \"\"\"\n    A layered pipeline service that references DataCatalogModel entries\n    to track the 'layer' (raw, processed, curated), usage guidelines, etc.\n    \"\"\"\n\n    def __init__(self, security_manager: SecurityManager, db_session: Session):\n        \"\"\"\n        :param security_manager: Manages access checks, encryption, etc.\n        :param db_session: SQLAlchemy session for DB ops\n        \"\"\"\n        self.security = security_manager\n        self.db_session = db_session\n\n    def ingest_data(self, catalog_id: str, data, user_id: str):\n        \"\"\"\n        Ingest raw data for a given catalog entry ID. This sets the layer to \"raw\"\n        in the database and optionally stores the data somewhere (S3, local, etc.).\n        \"\"\"\n        # 1) Check access\n        if not self.security.check_access(user_id, resource=f\"catalog:{catalog_id}\", action=\"ingest\"):\n            raise PermissionError(\"User not allowed to ingest data.\")\n\n        # 2) Load the existing catalog entry\n        catalog_entry = self.db_session.query(DataCatalogModel).filter_by(id=catalog_id).first()\n        if not catalog_entry:\n            raise ValueError(f\"Catalog entry {catalog_id} not found.\")\n\n        # 3) Store the data somewhere and update meta_info\n        #    Example: we might store data in an external location, then update meta_info\n        #    For demonstration, let's store it in meta_info\n        catalog_entry.meta_info[\"layer\"] = RAW_LAYER\n        catalog_entry.meta_info[\"raw_data\"] = data\n        catalog_entry.updated_at = None  # triggers auto-update timestamp\n\n        self.db_session.commit()\n\n        logging.info(f\"[INGEST] Catalog {catalog_id} set to layer: {RAW_LAYER}\")\n\n    def process_data(self, catalog_id: str, user_id: str):\n        \"\"\"\n        Example of moving from 'raw' to 'processed': \n        e.g., cleaning text, removing stopwords, or normalizing numeric data.\n        \"\"\"\n        if not self.security.check_access(user_id, resource=f\"catalog:{catalog_id}\", action=\"process\"):\n            raise PermissionError(\"User not allowed to process data.\")\n\n        catalog_entry = self.db_session.query(DataCatalogModel).filter_by(id=catalog_id).first()\n        if not catalog_entry:\n            raise ValueError(f\"Catalog entry {catalog_id} not found.\")\n\n        current_layer = catalog_entry.meta_info.get(\"layer\")\n        if current_layer != RAW_LAYER:\n            raise ValueError(f\"Cannot process data from layer '{current_layer}'. Must be '{RAW_LAYER}' first.\")\n\n        # Simulate data transformation\n        raw_data = catalog_entry.meta_info.get(\"raw_data\", {})\n        processed_data = self._transform_data(raw_data)\n\n        # Update DB\n        catalog_entry.meta_info[\"layer\"] = PROCESSED_LAYER\n        catalog_entry.meta_info[\"processed_data\"] = processed_data\n        catalog_entry.updated_at = None\n        self.db_session.commit()\n\n        logging.info(f\"[PROCESS] Catalog {catalog_id} moved from {RAW_LAYER} \u2192 {PROCESSED_LAYER}\")\n\n    def curate_data(self, catalog_id: str, user_id: str):\n        \"\"\"\n        Moves from 'processed' to 'curated', indicating final or validated data.\n        \"\"\"\n        if not self.security.check_access(user_id, resource=f\"catalog:{catalog_id}\", action=\"curate\"):\n            raise PermissionError(\"User not allowed to curate data.\")\n\n        catalog_entry = self.db_session.query(DataCatalogModel).filter_by(id=catalog_id).first()\n        if not catalog_entry:\n            raise ValueError(f\"Catalog entry {catalog_id} not found.\")\n\n        current_layer = catalog_entry.meta_info.get(\"layer\")\n        if current_layer != PROCESSED_LAYER:\n            raise ValueError(f\"Cannot curate data from layer '{current_layer}'. Must be '{PROCESSED_LAYER}' first.\")\n\n        processed_data = catalog_entry.meta_info.get(\"processed_data\", {})\n        curated_data = self._finalize_data(processed_data)\n\n        # Update DB\n        catalog_entry.meta_info[\"layer\"] = CURATED_LAYER\n        catalog_entry.meta_info[\"curated_data\"] = curated_data\n        catalog_entry.updated_at = None\n        self.db_session.commit()\n\n        logging.info(f\"[CURATE] Catalog {catalog_id} moved from {PROCESSED_LAYER} \u2192 {CURATED_LAYER}\")\n\n    def _transform_data(self, raw_data):\n        \"\"\"\n        Example data transformation logic. For text, you might do:\n          - Remove stopwords\n          - Tokenize\n          - Convert to embeddings\n        Or for numeric data, scale or remove outliers, etc.\n        \"\"\"\n        # Just a stub:\n        if isinstance(raw_data, dict):\n            raw_data[\"transformed\"] = True\n        return raw_data\n\n    def _finalize_data(self, processed_data):\n        \"\"\"\n        Final curation step\u2014maybe topic modeling or summary stats. \n        \"\"\"\n        if isinstance(processed_data, dict):\n            processed_data[\"curated\"] = True\n        return processed_data\n"
    },
    {
      "path": "app/services/domain/data_catalog_service.py",
      "metadata": {
        "path": "app/services/domain/data_catalog_service.py",
        "token_count": 142,
        "size_bytes": 617,
        "classes": [],
        "functions": [
          {
            "name": "add_to_data_catalog",
            "lineno": 5,
            "args": [
              "name",
              "description",
              "data_lake_id",
              "usage_guidelines",
              "metadata"
            ]
          }
        ],
        "imports": [
          "uuid.UUID",
          "uuid.uuid4",
          "app.core.database.SessionLocal",
          "app.models.domain.data_catalog_model.DataCatalogModel"
        ],
        "strings": []
      },
      "metrics": {
        "line_count": 18,
        "comment_ratio": 0.0,
        "avg_function_length": 14,
        "complexity_estimate": 0
      },
      "content": "from uuid import UUID, uuid4  # Import UUID and uuid4\nfrom app.core.database import SessionLocal\nfrom app.models.domain.data_catalog_model import DataCatalogModel\n\ndef add_to_data_catalog(name: str, description: str, data_lake_id: UUID, usage_guidelines: str, metadata: dict):\n    db = SessionLocal()\n    catalog_entry = DataCatalogModel(\n        id=uuid4(),\n        name=name,\n        description=description,\n        data_lake_id=data_lake_id,\n        usage_guidelines=usage_guidelines,\n        metadata=metadata\n    )\n    db.add(catalog_entry)\n    db.commit()\n    db.refresh(catalog_entry)\n    return catalog_entry"
    },
    {
      "path": "tests/error_test.py",
      "metadata": {
        "path": "tests/error_test.py",
        "token_count": 96,
        "size_bytes": 476,
        "classes": [],
        "functions": [
          {
            "name": "test_invalid_input",
            "lineno": 5,
            "args": []
          },
          {
            "name": "test_uninitialized_model",
            "lineno": 11,
            "args": []
          }
        ],
        "imports": [
          "pytest",
          "app.models.ml.prediction.ad_score_predictor.AdScorePredictor",
          "app.models.ml.prediction.anomaly_detector.EnhancedAnomalyDetector"
        ],
        "strings": [
          "error",
          "fallback"
        ]
      },
      "metrics": {
        "line_count": 14,
        "comment_ratio": 0.0,
        "avg_function_length": 4,
        "complexity_estimate": 0
      },
      "content": "import pytest\nfrom app.models.ml.prediction.ad_score_predictor import AdScorePredictor\nfrom app.models.ml.prediction.anomaly_detector import EnhancedAnomalyDetector\n\ndef test_invalid_input():\n    model = AdScorePredictor()\n    prediction = model.predict({})\n    assert prediction['fallback'] is True\n    assert 'error' in prediction\n\ndef test_uninitialized_model():\n    detector = EnhancedAnomalyDetector()\n    with pytest.raises(RuntimeError):\n        detector.detect({}, [])"
    },
    {
      "path": "app/models/ml/__init__.py",
      "metadata": {
        "path": "app/models/ml/__init__.py",
        "token_count": 75,
        "size_bytes": 338,
        "classes": [],
        "functions": [],
        "imports": [
          "prediction.ad_score_predictor.AdScorePredictor",
          "prediction.account_health_predictor.AdvancedHealthPredictor",
          "prediction.anomaly_detector.EnhancedAnomalyDetector"
        ],
        "strings": [
          "ML prediction models for WITHIN.",
          "AdScorePredictor",
          "AdvancedHealthPredictor",
          "EnhancedAnomalyDetector"
        ]
      },
      "metrics": {
        "line_count": 11,
        "comment_ratio": 0.09090909090909091,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": "\"\"\"ML prediction models for WITHIN.\"\"\"\n\nfrom .prediction.ad_score_predictor import AdScorePredictor\nfrom .prediction.account_health_predictor import AdvancedHealthPredictor\nfrom .prediction.anomaly_detector import EnhancedAnomalyDetector\n\n__all__ = [\n    \"AdScorePredictor\",\n    \"AdvancedHealthPredictor\", \n    \"EnhancedAnomalyDetector\"\n]"
    },
    {
      "path": "app/models/ml/prediction/__init__.py",
      "metadata": {
        "path": "app/models/ml/prediction/__init__.py",
        "token_count": 73,
        "size_bytes": 324,
        "classes": [],
        "functions": [],
        "imports": [
          "ad_score_predictor.AdScorePredictor",
          "account_health_predictor.AdvancedHealthPredictor",
          "anomaly_detector.EnhancedAnomalyDetector"
        ],
        "strings": [
          "Prediction models for ad scoring and account health.",
          "AdScorePredictor",
          "AdvancedHealthPredictor",
          "EnhancedAnomalyDetector"
        ]
      },
      "metrics": {
        "line_count": 11,
        "comment_ratio": 0.09090909090909091,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": "\"\"\"Prediction models for ad scoring and account health.\"\"\"\n\nfrom .ad_score_predictor import AdScorePredictor\nfrom .account_health_predictor import AdvancedHealthPredictor\nfrom .anomaly_detector import EnhancedAnomalyDetector\n\n__all__ = [\n    \"AdScorePredictor\",\n    \"AdvancedHealthPredictor\",\n    \"EnhancedAnomalyDetector\"\n]"
    },
    {
      "path": "app/services/domain/data_lake_service.py",
      "metadata": {
        "path": "app/services/domain/data_lake_service.py",
        "token_count": 108,
        "size_bytes": 443,
        "classes": [],
        "functions": [
          {
            "name": "save_to_data_lake",
            "lineno": 5,
            "args": [
              "name",
              "data",
              "metadata"
            ]
          }
        ],
        "imports": [
          "app.core.database.SessionLocal",
          "app.models.domain.data_lake_model.DataLakeModel",
          "uuid.uuid4"
        ],
        "strings": []
      },
      "metrics": {
        "line_count": 16,
        "comment_ratio": 0.0,
        "avg_function_length": 12,
        "complexity_estimate": 0
      },
      "content": "from app.core.database import SessionLocal\nfrom app.models.domain.data_lake_model import DataLakeModel\nfrom uuid import uuid4\n\ndef save_to_data_lake(name: str, data: bytes, metadata: dict):\n    db = SessionLocal()\n    data_lake_entry = DataLakeModel(\n        id=uuid4(),\n        name=name,\n        data=data,\n        metadata=metadata\n    )\n    db.add(data_lake_entry)\n    db.commit()\n    db.refresh(data_lake_entry)\n    return data_lake_entry"
    },
    {
      "path": "tests/test_anomaly_detector.py",
      "metadata": {
        "path": "tests/test_anomaly_detector.py",
        "token_count": 256,
        "size_bytes": 1057,
        "classes": [],
        "functions": [
          {
            "name": "sample_metrics",
            "lineno": 6,
            "args": []
          },
          {
            "name": "test_anomaly_training",
            "lineno": 21,
            "args": [
              "sample_metrics"
            ]
          },
          {
            "name": "test_anomaly_detection",
            "lineno": 27,
            "args": [
              "sample_metrics"
            ]
          }
        ],
        "imports": [
          "pytest",
          "app.models.ml.prediction.anomaly_detector.EnhancedAnomalyDetector"
        ],
        "strings": [
          "Generate sample metrics data with all required columns",
          "anomalies",
          "ctr",
          "conversion_rate",
          "clicks",
          "conversions",
          "spend",
          "revenue",
          "impressions",
          "ctr",
          "conversion_rate",
          "cost_per_conversion",
          "anomalies"
        ]
      },
      "metrics": {
        "line_count": 35,
        "comment_ratio": 0.05714285714285714,
        "avg_function_length": 9,
        "complexity_estimate": 0
      },
      "content": "# tests/test_anomaly_detector.py\nimport pytest\nfrom app.models.ml.prediction.anomaly_detector import EnhancedAnomalyDetector\n\n@pytest.fixture\ndef sample_metrics():\n    \"\"\"Generate sample metrics data with all required columns\"\"\"\n    return [\n        {\n            'clicks': 50,\n            'conversions': 5,\n            'spend': 100.0,\n            'revenue': 150.0,\n            'impressions': 1000,\n            'ctr': 0.05,\n            'conversion_rate': 0.03,\n            'cost_per_conversion': 10.0\n        }\n    ] * 10  # Create 10 identical samples for testing\n\ndef test_anomaly_training(sample_metrics):\n    detector = EnhancedAnomalyDetector()\n    detector.train(sample_metrics)\n    assert detector.is_trained\n    assert len(detector.thresholds) > 0\n\ndef test_anomaly_detection(sample_metrics):\n    detector = EnhancedAnomalyDetector()\n    detector.train(sample_metrics)\n    result = detector.detect(\n        {'ctr': 0.01, 'conversion_rate': 0.001},\n        sample_metrics\n    )\n    assert 'anomalies' in result\n    assert len(result['anomalies']) > 0"
    },
    {
      "path": "tests/test_data_pipeline_service.py",
      "metadata": {
        "path": "tests/test_data_pipeline_service.py",
        "token_count": 973,
        "size_bytes": 4731,
        "classes": [
          {
            "name": "MockPolicyEngine",
            "methods": [
              "evaluate"
            ],
            "lineno": 20
          },
          {
            "name": "MockAuditLogger",
            "methods": [
              "record_access_attempt"
            ],
            "lineno": 25
          },
          {
            "name": "MockEncryptionService",
            "methods": [
              "encrypt",
              "decrypt"
            ],
            "lineno": 32
          },
          {
            "name": "MockSession",
            "methods": [
              "__init__",
              "query",
              "commit"
            ],
            "lineno": 42
          },
          {
            "name": "QueryResultMock",
            "methods": [
              "__init__",
              "first"
            ],
            "lineno": 63
          },
          {
            "name": "TestDataPipelineService",
            "methods": [
              "setUp",
              "test_ingest_data",
              "test_process_data",
              "test_curate_data"
            ],
            "lineno": 74
          },
          {
            "name": "QueryMock",
            "methods": [
              "__init__",
              "filter_by"
            ],
            "lineno": 49
          }
        ],
        "functions": [
          {
            "name": "evaluate",
            "lineno": 21,
            "args": [
              "self",
              "user_id",
              "resource",
              "action"
            ]
          },
          {
            "name": "record_access_attempt",
            "lineno": 26,
            "args": [
              "self",
              "user_id",
              "resource",
              "action",
              "allowed"
            ]
          },
          {
            "name": "encrypt",
            "lineno": 34,
            "args": [
              "self",
              "data"
            ]
          },
          {
            "name": "decrypt",
            "lineno": 36,
            "args": [
              "self",
              "data"
            ]
          },
          {
            "name": "__init__",
            "lineno": 44,
            "args": [
              "self"
            ]
          },
          {
            "name": "query",
            "lineno": 48,
            "args": [
              "self",
              "model"
            ]
          },
          {
            "name": "commit",
            "lineno": 59,
            "args": [
              "self"
            ]
          },
          {
            "name": "__init__",
            "lineno": 65,
            "args": [
              "self",
              "item"
            ]
          },
          {
            "name": "first",
            "lineno": 67,
            "args": [
              "self"
            ]
          },
          {
            "name": "setUp",
            "lineno": 76,
            "args": [
              "self"
            ]
          },
          {
            "name": "test_ingest_data",
            "lineno": 104,
            "args": [
              "self"
            ]
          },
          {
            "name": "test_process_data",
            "lineno": 114,
            "args": [
              "self"
            ]
          },
          {
            "name": "test_curate_data",
            "lineno": 124,
            "args": [
              "self"
            ]
          },
          {
            "name": "__init__",
            "lineno": 50,
            "args": [
              "self",
              "store"
            ]
          },
          {
            "name": "filter_by",
            "lineno": 52,
            "args": [
              "self"
            ]
          }
        ],
        "imports": [
          "unittest",
          "unittest.mock.MagicMock",
          "sqlalchemy.orm.Session",
          "app.core.data_lake.data_pipeline_service.DataPipelineService",
          "app.core.data_lake.security_manager.SecurityManager",
          "app.models.domain.data_catalog_model.DataCatalogModel"
        ],
        "strings": [
          "No-op encryption/decryption for testing.",
          "Simulated DB session using in-memory store for catalog entries.",
          "Simulate a SQLAlchemy Query result.",
          "__main__",
          "123",
          "some",
          "raw_data",
          "raw",
          "processed",
          "processed_data",
          "curated",
          "curated_data",
          "Test Catalog Entry",
          "Testing pipeline",
          "dl001",
          "Test usage",
          "user1",
          "layer",
          "raw_data",
          "some",
          "raw_data",
          "user1",
          "user1",
          "layer",
          "some",
          "raw_data",
          "user1",
          "user1",
          "user1",
          "layer",
          "id"
        ]
      },
      "metrics": {
        "line_count": 137,
        "comment_ratio": 0.24817518248175183,
        "avg_function_length": 6,
        "complexity_estimate": 1
      },
      "content": "# /Users/alecposner/WITHIN/tests/test_data_pipeline_service.py\n\nimport unittest\nfrom unittest.mock import MagicMock\nfrom sqlalchemy.orm import Session\n\n# Import your DataPipelineService (not shown in your snippet, but assumed)\nfrom app.core.data_lake.data_pipeline_service import DataPipelineService\n\n# Import your SecurityManager\nfrom app.core.data_lake.security_manager import SecurityManager\n\n# Import the DataCatalogModel for checking test results\nfrom app.models.domain.data_catalog_model import DataCatalogModel\n\n\n########################################\n# Mock classes for policy, audit, etc.\n########################################\nclass MockPolicyEngine:\n    def evaluate(self, user_id: str, resource: str, action: str) -> bool:\n        # Always allow for testing\n        return True\n\nclass MockAuditLogger:\n    def record_access_attempt(self, user_id: str, resource: str, action: str, allowed: bool):\n        pass\n\n###############################\n# Option A: Minimal Encryption\n###############################\nclass MockEncryptionService:\n    \"\"\"No-op encryption/decryption for testing.\"\"\"\n    def encrypt(self, data: bytes) -> bytes:\n        return data  # Return as-is\n    def decrypt(self, data: bytes) -> bytes:\n        return data  # Return as-is\n\n###############################\n# Mock Session / Query Results\n###############################\nclass MockSession(Session):\n    \"\"\"Simulated DB session using in-memory store for catalog entries.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.catalog_store = {}\n\n    def query(self, model):\n        class QueryMock:\n            def __init__(self, store):\n                self.store = store\n            def filter_by(self, **kwargs):\n                # e.g., filter by id\n                id_ = kwargs.get(\"id\")\n                item = self.store.get(id_)\n                return QueryResultMock(item)\n        return QueryMock(self.catalog_store)\n\n    def commit(self):\n        # No-op for testing\n        pass\n\nclass QueryResultMock:\n    \"\"\"Simulate a SQLAlchemy Query result.\"\"\"\n    def __init__(self, item):\n        self.item = item\n    def first(self):\n        return self.item\n\n\n######################################\n# Tests for DataPipelineService\n######################################\nclass TestDataPipelineService(unittest.TestCase):\n\n    def setUp(self):\n        # 1. Mock DB session\n        self.mock_session = MockSession()\n\n        # 2. Provide ALL required SecurityManager arguments:\n        #    - policy_engine\n        #    - encryption_service\n        #    - audit_logger\n        self.sec_mgr = SecurityManager(\n            policy_engine=MockPolicyEngine(),\n            encryption_service=MockEncryptionService(),  # or MagicMock()\n            audit_logger=MockAuditLogger()\n        )\n\n        # 3. Initialize the service that you'll test\n        self.service = DataPipelineService(self.sec_mgr, self.mock_session)\n\n        # 4. Insert a dummy catalog entry in the in-memory store\n        self.test_catalog_id = \"123\"\n        self.mock_session.catalog_store[self.test_catalog_id] = DataCatalogModel(\n            id=self.test_catalog_id,\n            name=\"Test Catalog Entry\",\n            description=\"Testing pipeline\",\n            data_lake_id=\"dl001\",\n            usage_guidelines=\"Test usage\",\n            meta_info={},\n        )\n\n    def test_ingest_data(self):\n        # Ingest raw data\n        data = {\"some\": \"raw_data\"}\n        self.service.ingest_data(self.test_catalog_id, data, user_id=\"user1\")\n\n        # Verify changes in the in-memory catalog\n        entry = self.mock_session.catalog_store[self.test_catalog_id]\n        self.assertEqual(entry.meta_info[\"layer\"], \"raw\")\n        self.assertEqual(entry.meta_info[\"raw_data\"], data)\n\n    def test_process_data(self):\n        # Must ingest first\n        self.service.ingest_data(self.test_catalog_id, {\"some\": \"raw_data\"}, user_id=\"user1\")\n        # Then process\n        self.service.process_data(self.test_catalog_id, user_id=\"user1\")\n\n        entry = self.mock_session.catalog_store[self.test_catalog_id]\n        self.assertEqual(entry.meta_info[\"layer\"], \"processed\")\n        self.assertIn(\"processed_data\", entry.meta_info)\n\n    def test_curate_data(self):\n        # Ingest -> Process -> Curate\n        self.service.ingest_data(self.test_catalog_id, {\"some\": \"raw_data\"}, user_id=\"user1\")\n        self.service.process_data(self.test_catalog_id, user_id=\"user1\")\n        self.service.curate_data(self.test_catalog_id, user_id=\"user1\")\n\n        entry = self.mock_session.catalog_store[self.test_catalog_id]\n        self.assertEqual(entry.meta_info[\"layer\"], \"curated\")\n        self.assertIn(\"curated_data\", entry.meta_info)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"
    },
    {
      "path": "app/models/domain/__init__.py",
      "metadata": {
        "path": "app/models/domain/__init__.py",
        "token_count": 57,
        "size_bytes": 224,
        "classes": [],
        "functions": [],
        "imports": [
          "app.models.domain.data_lake_model.DataLakeModel",
          "app.models.domain.data_catalog_model.DataCatalogModel"
        ],
        "strings": [
          "DataLakeModel",
          "DataCatalogModel"
        ]
      },
      "metrics": {
        "line_count": 5,
        "comment_ratio": 0.2,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": "# /Users/alecposner/WITHIN/app/models/__init__.py\nfrom app.models.domain.data_lake_model import DataLakeModel\nfrom app.models.domain.data_catalog_model import DataCatalogModel\n\n__all__ = [\"DataLakeModel\", \"DataCatalogModel\"]"
    },
    {
      "path": "app/core/database.py",
      "metadata": {
        "path": "app/core/database.py",
        "token_count": 321,
        "size_bytes": 1484,
        "classes": [
          {
            "name": "BaseModel",
            "methods": [
              "generate_uuid",
              "to_dict"
            ],
            "lineno": 40
          }
        ],
        "functions": [
          {
            "name": "generate_uuid",
            "lineno": 45,
            "args": [
              "cls"
            ]
          },
          {
            "name": "to_dict",
            "lineno": 49,
            "args": [
              "self"
            ]
          }
        ],
        "imports": [
          "sqlalchemy.create_engine",
          "sqlalchemy.orm.sessionmaker",
          "sqlalchemy.orm.declarative_base",
          "sqlalchemy.pool.QueuePool",
          "uuid"
        ],
        "strings": [
          "sqlite:///./test.db",
          "Database session dependency",
          "Base model with common attributes and methods",
          "Create all database tables defined in models",
          "Generate a UUID string",
          "Convert model instance to dictionary"
        ]
      },
      "metrics": {
        "line_count": 59,
        "comment_ratio": 0.2033898305084746,
        "avg_function_length": 4,
        "complexity_estimate": 1
      },
      "content": "# app/core/database.py\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker, declarative_base\nfrom sqlalchemy.pool import QueuePool\nimport uuid\n\n# Create a base class for declarative models\nBase = declarative_base()\n\n# Database connection configuration\nDATABASE_URL = \"sqlite:///./test.db\"  # Update with your actual database URL\n\n# Create engine with connection pooling\nengine = create_engine(\n    DATABASE_URL, \n    echo=False,  # Set to True for SQL logging during development\n    poolclass=QueuePool,\n    pool_size=10,\n    max_overflow=20,\n    pool_timeout=30,\n    pool_recycle=1800\n)\n\n# Create session factory\nSessionLocal = sessionmaker(\n    autocommit=False, \n    autoflush=False, \n    bind=engine\n)\n\ndef get_db():\n    \"\"\"Database session dependency\"\"\"\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n# Base model with common functionality\nclass BaseModel(Base):\n    \"\"\"Base model with common attributes and methods\"\"\"\n    __abstract__ = True\n\n    @classmethod\n    def generate_uuid(cls):\n        \"\"\"Generate a UUID string\"\"\"\n        return str(uuid.uuid4())\n\n    def to_dict(self):\n        \"\"\"Convert model instance to dictionary\"\"\"\n        return {\n            column.name: getattr(self, column.name)\n            for column in self.__table__.columns\n        }\n\n# Create all tables defined in models\ndef create_tables():\n    \"\"\"Create all database tables defined in models\"\"\"\n    Base.metadata.create_all(bind=engine)"
    },
    {
      "path": "app/__init__.py",
      "metadata": {
        "path": "app/__init__.py",
        "token_count": 0,
        "size_bytes": 0,
        "classes": [],
        "functions": [],
        "imports": [],
        "strings": []
      },
      "metrics": {
        "line_count": 1,
        "comment_ratio": 0.0,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": ""
    },
    {
      "path": "app/core/__init__.py",
      "metadata": {
        "path": "app/core/__init__.py",
        "token_count": 0,
        "size_bytes": 0,
        "classes": [],
        "functions": [],
        "imports": [],
        "strings": []
      },
      "metrics": {
        "line_count": 1,
        "comment_ratio": 0.0,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": ""
    },
    {
      "path": "app/core/config/__init__.py",
      "metadata": {
        "path": "app/core/config/__init__.py",
        "token_count": 0,
        "size_bytes": 0,
        "classes": [],
        "functions": [],
        "imports": [],
        "strings": []
      },
      "metrics": {
        "line_count": 1,
        "comment_ratio": 0.0,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": ""
    },
    {
      "path": "app/core/ml/__init__.py",
      "metadata": {
        "path": "app/core/ml/__init__.py",
        "token_count": 0,
        "size_bytes": 0,
        "classes": [],
        "functions": [],
        "imports": [],
        "strings": []
      },
      "metrics": {
        "line_count": 1,
        "comment_ratio": 0.0,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": ""
    },
    {
      "path": "app/utils/__init__.py",
      "metadata": {
        "path": "app/utils/__init__.py",
        "token_count": 0,
        "size_bytes": 0,
        "classes": [],
        "functions": [],
        "imports": [],
        "strings": []
      },
      "metrics": {
        "line_count": 1,
        "comment_ratio": 0.0,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": ""
    },
    {
      "path": "app/api/__init__.py",
      "metadata": {
        "path": "app/api/__init__.py",
        "token_count": 0,
        "size_bytes": 0,
        "classes": [],
        "functions": [],
        "imports": [],
        "strings": []
      },
      "metrics": {
        "line_count": 1,
        "comment_ratio": 0.0,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": ""
    },
    {
      "path": "app/api/v1/__init__.py",
      "metadata": {
        "path": "app/api/v1/__init__.py",
        "token_count": 0,
        "size_bytes": 0,
        "classes": [],
        "functions": [],
        "imports": [],
        "strings": []
      },
      "metrics": {
        "line_count": 1,
        "comment_ratio": 0.0,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": ""
    },
    {
      "path": "app/api/v1/routes/__init__.py",
      "metadata": {
        "path": "app/api/v1/routes/__init__.py",
        "token_count": 0,
        "size_bytes": 0,
        "classes": [],
        "functions": [],
        "imports": [],
        "strings": []
      },
      "metrics": {
        "line_count": 1,
        "comment_ratio": 0.0,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": ""
    },
    {
      "path": "app/services/__init__.py",
      "metadata": {
        "path": "app/services/__init__.py",
        "token_count": 0,
        "size_bytes": 0,
        "classes": [],
        "functions": [],
        "imports": [],
        "strings": []
      },
      "metrics": {
        "line_count": 1,
        "comment_ratio": 0.0,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": ""
    },
    {
      "path": "app/services/ml/__init__.py",
      "metadata": {
        "path": "app/services/ml/__init__.py",
        "token_count": 0,
        "size_bytes": 0,
        "classes": [],
        "functions": [],
        "imports": [],
        "strings": []
      },
      "metrics": {
        "line_count": 1,
        "comment_ratio": 0.0,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": ""
    },
    {
      "path": "tests/__init__.py",
      "metadata": {
        "path": "tests/__init__.py",
        "token_count": 0,
        "size_bytes": 0,
        "classes": [],
        "functions": [],
        "imports": [],
        "strings": []
      },
      "metrics": {
        "line_count": 1,
        "comment_ratio": 0.0,
        "avg_function_length": 0,
        "complexity_estimate": 0
      },
      "content": ""
    }
  ]
}